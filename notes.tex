\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath, amsthm}
\usepackage{xcolor}
\usepackage[
    top=10mm,
    bottom=10mm,
    left=30mm,
    right=30mm,
    marginparwidth=0mm,
    marginparsep=0mm,
    % headheight=15pt,
    centering,
    % showframe,
    includefoot,
    % includehead
]{geometry}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{framed}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{chngcntr}
\usepackage{float}
\usepackage{multicol}


% \input xypic (for commutative diagrams)
% \include{mssymb}

\def\A{{\mathbb A}}
\def\P{{\mathbb P}}
\def\N{{\mathbb N}}
\def\Z{{\mathbb Z}}
\def\Q{{\mathbb Q}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\F{{\mathbb F}}
\def\O{{\cal O}}
\let\sec\S
\let\S\relax
\def\S{{\mathfrak S}}
\def\g{{\mathfrak g}}
\def\p{{\mathfrak p}}
\def\h{{\mathfrak h}}
\def\n{{\mathfrak n}}
\def\v{{\mathfrak v}}
\def\m{{\mathfrak m}}
\def\a{{\alpha}}


\newcommand{\skipline}{\vspace{\baselineskip}}
\newcommand{\dis}{\displaystyle}
\newcommand{\noin}{\noindent}


% remove all paragraph indents
\setlength{\parindent}{0pt}

% Figure counter include section
\counterwithin{figure}{section}

% Cleaner figures
\newcommand{\fig}[3][0.4]{
  \begin{figure}[H]
    \centering
    \includegraphics[width=#1\textwidth, keepaspectratio]{#2}
    \caption{#3}
  \end{figure}
}
% Mathematical notation


\newcommand{\Span}{\mathrm{Span}}
\newcommand{\rank}{\mathrm{Rank}}
\newcommand{\nullity}{\mathrm{Nullity}}
\newcommand{\longhookrightarrow}{\lhook\joinrel\relbar\joinrel\rightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\La}{\Leftarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\dbar}{\overline{\partial}}
\newcommand{\gequ}{\geqslant}
\newcommand{\lequ}{\leqslant}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Coker}{\mathrm{Coker}}
\newcommand{\Row}{\mathrm{Row}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\Id}{\mathrm{Id}}
% \newcommand{\mod}{\mathrm{mod }}
\newcommand{\un}{\underline}
\newcommand{\ov}{\overline}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\pr}{\prime}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\im}{\mathrm{Im}}

% Linear Algebra

\newcommand{\lind}{linearly independent}
\newcommand{\ldep}{linearly dependent}
\renewcommand{\vec}[1]{
  {\bf #1}
}
\newcommand{\lincomb}[3]{
  #1_{1}#2_{1} + #1_{2}#2_{2} + \cdots + #1_{#3}#2_{#3}
}
\newcommand{\neglincomb}[3]{
  -#1_{1}#2_{1} - #1_{2}#2_{2} - \cdots - #1_{#3}#2_{#3}
}
\newcommand{\constants}[2]{
  #1_{1}, #1_{2}, \cdots, #1_{#2}
}
\newcommand{\constantsz}[2]{
  #1_{0}, \constants{#1}{#2}
}

% Analysis
\newcommand{\limfty}[1]{\lim_{#1 \to \infty}}
\newcommand{\seq}[2]{\{#1_{#2}\}_{#2 \in \N}}
\newcommand{\sseq}[3]{\{#1_{#2_{#3}}\}_{#3 \in \N}}
\newcommand{\chep}{Let $\epsilon > 0$}

% Category Theory
\newcommand{\catC}{\mathcal{C}}
\newcommand{\catD}{\mathcal{D}}
\newcommand{\functF}{\mathcal{F}}
\newcommand{\functG}{\mathcal{G}}
\newcommand{\functI}{\mathcal{I}}
\newcommand{\functU}{\mathcal{U}}

\newcommand{\op}[1]{#1^{\mathrm{op}}}
\newcommand{\Obj}{\mathrm{Obj}}

\newcommand{\Set}{\mathbf{Set}}
\newcommand{\Grp}{\mathbf{Grp}}
\newcommand{\Top}{\mathbf{Top}}
\newcommand{\cRing}{\mathbf{cRing}}
\newcommand{\BanAnaMan}{\mathbf{BanAnaMan}}
\newcommand{\FinSet}{\mathbf{FinSet}}
\newcommand{\Vect}{\mathbf{Vect}}
\newcommand{\Two}{\mathbf{2}}


% ================= %
% Headers & Footers
% ================= %
\pagestyle{fancy}
\fancyhf{}
\newcommand{\intros}[3]{
  \lhead{\textbf{#1} {#2}}
  \rhead{#3}}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}



% ================= %
%       Utils
% ================= %
\newcommand{\induction}[3]{
  \textbf{Base Case} #1 \\
  \textbf{Inductive Hypothesis} \\ #2 \\
  \textbf{Inductive Step} \\ #3
}



% Used to list all problems on homework
\newcommand{\problems}[1]{
  \medskip \noin
  {\bf Problems}

  #1

  \medskip{}
}


% augmented matrices
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

% ================= %
%      Box Meta
% ================= %

% #2 - FG Color
% #3 - BG Color
\newenvironment{fancyleftbar}[3][\hsize]
{%
    \def\FrameCommand
    {%
        {\color{#2}\vrule width 3pt}%
        \hspace{0pt}%must no space.
        \fboxsep=\FrameSep\colorbox{#3}%
    }%
    \MakeFramed{\hsize#1\advance\hsize-\width\FrameRestore}%
}
{\endMakeFramed}

% Used to allow the color argument to pass through the environment%
\newsavebox{\boxqed} 

% #1 - Header
% #2 - FG Color
% #3 - BG Color
\newenvironment{fancybox}[3]{
  \sbox\boxqed{\textcolor{#2}{$\blacksquare$}}
  \begin{fancyleftbar}{#2}{#3}

  \noin
  {\large \sc #1} \\
}
{

  \medskip
  \noin
  \usebox\boxqed

  \end{fancyleftbar}
}

% #1 - Text header
% #2 - Outer Text
% #3 - Inner Text
% #4 - Inner Header
% #5 - FG Color
% #6 - Background Color
\newcommand{\boxmeta}[6]{
  \bigskip \bigskip \noin
  {\large\sc #1}

  \smallskip\noin
  #2

  \begin{fancybox}{#4}{#5}{#6}
    \noin
    #3
  \end{fancybox}
}

% #1 - Title
% #2 - FG Color
% #3 - BG Color
% #4 - Inner Text
\newcommand{\baronly}[4]{
  \begin{fancyleftbar}{#2}{#3}{\large \sc #1}

    #4
  \end{fancyleftbar}
}

% ================= %
%     Box Colors
% ================= %

\definecolor{theorem_fg}{HTML}{EABAC3}
\definecolor{theorem_bg}{HTML}{F9EEF0}

\definecolor{problem_fg}{HTML}{ABABAB}
\definecolor{problem_bg}{HTML}{EDEDED}

\definecolor{lemma_fg}{HTML}{D0C97D}
\definecolor{lemma_bg}{HTML}{FCF9DB}

\definecolor{prop_fg}{HTML}{7DDB89}
\definecolor{prop_bg}{HTML}{D7FADB}

\definecolor{defn_fg}{HTML}{83D4CF}
\definecolor{defn_bg}{HTML}{E7FCFB}

% ================= %
%     Box Meso
% ================= %

\newcommand{\thm}[2]{
  \boxmeta{Theorem}{#1}{#2}{Proof}{theorem_fg}{theorem_bg}
}

\newcommand{\namedtheorem}[3]{
  \boxmeta{#1}{#2}{#3}{Proof}{theorem_fg}{theorem_bg}
}

\newcommand{\prob}[3]{
  \boxmeta{Problem #1}{#2}{#3}{Solution}{problem_fg}{problem_bg}
}

\newcommand{\exampleprob}[2]{
  \boxmeta{Example}{#1}{#2}{Solution}{problem_fg}{problem_bg}
}

\newcommand{\lemma}[2]{
  \boxmeta{Lemma}{#1}{#2}{Proof}{lemma_fg}{lemma_bg}
}

\newcommand{\namedlemma}[3]{
  \boxmeta{#1}{#2}{#3}{Proof}{lemma_fg}{lemma_bg}
}

\newcommand{\corr}[2]{
  \boxmeta{Corrolary}{#1}{#2}{Proof}{lemma_fg}{lemma_bg}
}

\newcommand{\proposition}[2]{
  \boxmeta{Proposition}{#1}{#2}{Proof}{prop_fg}{prop_bg}
}

\newcommand{\definition}[1]{
  \baronly{Definition}{defn_fg}{defn_bg}{#1}
}

\newcommand{\example}[1]{
  \baronly{Example}{problem_fg}{problem_bg}{#1}
}

\newcommand{\note}[1]{
  \baronly{Note}{problem_fg}{problem_bg}{#1}
}

\def\B{\mathcal B}
\def\P{\mathcal P}

\newcommand{\btw}[1]{
    $\langle$ #1 $\rangle$
}

\renewcommand{\date}[1]{\underline{\bf #1}}

\def\eps{\varepsilon}
\def\range{\text{Range}}
\def\Null{\text{Null}}

\def\RowSpace{\text{RowSpace}}
\def\ColSpace{\text{ColSpace}}


% create a command for TODOs
\newcommand{\TODO}{\color{red}\textbf{TODO}\color{black}}



\begin{document}
  \footnotesize

    \tableofcontents
    
    \newpage

    \date{Fri. Feb 10 2023}

    \section{Vector Spaces}

    Suppose that $V$ is a finite dimensional vector space over $F$, with $\dim(V)
    = n$.

    $V$ may have {\it many different} bases, we know that they all have the same
    size $n$.

    Say $\B = \{\alpha_1, ..., \alpha_n\}$ is a basis fix the ordering of $\B$.

    Fix the ordering of $\B$.

    \thm {
      For any $\alpha \in V$, there is a unique $n$ tuple $(x_1, ..., x_n) \in F^n$
      such that

      \[
        \alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n
      \]
    }
    {
      Existence is immediate, since $\B$ is a basis, thus $\B$ spans $V$.

      {\bf Uniqueness}

      Say $\alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n$
      and $\alpha = y_1 \alpha_1 + \cdots + y_n \alpha_n$.

      Then we have that

      $x_1 \alpha_1 + \cdots + x_n \alpha_n - y_1 \alpha_1 + \cdots + y_n \alpha_n
      = 0$, so $(x_1 - y_1)\alpha_1 + \cdots + (x_n - y_n)\alpha_n = 0$

      But since $\{\alpha_1, ..., \alpha_n\}$ is linearly independent, all
      coefficients must be $0$.
    }

    What this means is that, for a vector space $V$, there is an associated
    mapping in $F^n$. Notice that we know nothing about the vectors $\alpha_i$.

    We define $[\alpha]_{\B}$ to be the {\it coordinates} of $\alpha$ with
    respect to $\B$.

    {\bf Check}: The mapping $\alpha \mapsto [\alpha]_{\B} \in F^n$ satisfies

    \begin{enumerate}
      \item One to one-ness
      \item Onto-ness
      \item "Additive", for any $\alpha, \beta \in V$, if $\alpha = x_1 \alpha_1
        + \cdots + x_n \alpha_n$ and $\beta = y_1 \alpha_1 + \cdots + y_n
        \alpha_n$. Then

        \[
          [\alpha + \beta]_{\B} = \begin{bmatrix}
            x_1 + y_1 \\
            x_2 + y_2 \\
            \vdots \\
            x_n + y_n \\
          \end{bmatrix}
          =
          \begin{bmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n \\
          \end{bmatrix}
          +
          \begin{bmatrix}
            y_1 \\
            y_2 \\
            \vdots \\
            y_n \\
          \end{bmatrix}
          =
          [\alpha]_{\B}
          +
          [\beta]_{\B}
        \]
      \item $[c\alpha]_{\B} + c[\alpha]_{\B}$
    \end{enumerate}

    There exists an {\it isomorphism} between $V$ and $F^n$.

    \example{
      Let $\P$ be the space of al polynomials. Let $f(x) = x^3$, and $g(x) =
      x^5$. Then, let

      \[
        V = \Span\{f, g\} = \{\text{all } ax^3 + bx^5 : a, b \in F\}
      \]

      then, $\dim(V) = 2$, since $f$ and $g$ are linearly independent.

      Typical $h(x) \in V$, say $h(x) = 10x^3 - 2x^5$.

      \[
        [h]_{\B} = \begin{bmatrix}
          10 \\
          -2
        \end{bmatrix}
      \]
    }

    \btw {
      $[h]_{\B}$ is the mapping of $h$ to $F^n$. \TODO{} is this right?
    }

    Now let $k(x) = 2x^3 + 4x^5$ and $l(x) = x^3 + 3x^5$. Since $k, l$ are
    linearly independent, they form another basis of $V$.

    \[
      \B' = \{k(x), l(x)\}
    \]

    \subsection{Change of Basis}

    Given $\B = \{\alpha_1, \dots, \alpha_n\}$, and $\B' = \{\alpha_1', ...,
    \alpha_n'\}$ bases for $V$.

    We want to describe the map going from $[\alpha]_{\B} \mapsto
    [\alpha]_{\B'}$.

    \btw{ We want to find $\text{The $\B$ coordinate of $\alpha$} \mapsto
    \text{the $\B'$ coordinate of $\alpha$}$}

    {\bf Step 1}.

    Compute the $\B$ coordinate of $\alpha_1', ..., \alpha_n'$, {\it old}
    coordinates of the {\it new} basis elements. \\

    {\bf Step 2}.

    For an $n \times m$ matrix

    \[
      P = \Big[ [\alpha_1']_{\B}, \dots, [\alpha_n']_{\B} \Big]
    \]

    {\bf Check}: for any $\alpha \in V$

    \[
      [\alpha]_{\B} = P [\alpha]_{\B'}
    \]

    {\bf Ans}: This is what we actually want

    \[
      [\alpha]_{\B'} = P^{-1} [\alpha]_{\B}
    \]

    \date{Mon. Feb 13 2023}

    \TODO{} Missing {\it some} info

    {\bf Want}: Describe the mapping $T: F^n \to F^n$

    \[
      T([\alpha]_{\B_\text{old}}) = [\alpha]_{\B'_{\text{new}}}
    \]

    \btw {
      If we switch the basis for some reason, we want to see what the new
      coordinates are.
    }

    {\bf To do this}: For each $\alpha_j'$, compute
    $[\alpha_j']_{\B_\text{old}}$. Let

    \[
      P = \Big[[\alpha_1']_{\B_\text{old}} \cdots [\alpha_n']_{\B_{\text{old}}}\Big]
    \]

    be an $n \times n$ matrix.

    {\bf Claim}: For any $\alpha \in V$

    \[
      P \cdot [\alpha]_{\B'_\text{new}} = [\alpha]_{\B_\text{old}}
    \]

    {\bf How?}

    \[
      P \cdot [\alpha_1']_{\B'_\text{new}} = P \cdot \begin{bmatrix}
        1 \\
        0 \\
        \vdots \\
        0
      \end{bmatrix}
      = [\alpha_1']_{\B_\text{old}}
    \]

    This is the $1^{st}$ column of $P$, and similarly for all columns.

    {\bf Thus}: For any $\alpha \in V$, 

    \[
      [\alpha]_\text{new} = P^{-1} \cdot [\alpha]_\text{old}
    \]

    \example {
      In practice, we have the following.

      $V = \Span(\{x^3, x^5\})$ subspace of $\P$, the set of all polynomials. Let
      $f(x) = x^3, g(x) = x^5, \B = \{x^3, x^5\}$. Let $h(x) = 10x^3 - 2x^5 \in V$.

      {\bf Question}: What are the coordinates of $h$ with respect to $\B$?

      {\bf Answer}:
      \[
        [h]_{\B} = \begin{bmatrix}
          10 \\
          -2
        \end{bmatrix}
      \]
    }

    Let's now see what happens when we create a new basis $\B'$.

    \example{
      Let $k(x) = 2x^3 + 5x^5$, $l(x) = x^3 + 3x^5$.

      Let $\B' = \{k(x), l(x)\} = \{2x^3 + 5x^5, x^3 + 3x^5\}$ be another basis
      of $V$, still with $\B = \{f(x), g(x)\} = \{x^3, x^5\}$.

      {\bf Question}: What are the coordinates of $h(x) = 10x^3 - 2x^5$ with
      respect to $\B'$ now?

      {\bf Answer}:

      Well we know that $[k(x)]_{\B} = \begin{bmatrix} 2 \\ 5 \end{bmatrix}$ and
      $[l(x)]_{\B} = \begin{bmatrix} 1 \\ 3 \end{bmatrix}$, these are just the
      coordinates of $k$, and $l$ with respect to $\B$.

      So now we can construct our $P$ matrix

      \[
        P = \Big[ [k(x)]_{\B}, [l(x)]_{\B} \Big] = \begin{bmatrix}
          2 & 1 \\
          5 & 3 \\
        \end{bmatrix}
      \]

      notice that $P$'s columns are constructed from $k(x)$ and $l(x)$,
      expressed in terms of our standard basis $\B$.

      {\bf Check}:

      \[
        P^{-1} = \begin{bmatrix}
          3 & -1 \\
          -5 & 2 \\
        \end{bmatrix}
      \]

      Then

      \[
        \begin{bmatrix}
          3 & -1 \\
          -5 & 2 \\
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
          10 \\ -2
        \end{bmatrix}
        =
        \begin{bmatrix}
          32 \\ -54
        \end{bmatrix}
      \]

      {\bf This means}:

      \[
        h(x) = 32k(x) - 54l(x) = 10x^3 - 2x^5
      \]

      Which is what we expect.

    }

    \example {
      Let $V = \R^2$. Standard basis $\B = \{\eps_1, \eps_2\} = \{(1, 0), (0, 1)\}$

      \[
        [(5, 4)]_{\B} = \begin{bmatrix}
          5 \\ 4
        \end{bmatrix}
      \]

      Fix angle $\theta$, Let

      \[
        \B' = \{(\cos(\theta), \sin(\theta)), (-\sin(\theta), \cos(\theta))\}
      \]

      {\bf Question}: What is $\begin{bmatrix} 5 \\ 4
      \end{bmatrix}_{\B'_\text{new}}$?

      {\bf Answer}:

      \begin{enumerate}
        \item Form $P$

          \[
            [(\cos(\theta), \sin(\theta))]_{\B} = \begin{bmatrix} \cos(\theta)
            \\ \sin(\theta) \end{bmatrix}
          \]

          \[
            [(-\sin(\theta), \cos(\theta))]_{\B} = \begin{bmatrix} -\sin(\theta)
            \\ \cos(\theta) \end{bmatrix}
          \]

          Then 

          \[
            P = \begin{bmatrix}
              \cos(\theta) & -\sin(\theta) \\
              \sin(\theta) & \cos(\theta)
            \end{bmatrix}
          \]

          {\bf Fact}:

          \[
            P^{-1} = \begin{bmatrix}
              \cos(\theta) & \sin(\theta) \\
              -\sin(\theta) & \cos(\theta)
            \end{bmatrix}
          \]

          so we have

          \begin{align*}
            [(5, 4)]_{\B'_\text{new}} =&P^{-1}
            \begin{bmatrix} 5 \\ 4 \end{bmatrix} \\
            =&\begin{bmatrix}
              \cos(\theta) & \sin(\theta) \\
              -\sin(\theta) & \cos(\theta)
            \end{bmatrix}
            \begin{bmatrix} 5 \\ 4 \end{bmatrix} \\
            =&\begin{bmatrix}
              5\cos(\theta) & 4\sin(\theta) \\
              -5\sin(\theta) & 4\cos(\theta)
            \end{bmatrix}
          \end{align*}
      \end{enumerate}
    }

    \section{Linear Transformations}

    Say $V$, $W$ are both vector spaces over the same field $F$.
    
    \definition{
      A {\bf Linear Transformation} $T: V \to W$ is a function satisifying two
      rules

      \begin{enumerate}
        \item For all $\alpha, \beta \in V$,

          \[
            T(\alpha + \beta) = T(\alpha) + T(\beta)
          \]

          Note that the first $+$ is addition in $V$, but the second
          is addition in $W$.

        \item For all $\alpha \in V$ and $c \in F$,

          \[
            T(c\alpha) = cT(\alpha)
          \]
      \end{enumerate}
    }

    The book combines the two definitions above into one, like this,

    \[
      T(c\alpha + \beta) = cT(\alpha) + T(\beta)
    \]

    Let's quickly take some time to understand what $V$ and $W$ are here.
    Suppose we have a transformation $T: V \to W$, then $V$ is the {\bf
    domain} and $W$ is the {\bf codomain}.

    Here, $T$ is just a function, which means that it {\it must} use all of $T$,
    but it {\it does not} have to use all of $W$. For example, the following is
    a perfectly valid transformation.

    \example {
      Let $T: \P^3 \to \P^2$ be the transformation that takes all degree $3$
      polynomials to the space of degree $2$ polynomials, with

      \[
        T(f) = \vec{0}
      \]

      for all $f \in \P^3$.
    }

    Its obvious that there are more degree $2$ polynomials in the world than
    just the $\vec{0}$ polynomial. So here, we say that the $\range(T) =
    \{\vec{0}\}$, and that 

    \[
      \range(T) \subseteq W
    \]

    but maybe we are getting ahead of ourselves.

    \subsection{Basic Facts}

    Suppose that $T: V \to W$ is a linear transformation
    \begin{enumerate}
      \item $T(0) = 0$

        {\bf Proof}:

        \[
          T(0 + 0) = T(0) + T(0) = \vec{0} + \vec{0} = \vec{0}
        \]

        {\bf Note}: $0$ lives in the field, and $\vec{0}$ lives in $W$, the {\bf
        codomain} of the transformation $T$.

        \btw{Always be aware of where the $0$ lives}

      \item For all $\{\alpha_1, ..., \alpha_n\} \subseteq V$, all $\{c_1, ...,
        c_n\} \in F$,

        \[
          c_1 T(\alpha_1) + \cdots + c_n T(\alpha_n)
        \]

        {\bf Proof} Easy induction on $n$, just follows from part (2) of the
        definition.
    \end{enumerate}


    \subsection{Examples}

    Let's look at multiple examples of linear transformations to get an idea of
    how they behave.

    \example{
      We already know that each matrix $A$ has an associated linear
      transformation $T_A$. Let's look at this in more detail now.

      Let $A \in F^{m \times n}$ be an $m \times n$ matrix with entries from a
      field $F$.

      Then, let $T_A: F^n \to F^m$ be defined by

      \[
        T_A(\vec{x}) = A \vec{x}
      \]

      where $\vec{x}$ is a vector in $F^n$.

      Let's check that this is indeed a linear transformation.

      Chose any $\vec{x}, \vec{y} \in F^n$, then

      \begin{enumerate}
        \item \btw{Check that $T_A(\vec{x} + \vec{y}) = T_A(\vec{x}) + T_A(\vec{y})$}

          Let $\vec{x}, \vec{y} \in V$, then

          \[
            T_A(\vec{x} + \vec{y}) = A (\vec{x} + \vec{y}) = A \vec{x} + A \vec{y} = T_A(\vec{x}) + T_A(\vec{y})
          \]

          so this works as we expect.

        \item \btw{Check that $T_A(c\vec{x}) = cT_A(\vec{x})$ for $c \in F$.}

          let $c \in F$, then we have

          \[
            T_A(cX) = A \cdot (cX) = cAX = c T_A(X)
          \]

          which is also what we expect.
      \end{enumerate}

      so we have proved that $T_A$ is a linear transformation!
    }

    \example {
      Consider $\P$ the set of all polynomials $a_0 + a_1x + \cdots + a_n x^n$.

      Let's define $D: \P \to \P$ which takes a function $f \in \P$ to $f' \in
      \P$, where $f'$ is the {\it derivative} of $f$.

      \[
        D(f) = f'
      \]

      {\bf Claim}:

      $D$ is a linear transformation.

      {\bf Proof}: 

      Take two functions $f, g \in \P$, then by definition of $D$, we have

      \[
        D(f + g) = (f + g)' = f' + g' = D(f) + D(g)
      \]

      and for $c \in F$,

      \[
        D(cf) = (cf)' = c \cdot f' = c D(f)
      \]

      so the derivative is a linear transformation!
    }

    \example {
      Let $C(\R)$ be the set of all continuous functions $f: \R \to \R$.

      Let's define $I: C(\R) \to C(\R)$ which takes a function $f \in C(\R)$ to
      $F \in C(\R)$, where $F$ is the {\it antiderivative} of $f$.

      \[
        I(f) = \int_0^x f(t)dt
      \]

      \btw {
        Note that the integral exists because you can always integrate a
        continuous function.
      }

      The result is also continuous and differentiable by the Fundamental
      Theorem of Calculus.

      \[
        D(I(f)) = f
      \]

      Is the {\bf Fundamental Theorem of Calculus}.

      Therefore $I(f)$ really {\it is} continuous, $I(f) \in C(\R)$.

      {\bf Claim}:

      $I$ is a linear transformation.

      {\bf Proof}:

      Take two functions $f, g \in \P$, then by definition of $I$, we have

      \begin{align*}
        I(f + g) =&\int_0^x (f(t) + g(t))dt \\
                 =&\int_0^x f(t)dt + \int_0^x g(t)dt \\
                 =&I(f) + I(g)
      \end{align*}

      and

      \[
        I(cf) = \int_0^x cf(t)dt = c\int_0^x f(t)dt = cI(f)
      \]

      so the integral is a linear transformation!
    }

    \newpage

    \date{Fri. Feb 15 2023}

    Recall: A linear transformation $T: V \to W$ is a function between two
    vector spaces over the same field $F$, satisifying
    \begin{enumerate}
      \item For all $\alpha, \beta \in V$,

        \[
          T(\alpha + \beta) = T(\alpha) + T(\beta)
        \]

        Note that the first $+$ is addition in $V$, but the second
        is addition in $W$.

      \item For all $\alpha \in V$ and $c \in F$,

        \[
          T(c\alpha) = cT(\alpha)
        \]
    \end{enumerate}

    For all $\alpha_1,..., \alpha_k \in V$,  and $c_1, ..., c_k \in F$, it
    breaks nicely into

    \[
      T(c_1 \alpha_1 + \cdots + c_k \alpha_k) = c_1 T(\alpha_1) + \cdots + c_k
      T(\alpha_k)
    \]

    \example {
      $I^*: C(\R) \to \R$ (all continuous functions from $\R$ to $\R$)

      \[
        I^*(f) = \int_0^1 f(x)dx
      \]

      \[
        I^*(x^2) = \int_0^1 x^2 dx = \frac{x^3}{x} \Big|_0^1 = \frac{1}{3}
      \]

      Note that the output of $I*$ is just a number here. Additionally, $I^*$ is
      linear: you can split integrals up for polynomials, and you can take
      constants outside.
    }

    For any $V, W$, we also have

    \[
      X: V \to W
    \]

    Is the zero transformation. It takes any $\alpha \in V$ to the $0$ of $W$.
    We'll use this to prove theorems about linear transformations later.

    \thm{
      Let's prove existence and uniqueness of linear transformations.

      \begin{enumerate}
        \item Linear Transformations $T: V \to W$ are {\bf determined} by their
          behavior on a basis $\B$ of $V$. More precisely,

          Suppose that $\B = \{\alpha_1, ..., \alpha_n\}$ is a basis for $V$ and
          suppose that $T, U: V \to W$ are both linear transformations (and they
          agree on a basis), such that

          \[
            T(\alpha_1) = U(\alpha_1), T(\alpha_2) = U(\alpha_2), ..., T(\alpha_n) =
            U(\alpha_n)
          \]

          Then $T = U$

        \item For {\bf any map} $T_0: \B \to W$, there s a unique linear
          transformation $T: V \to W$ with $T \supseteq T_0$. In other words,

          Let $\B = \{\alpha_1, ..., \alpha_n\}$ be {\bf any basis} for $V$
          and let $\beta_1, ..., \beta_n$ be {\bf any vectors} in $W$.

          Then, there is a {\bf unique} linear transformation $T: V \to W$ such
          that

          \[
            T(\alpha_1) = \beta_1, T(\alpha_2) = \beta_2, ..., T(\alpha_n) = \beta_n
          \]
      \end{enumerate}
    }
    {
      \begin{enumerate}
        \item {\bf Uniqueness}: Chose any $\alpha \in V$, since $\B$ is a basis,

        \btw{
            Will show that $T = U \Leftrightarrow$ For any $\alpha \in V$,
            $T(\alpha) = U(\alpha)$
        }

        \[
          \alpha = c_1 \alpha_1 + \cdots + c_n \alpha_n
        \]
        
        for some {\bf unique} $c_1, ..., c_n \in F$.

        Since $T$ is a linear transformation,

        \[
          T(\alpha) = c_1 T(\alpha_1) + \cdots + c_n T(\alpha_n)
        \]

        Likewise with $U$,

        \[
          U(\alpha) = c_1 U(\alpha_1) + \cdots + c_n U(\alpha_n)
        \]

        But, since $T(\alpha_1) = U(\alpha_1), ..., T(\alpha_n) = U(\alpha_n)$,
        $T(\alpha) = U(\alpha)$.

        \btw{
          Essentially, if $T, U$ work the same for all $\alpha_i$, then their
          sum will obviously be the same, and so they'll give the same result
          for the same $\alpha$.
        }

        Note that this theorem {\it still} works for infinite dimensional
        vector spaces.

      \item {\bf Existence}: Chose any $\alpha \in V$. \btw{ We must define
        $T(\alpha)$}

        Since $\B$ is a basis, we can write

        \[
          \alpha = c_1 \alpha_1 + \cdots + c_n \alpha_n
        \]

        which is unique.

        Define 

        \[
          T(\alpha) := c_1 \beta_1 + \cdots + c_n \beta_n \in W
        \]

        {\bf Check}: Is $T$ linear?

        Say $\gamma = d_1 \alpha_1 + \cdots + d_n \alpha_n$, $\delta = e_1
        \alpha_1 + \cdots + e_n \alpha_n$.

        In $V$, we have that $\gamma + \delta = (d_1 + e_1) \alpha_1 + \cdots +
        (d_n + e_n) \alpha_n$.

        By our definition of $T$, we have

        \begin{align*}
          T(\gamma + \delta) =&(d_1 + e_1) \beta_1 + \cdots + (d_n + e_n) \beta_n \\
          =&(d_1 \beta_1 + \cdots + d_n \beta_n) + (e_1 \beta_1 +
          \cdots + e_n \beta_n) \\
          =&T(\gamma) + T(\delta)
        \end{align*}

        {\bf Check}: $T(c\gamma) = cT(\delta)$

        So such a tranformation $T$ exists. Additionally by part (1), it is
        unique.

      \end{enumerate}
    }

    Let $T: V \to W$ be a linear transformation.

    \definition{
      $\range(T) = \{ T(\alpha) : \alpha \in V\} \subseteq W$ is the set of all
      vectors in $W$ hit by $T$.

      {\bf Fact}: $\range(T)$ is a {\bf subspace} of $W$.

      \begin{enumerate}
        \item $0$ is in it. This is because $T(0) = 0$, obviously.
        \item {\bf Combinations of $\alpha_i$ are in it}

          Say that $\beta_1, \beta_2 \in \range(T)$. \btw {must show that $\beta_1 +
          \beta_2 \in \range(T)$ }

          Since $\beta_1 \in \range(T)$, there is some $\alpha_1 \in V$ such that

          \[
            T(\alpha_1) = \beta_1
          \]

          similarly for $\beta_2$. Now $T(\alpha_1 + \alpha_2) = T(\alpha_1) +
          T(\alpha_2) = \beta_1 + \beta_2$, since $T$ is linear. So $T(\alpha_1 +
          \alpha_2) = \beta_1 + \beta_2$ so $\beta_1 + \beta_2 \in \range(T)$ \btw{
          since $\alpha_1, \alpha_2 \in V$ means that $\alpha_1 + \alpha_2 \in V$,
          because it's a vector space! }

        \item {\bf Scaling Works:}
          Say $\beta \in \range(T)$, and $c \in F$.  Chose $\alpha \in V$ such that
          $T(\alpha) = \beta$. Then $T(c\alpha) = cT(\beta) c \beta$, therefore
          $c\alpha \in \range(T)$.
      \end{enumerate}

      In other books this space is also called the \textbf{image} of $T$.
    }

    % \date{Fri. Feb 17 2023}
    %
    % Recall $T: V \to W$ is a linear transformation over $F$
    %
    % see grayson for notes today

    
  \definition{
    The \textbf{Null Space} of $T: V \to W$ is the set

    \[
      \Null(T) = \{\alpha \in V \vert T(\alpha) = \vec{0}\}
    \]

    \btw{
      In other words, this is the set of all vectors $\alpha$ in $V$ that, after
      a transformation $T$ is applied, go to $\vec{0}$. Note that $\vec{0}$ here
      is the zero of the vector space $W \subseteq V$.
    }

    This is also sometimes called the \textbf{Kernel} of $T$.
  }

  \thm{
    Let $T: V \to W$ be a linear transformation. $\Null(T)$ is a subspace of $V$.
  }
  {
    Let $\alpha, \beta \in \Null(T)$ and $c \in F$. Then,
    \[
      T(c\alpha + \beta) = cT(\alpha) + T(\beta) = c\vec{0} + \vec{0} = \vec{0}
      \Ra c\alpha + \beta \in \Null(T)
    \]
  }

  It's pretty easy to see from this (and it should make sense) that the Null
  Space for a transformation $T$ is itself a vector space.

  \definition{
    The \textbf{Nullity} of $T$ is the dimension of the Null space of $T$.
  }

  \newpage

  \definition{
    The \textbf{Rank} of $T$ is the dimension of $\range(T)$. Is this is equal
    to the dimension of $W$, $T$ is said to have \textbf{full rank}.
  }

  Note again that this comes back to our definition of $W$ for our
  transformation $T$. Earlier, we saw that $W$ was the {\it codomain} of $T$. If
  you think about how functions behave, this is like having a {\it surjective}
  function.

  \example{
    Let $\P_2$ be the set of all polynomials of degree 2 or less over a field
    $F$. Then, we have $\dim(\P_2)= 3$.
    
    Consider the linear transformation $D: \P_2 \to \P_2$, the differentiation
    operator. Then
    
    \[
      \range(D) = \Span(\{D(1), D(x), D(x^2)\}) = \Span(\{1, 2x\}) \Rightarrow \rank(D) = 2
    \]

    In other words, the Range of $D$ is the Span of a basis of $\P_2$ (in this
    case $\{1, x, x^2\}$) after being evaluated through $D$, so $\{1, 2x\}$. So
    the rank of $D$ here is $2$.

    For the Null Space of $D$, we have that

    \[
      \Null(D) = \{c \in F\} \Rightarrow \nullity(D) = 1
    \]

    The Null Space is the set of all constant functions since those are the
    function that, on $D$, go to $\vec{0}$.
  }

  \subsection{The Rank-Nullity Theorem}
  \namedtheorem{Rank-Nullity Theorem}{
    Let $V$ be a vector space with $\dim V = n$. Let $T: V \to W$.
    \[ \rank(T) + \nullity(T) = \dim V = n \] 
  }{
    First, choose $\{\constants{\alpha}{k}\}$ to be a basis for $\Null(T)$. This
    set is necessarily linearly independent in $V$. So, we can choose an
    additional $\{\alpha_{k+1}, \cdots, \alpha_n\}$ so that
    $\{\constants{\alpha}{n}\}$ is a basis of $V$.
    
    Certainly, $k \leq n$, since $\Null(T)$ is a subspace of $V$.
    
    We claim $A = \{T(\alpha_{k+1}), \cdots, T(\alpha_{n})\}$ is a basis for
    $\range(T)$. From this we have our theorem.
    
    Clearly, $A \subseteq \range(T)$. We also have, that since
    $\{\constants{\alpha}{n}\}$ is a basis of $V$, $\{T(\alpha_i)\}$ spans
    $\range(T)$.
    
    However, $T(\alpha_1) = T(\alpha_2) = \cdots = T(\alpha_k) = \vec{0}$, since
    they are in the null space, and hence do not contribute to the span. Thus,
    $A$ spans $\range(V)$. Now we need only show $A$ is \lind. We choose
    constants such that

    \[
      c_{k+1}T(\alpha_{k+1}) + \cdots + c_nT(\alpha_n) = \vec{0}
    \]

    Let 

    \[
      \alpha^* = c_{k+1}\alpha_{k+1} + \cdots + c_n\alpha_n \in V
    \]

    We then have
    
    \[
      T(\alpha^*) = c_{k+1}T(\alpha_{k+1}) + \cdots + c_nT(\alpha_n) = \vec{0} \Rightarrow \alpha^* \in \Null(T)
    \]

    So, we then have that, since $\alpha^*$ is in the null space,

    \[
      \alpha^* = \lincomb{d}{\alpha}{k} = c_{k+1}\alpha_{k+1} + \cdots + c_n\alpha_n
    \]

    \[
      \lincomb{d}{\alpha}{k} - c_{k+1}\alpha_{k+1} - \cdots - c_n\alpha_n = \vec{0} \in V
    \]

    But since $\{\constants{\alpha}{n}\}$ is a basis of $V$, all the constants
    are zero, and in particular all of the $c_i$ are zero. So,
    $\{T(\alpha_{k+1}), \cdots, T(\alpha_n)\}$ is linearly independent and is
    thus a basis of $\range(T)$.
  }

  Now that we have the rank-nullity theorem, we can analyze transformations and
  their matrices.

  \definition{
    Let $A$ be a matrix in $F^{m \times n}$.
    
    The \textbf{Column Space} is the vector space spanned by the $n$ columns of
    $A$. This is precisely $\range(T_A)$.
    
    The \textbf{Row Space} is the vector space spanned by the $m$ rows of $A$.
  }

  \thm{
    Let $A$ be a matrix, that when row-reduced has $n$ unknowns and $r$ non-zero
    rows. $\nullity(T_A) = n - r$
  }{
    This follows from the fact that elementary row operations preserve the row
    space, and that solving a linear system in $r$ equations with $n$ unknowns
    will have $n - r$ degrees of freedom.

    \TODO{} I guess I can believe this but some more info would be nice.
  }

  \note{
    Let $A$ be a matrix. Then the following are equal
    \begin{itemize}
      \item The dimension of the row space of $A$
      \item The dimension of the column space of $A$
      \item The number of nonzero rows in the row-reduced form of $A$
      \item $\rank(T_A)$	
    \end{itemize}

  }{
    This follows immediately from the above and the Rank-Nullity Theorem.
  }

  \date{Mon. Feb 20 2023}

  Suppose that $A$ is an $m \times n$ matrix. Now suppose that we row reduce
  $A$, let's call this matrix $A^{rr}$. Then we have that

  \[
    \RowSpace(A) = \RowSpace(A^{rr})
  \]

  And we know that $\rank(A)$ is the number of non-zero rows of $A^{rr}$ which
  we call $r$.

  Moreover, the solution set of the homogeneous system $A\vec{x} = \vec{0}$ has
  dimension $n - r$, where $n$ is the number of columns subtract the number of
  redundant equations.

  Now, we know that for a matrix $A$, there is an associated linear
  transformation $T_A: F^n \to F^m$.

  Last time, we also saw that 
  \begin{enumerate}
    \item $\range(T_A) = \ColSpace(A)$, 
    \item $\Null(T_A)$ is the solution set of $A \vec{x} = \vec{0}$.
  \end{enumerate}

  Now we can put everything together. Recall the Rank-Nullity theorem, then we
  have that, for any linear transformation $T_A$,

  \begin{enumerate}
    \item $\rank(T_A) + \nullity(T_A) = \dim(F^n) = n$
    \item $\rank(T_A) := \dim(\range(T_A))$
    \item $\nullity(T_A) = \dim(\Null(A)) = n - r$, which is exactly the dimension of
      the set of all solutions to the homogeneous.
    \item Finally we have that
      \begin{align*}
        \rank(A) &= \dim(\RowSpace(A)) = \dim(\ColSpace(A)) \\
                 &= \dim(\RowSpace(A^{rr})) \\
                 &= \rank(T_A) \\
                 &= r
      \end{align*}
  \end{enumerate}

  Recall also that $\nullity(T_A) = \dim(\Null(T_A)) = n - r$.

  Consider a matrix $A$ where

  \[
    A = \begin{bmatrix}
      1 & 2 & 3 & 4 \\
      2 & 4 & 6 & 8 \\
      1 & 0 & 1 & 1
    \end{bmatrix}
  \]

  Then

  \[
    A^{rr} = \begin{bmatrix}
      1 & 0 & 1 & 1 \\
      0 & 1 & 1 & 3/2 \\
      0 & 0 & 0 & 0
    \end{bmatrix}
  \]

  Is the row reduced matrix.

  A basis for the row space of $A$ is
  \[
    \{(1, 0, 1, 1), (0, 1, 1, 1/3)\}
  \]

  but another is

  \[
    \{(1, 2, 3, 4), (1, 0, 1, 1)\}
  \]

  We have $T_A: \R^4 \to \R^3$, and $\rank(T_A) = 2$.

  Basis for $\range(T_A)$ equals the basis for $\text{Col Space}(A)$

  % Note: doing row reductions {\it affects} the columns, 

  There are many more linear transformations than the ones given by a matrix,
  for instance the derivative or integrals.

  Let $T: V \to W$ be a linear transformation. From here there are two questions
  we can now ask.

  \begin{enumerate}
    \item Is $T$ onto?

      It is if and only if $\range(T) = W$. We saw this earlier.
      In terms of dimension, this means that $\rank(T) = \dim(W)$.

      Note that here, $V, W$ must be {\bf finite dimensional}.

    \item Is $T$ one to one?

      This requires some more work.
  \end{enumerate}

  \thm{
    $T: V \to W$ is one to one if and only if $\Null(T) = \{\vec{0}\}$.

    \btw{In other words, the Null space must only contain the zero vector.}
  }
  {
    Assume that $T$ is one to one. We know that $T(\vec{0}_V) = \vec{0}_W$.
    Chose any $\alpha \in \Null(T)$, then $T(\alpha) = \vec{0}_W$, by definition
    of being in the Null Space. Since $T$ is one to one, $\alpha$ must equal
    $\vec{0}_V$. \\

    Now assume that $\Null(T)$ is just $\vec{0}_W$. To see that $T$ is one to
    one, chose any $\alpha, \alpha' \in V$, with $T(\alpha) = T(\alpha')$. Then
    $T(\alpha - \alpha') = T(\alpha) - T(\alpha')$ by linearity, but then since
    $\alpha = \alpha'$, $T(\alpha - \alpha') = \vec{0}$ so $T(\alpha - \alpha')$
    must be in the Null space of $T$, and since $\Null(T) = \{\vec{0}\}$, and
    $\alpha - \alpha' = 0$, so $\alpha = \alpha'$ and thus $T$ is one to one.
  }

  \definition{
    $T$ is called {\bf non-singular} if $T$ is one to one.

    This is just another term for something we already know.
  }

  \thm {
    Now suppose that $T: V \to W$ is a linear transformation with $\dim(V) =
    \dim(W)$. Then $T$ is one to one if and only if $T$ is onto.
  }
  {
    By the Rank-Nullity theorem from last time, we have that

    \[
      \rank(T) + \nullity(T) = \dim(V)
    \]

    Now, assume that $T$ is one to one, then $\nullity(T) = 0$, but then $\rank(T)
    = \dim(V) = \dim(W)$.

    Now conversely, assume that $T$ is onto. Then

    \[
      \rank(T) = \dim(W) = \dim(V)
    \]

    Therefore $\nullity(T) = 0$, and so $T$ is one to one.
  }

  We are now starting to get a pretty good understanding of linear
  transformations, but suppose that we now want to combine them.

  \subsection{Combining Linear Transformations}

  Say $T: V \to W$ and $U: W \to Y$ are linear transformations over $F$.

  \btw{ then $U \circ T: V \to Y$ is a function. }

  {\bf Check the following}:

  \begin{enumerate}
    \item $U \circ T$ is a linear transformation.

      \btw{
        You know how to do this, just check that they scale and add as we
        expect.
      }

    \item If both $T$ and $U$ are one to one, then the composition is also one
      to one.

    \item If both $T$ and $U$ are onto, the composition is also onto.
  \end{enumerate}

  \note{
    $T \circ U$ would {\bf not} be a linear transformation, assuming that $Y$
    and $V$ are not the same vector space.

    \btw {Linear transformations don't commute nicely like that.}
  }

  % \TODO{} Mention somewhere that a linear transformation is nothing but a
  % function, a nice function (additionally, its just a set too)

  Let's now look at $T$ again.

  \definition{
    A linear transformation $T: V \to W$ is called {\bf invertible} if there is
    a linear transformation $U: W \to V$ such that

    \begin{enumerate}
      \item $U \circ T: V \to V$ is the identity from $V$. In other words

        \[
          U(T(\alpha)) = \alpha
        \]

        For any $\alpha \in V$.

      \item $T \circ U: W \to W$

        \[
          T(U(\alpha)) = \alpha
        \]

        For any $\alpha \in W$.

    \end{enumerate}
  }

  \note {
    It might be interesting for you to prove that, if one of the above applies,
    the other automatically applies as well.
  }

  If $T$ is invertible, we call such a $U$ $T^{-1}$, the inverse
  transformation of $T$.

  \note{
    Inverse transformations are unique, if they exist.

    \btw{We didn't talk about this in class but it {\it has} to be true.}
  }

  {\bf Proposition}: If $T: V \to W$ is an {\it invertible} linear
  transformation if and only if $T$ is both one to one and onto.

  \note {
    If $T$ has an inverse, then it must be the case that $\dim(V) = \dim(W)$.

    If this is surprising, just consider that this follows from the fact that
    $T$ must be both one to one, and onto in order to have an inverse.
  }

\end{document}

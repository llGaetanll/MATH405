\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath, amsthm}
\usepackage{xcolor}
\usepackage[
    top=10mm,
    bottom=10mm,
    left=10mm,
    right=10mm,
    marginparwidth=0mm,
    marginparsep=0mm,
    headheight=15pt,
    centering,
    % showframe,
    includefoot,
    includehead
]{geometry}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{framed}
\usepackage{parskip}
\usepackage{cancel}
\usepackage{multicol}



% \input xypic (for commutative diagrams)
% \include{mssymb}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}

\def\A{{\mathbb A}}
\def\P{{\mathbb P}}
\def\N{{\mathbb N}}
\def\Z{{\mathbb Z}}
\def\Q{{\mathbb Q}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\F{{\mathbb F}}
\def\O{{\cal O}}
\let\sec\S
\let\S\relax
\def\S{{\mathfrak S}}
\def\g{{\mathfrak g}}
\def\p{{\mathfrak p}}
\def\h{{\mathfrak h}}
\def\n{{\mathfrak n}}
\def\v{{\mathfrak v}}
\def\m{{\mathfrak m}}
\def\a{{\alpha}}


\newcommand{\skipline}{\vspace{\baselineskip}}
\newcommand{\dis}{\displaystyle}
\newcommand{\noin}{\noindent}


% remove all paragraph indents
\setlength{\parindent}{0pt}



% Mathematical notation


\newcommand{\Span}{\mathrm{Span}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\nullity}{\mathrm{nullity}}
\newcommand{\longhookrightarrow}{\lhook\joinrel\relbar\joinrel\rightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\dbar}{\overline{\partial}}
\newcommand{\gequ}{\geqslant}
\newcommand{\lequ}{\leqslant}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Coker}{\mathrm{Coker}}
\newcommand{\Row}{\mathrm{Row}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\Id}{\mathrm{Id}}
% \newcommand{\mod}{\mathrm{mod }}
\newcommand{\un}{\underline}
\newcommand{\ov}{\overline}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\pr}{\prime}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\im}{\mathrm{Im}}



% ================= %
% Headers & Footers
% ================= %
\pagestyle{fancy}
\fancyhf{}
\lhead{{\bf MATH405 Practice Problems} Spring 2023}
\rhead{Gaetan Almela}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}

% colors used in proof boxes
\definecolor{proof_fg}{HTML}{ABABAB}
\definecolor{proof_bg}{HTML}{EDEDED}



% ================= %
%       Utils
% ================= %
\newcommand{\induction}[3]{
  \textbf{Base Case} #1 \\
  \textbf{Inductive Hypothesis} \\ #2 \\
  \textbf{Inductive Step} \\ #3
}



% Used to list all problems on homework
\newcommand{\problems}[1]{
  \medskip \noin
  {\bf Problems}

  #1

  \medskip{}
}

% restyle bar left of proofs
\renewenvironment{leftbar}[1][\hsize]
{%
    \def\FrameCommand
    {%
        {\color{proof_fg}\vrule width 3pt}%
        \hspace{0pt}%must no space.
        \fboxsep=\FrameSep\colorbox{proof_bg}%
    }%
    \MakeFramed{\hsize#1\advance\hsize-\width\FrameRestore}%
}
{\endMakeFramed}


% box solutions
\newenvironment{solution}{
  \begin{leftbar}

  \noin
  {\large \sc solution} \\
}
{

  \medskip
  \noin
  \textcolor{proof_fg}{$\blacksquare$}

  \end{leftbar}
}

% style a problem and solution
\newcommand{\prob}[3]{
  \bigskip \bigskip \noin
  \subsection{Problem #1}
  % {\Large\sc Problem #1}

  \medskip\noin
  #2

  \begin{solution}
    \noin
    #3
  \end{solution}
}

\renewcommand{\vec}[1]{
  {\bf #1}
}

% augmented matrices
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother


\def\B{\mathcal B}


\begin{document}
  
  This document compiles all homework problems from MATH405 in Spring2023. It
  should be used as practice, but it's important for the reader to understand
  the solutions.

  The reader is encouraged to attempt the problems before looking at the
  solutions, obviously.

  \tableofcontents

  \newpage

  \section{Homework 1}

  \vspace{0.7cm}

  \prob{1.3.2}
  {
    If

    \[
      A = \begin{bmatrix}
        3 & -1 & 2 \\
        2 &  1 & 1 \\
        1 & -3 & 0 \\
      \end{bmatrix}
    \]

    find all solutions to $AX = 0$ by row-reducing $A$.
  }
  {
    \begin{align*}
      A &=
      \begin{bmatrix}
        3 & -1 & 2 \\
        2 &  1 & 1 \\
        1 & -3 & 0 \\
      \end{bmatrix}
      \sim 
      \begin{bmatrix}
        1 & -2 & 1 \\
        2 &  1 & 1 \\
        1 & -3 & 0 \\
      \end{bmatrix}
      \sim 
      \begin{bmatrix}
        1 & -2 &  1 \\
        0 &  5 & -1 \\
        1 & -3 &  0 \\
      \end{bmatrix}
      \sim 
      \begin{bmatrix}
        1 & -2 &  1 \\
        0 &  5 & -1 \\
        0 & -1 & -1 \\
      \end{bmatrix} \\
      &\sim 
      \begin{bmatrix}
        1 & -2 &  1 \\
        0 &  6 &  0 \\
        0 & -1 & -1 \\
      \end{bmatrix}
      \sim 
      \begin{bmatrix}
        1 & -2 &  1 \\
        0 &  1 &  0 \\
        0 &  1 &  1 \\
      \end{bmatrix}
      \sim 
      \begin{bmatrix}
        1 & -2 &  1 \\
        0 &  1 &  0 \\
        0 &  0 &  1 \\
      \end{bmatrix}
      \sim 
      \begin{bmatrix}
        1 &  0 &  0 \\
        0 &  1 &  0 \\
        0 &  0 &  1 \\
      \end{bmatrix}
    \end{align*}

    Since $A \sim I$, the only vector $\vec{x}$ that maps to $\vec{0}$ is the
    $\vec{0}$ vector itself.
  }

  \prob{1.3.3}
  {
    If

    \[
      A = \begin{bmatrix}
        6 & -4 & 0 \\
        4 & -2 & 0 \\
       -1 &  0 & 3 \\
      \end{bmatrix}
    \]

    find all solutions to $A\vec{x} = 2\vec{x}$ and all solutions to $A\vec{x} =
    3\vec{x}$.
  }
  {
    Let $\vec{x}$ be such that $A\vec{x} = 3\vec{x}$, then we have that $A\vec{x}
    - 3\vec{x} = 0$ and so $(A - 3I)\vec{x} = 0$. Solving, we get


    \begin{align*}
      A - 3I &= \begin{bmatrix}
        6 - 3 & -4 & 0 \\
        4 & -2 - 3 & 0 \\
       -1 &  0 & 3 - 3 \\
      \end{bmatrix} 
      =
      \begin{bmatrix}
        3 & -4 & 0 \\
        4 & -5 & 0 \\
       -1 &  0 & 0 \\
      \end{bmatrix} \\
      &\sim
      \begin{bmatrix}
        3 & -4 & 0 \\
        1 & -1 & 0 \\
       -1 &  0 & 0 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        3 & -4 & 0 \\
        0 & -1 & 0 \\
       -1 &  0 & 0 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        0 &  0 & 0 \\
        0 &  1 & 0 \\
        1 &  0 & 0 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        1 &  0 & 0 \\
        0 &  1 & 0 \\
        0 &  0 & 0 \\
      \end{bmatrix}
    \end{align*}

    so we get

    \begin{align*}
      (A - 3I)\vec{x} &= \vec{0} \\
      \begin{bmatrix}
        1 &  0 & 0 \\
        0 &  1 & 0 \\
        0 &  0 & 0 \\
      \end{bmatrix}
      \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3 \\
      \end{bmatrix}
      &=
      \begin{bmatrix}
        0 \\
        0 \\
        0 \\
      \end{bmatrix}
    \end{align*}

    So we get that $x_1 = x_2 = 0$, and $x_3$ is free. In other words, $\vec{x}$
    is of the form

    \[
      \vec{x} = \begin{bmatrix}
        0 \\
        0 \\
        c \\
      \end{bmatrix}
    \]

    for $c \in \mathbb R$.

    Similarly for $A\vec{x} = 2\vec{x}$,
    \begin{align*}
      A - 2I &= \begin{bmatrix}
        6 - 2 & -4 & 0 \\
        4 & -2 - 2 & 0 \\
       -1 &  0 & 3 - 2 \\
      \end{bmatrix} 
      =
      \begin{bmatrix}
        4 & -4 & 0 \\
        4 & -4 & 0 \\
       -1 &  0 & 1 \\
      \end{bmatrix} \\
      &\sim
      \begin{bmatrix}
        1 & -1 & 0 \\
        1 & -1 & 0 \\
       -1 &  0 & 1 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        1 & -1 & 0 \\
        0 &  0 & 0 \\
       -1 &  0 & 1 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        1 & -1 & 0 \\
       -1 &  0 & 1 \\
        0 &  0 & 0 \\
      \end{bmatrix}
    \end{align*}

    So we get

    \begin{align*}
      (A - 2I)\vec{x} &= \vec{0} \\
      \begin{bmatrix}
        1 & -1 & 0 \\
       -1 &  0 & 1 \\
        0 &  0 & 0 \\
      \end{bmatrix}
      \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3 \\
      \end{bmatrix}
      &=
      \begin{bmatrix}
        0 \\
        0 \\
        0 \\
      \end{bmatrix}
    \end{align*}

    so we get that $x_1 - x_2 = 0$, and $-x_1 + x_3 = 0$, which entails that $x_1
    = x_2 = x_3$. In other words, $\vec{x}$ is of the form

    \[
      \vec{x} = \begin{bmatrix}
        c \\
        c \\
        c \\
      \end{bmatrix}
    \]

    for $c \in \mathbb R$.
  }

  \prob{1.5.3}
  {
    Find two different $2 \times 2$ matrices $A$ such that $A^2 = 0$ and $A \ne
    0$.
  }
  {
    Consider the matrices
    \[
      A = \begin{bmatrix}
        0 & c \\
        0 & 0 \\
      \end{bmatrix}
    \]
    for $c \in \mathbb R$. Then,
    \begin{align*}
      A^2 =&
      \begin{bmatrix}
        0 & c \\
        0 & 0 \\
      \end{bmatrix}
      \begin{bmatrix}
        0 & c \\
        0 & 0 \\
      \end{bmatrix} \\
      =&
      \begin{bmatrix}
        0 & 0 \\
        0 & 0 \\
      \end{bmatrix}
    \end{align*}
    by matrix multiplication.
  }


  \prob{1.5.6}
  {
    Discover whether
    \[
      A = \begin{bmatrix}
        1 & 2 & 3 & 4 \\
        0 & 2 & 3 & 4 \\
        0 & 0 & 3 & 4 \\
        0 & 0 & 0 & 4 \\
      \end{bmatrix}
    \]
    is invertible, and find $A^{-1}$ if it exists.
  }
  {
    \[
      A = \begin{bmatrix}
        1 & 2 & 3 & 4 \\
        0 & 2 & 3 & 4 \\
        0 & 0 & 3 & 4 \\
        0 & 0 & 0 & 4 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 2 & 3 & 4 \\
        0 & 0 & 3 & 4 \\
        0 & 0 & 0 & 4 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 2 & 0 & 0 \\
        0 & 0 & 3 & 4 \\
        0 & 0 & 0 & 4 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 2 & 0 & 0 \\
        0 & 0 & 3 & 0 \\
        0 & 0 & 0 & 4 \\
      \end{bmatrix}
      \sim
      \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 \\
      \end{bmatrix}
    \]

    Since $A \sim I$, $A$ is invertible,

    \begin{align*}
      &\begin{bmatrix}[cccc|cccc]
        1 & 2 & 3 & 4 & 1 & 0 & 0 & 0 \\ 
        0 & 2 & 3 & 4 & 0 & 1 & 0 & 0 \\ 
        0 & 0 & 3 & 4 & 0 & 0 & 1 & 0 \\ 
        0 & 0 & 0 & 4 & 0 & 0 & 0 & 1
      \end{bmatrix}
      \sim
      \begin{bmatrix}[cccc|cccc]
        1 & 0 & 0 & 0 & 1 & -1 & 0 & 0 \\ 
        0 & 2 & 3 & 4 & 0 & 1 & 0 & 0 \\ 
        0 & 0 & 3 & 4 & 0 & 0 & 1 & 0 \\ 
        0 & 0 & 0 & 4 & 0 & 0 & 0 & 1
      \end{bmatrix} \\
      \sim
      &\begin{bmatrix}[cccc|cccc]
        1 & 0 & 0 & 0 & 1 & -1 & 0 & 0 \\ 
        0 & 2 & 0 & 0 & 0 & 1 & -1 & 0 \\ 
        0 & 0 & 3 & 4 & 0 & 0 & 1 & 0 \\ 
        0 & 0 & 0 & 4 & 0 & 0 & 0 & 1
      \end{bmatrix}
      \sim
      \begin{bmatrix}[cccc|cccc]
        1 & 0 & 0 & 0 & 1 & -1 & 0 & 0 \\ 
        0 & 2 & 0 & 0 & 0 & 1 & -1 & 0 \\ 
        0 & 0 & 3 & 0 & 0 & 0 & 1 & -1 \\ 
        0 & 0 & 0 & 4 & 0 & 0 & 0 & 1
      \end{bmatrix} \\
      \sim
      &\begin{bmatrix}[cccc|cccc]
        1 & 0 & 0 & 0 & 1 & -1 & 0 & 0 \\ 
        0 & 1 & 0 & 0 & 0 & \frac{1}{2} & -\frac{1}{2} & 0 \\ 
        0 & 0 & 1 & 0 & 0 & 0 & \frac{1}{3} & -\frac{1}{3} \\ 
        0 & 0 & 0 & 1 & 0 & 0 & 0 & \frac{1}{4}
      \end{bmatrix}
    \end{align*}

    Thus we have

    \[
      A^{-1} = \begin{bmatrix*}[r]
        1 & -1 & 0 & 0 \\ 
        0 & 1/2 & -1/2 & 0 \\ 
        0 & 0 & 1/3 & -1/3 \\ 
        0 & 0 & 0 & 1/4
      \end{bmatrix*}
    \]
  }


  \prob{2.1.4}
  {
    Let $V$ be the set of all pairs $(x, y)$ of real numbers, and $F$ be the field
    of real numbers. Define

    \begin{align*}
      (x, y) + (x_1, y_1) &= (x + x_1, y + y_1) \\
                  c(x, y) &= (cx, y) \\
    \end{align*}

    Is $V$, with these operations, a vector space over the field of real numbers?
  }
  {
    Let $c = 0, v = (x_1, y_1)$, and $w = (x_2, y_2)$ with $y_1 \ne y_2$. Then
    $c\vec{v} = (cx_1, y_1) = (0, y_1) = \vec{0}$, and $c\vec{w} = (cx_2, y_2) =
    (0, y_2) = \vec{0}$. But then $\vec{0}$ is not unique, so this is not a vector
    space.
  }


  \prob{2.1.5}
  {
    On $R^n$, define two operations

    \begin{align*}
      \alpha \oplus \beta =&\alpha - \beta \\
           c \cdot \alpha =& -c \alpha
    \end{align*}

    The operations on the right are usual ones. Which of the axioms for a vector
    space are satisfied by $(R^n, \oplus, \cdot)$?
  }
  {
    Let $\alpha, \beta, \gamma \in V$. Now consider

    \begin{align*}
      &\alpha \oplus (\beta \oplus \gamma) \\
     =&\alpha - (\beta - \gamma) \\
     =&\alpha - \beta + \gamma
    \end{align*}

    and

    \begin{align*}
      &(\alpha \oplus \beta) \oplus \gamma \\
     =&(\alpha - \beta) - \gamma \\
     =&\alpha - \beta - \gamma
    \end{align*}

    So $V$ is not associative, and thus it is not a vector space.
  }


  \prob{2.1.7}
  {
    Let $V$ be the set of all pairs $(x, y)$ of real numbers, and $F$ be the field
    of real numbers. Define

    \begin{align*}
      (x, y) + (x_1, y_1) &= (x + x_1, 0) \\
                  c(x, y) &= (cx, 0) \\
    \end{align*}

    Is $V$, with these operations, a vector space over the field of real numbers?
  }
  {
    No. Consider the vector $\vec{v} = (x, y)$, $\vec{w_1} = (0, c_1)$ and
    $\vec{w_2} = (0, c_2)$ for $c_1, c_2 \in F$. Then, by definition of $+$ in
    $V$, $\vec{v} + \vec{w_1} = (x + 0, 0) = \vec{v} + \vec{w_2}$. Thus \vec{0} is
    not unique, and so $V$ is not a vector space.
  }


  \prob{2.2.1}
  {
    Which of the following sets of vectors $\alpha = (a_1, ..., a_n)$ in
    $R^n$ are subspaces of $R^n$ $(n \ge 3)$?

    \begin{enumerate}[label=$\alph*$)]
      \item all $\alpha$ such that $a_1 \ge 0$
      \item all $\alpha$ such that $a_1 + 3a_2 = a_3$
      \item all $\alpha$ such that $a_2 = a_1^2$
      \item all $\alpha$ such that $a_1a_2 = 0$
      \item all $\alpha$ such that $a_2$ is rational
    \end{enumerate}
  }
  {
    Let $W$ be the subspace.

    \begin{enumerate}[label=$\alph*$)]
      \item $W$ is not a subspace. Let $c \in R$ with $c < 0$ and $\alpha$ be
        such that $a_1 > 0$. Then $c \alpha = (ca_1, ..., ca_n)$ but since $a_1 >
        0$ and $c < 0$, $ca_1 < 0$ and thus $c\alpha \not\in \{ \alpha \}$ but
        then $\{ \alpha \}$ is not closed under scalar multiplication, and so it
        is not a subspace of $R^n$.
      \item $W$ is a subspace.
        \begin{enumerate}
          \item $0 \in W$

            The $\vec{0}$ vector is produced by letting all $a_i = 0$, then $a_3 =
            0 + 3(0) = 0 \in W$.

          \item $W$ is closed under addition

            Let $\alpha_1 = (a_{11}, a_{12}, ..., a_{1n}), \alpha_2 = (a_{21},
            a_{22}, ..., a_{2n}) \in V$, then $\alpha_3 = \alpha_1 + \alpha_2 =
            (a_{11} + a_{21}, a_{12} + a_{22}, a_{13} + a_{23})$ But $a_{33} =
            a_{13} + a_{23} = (a_{11} + a_{21}) + 3(a_{12} + a_{22})$ so $\alpha_3
            \in W$.

          \item $W$ is closed under scalar multiplication

            Let $\alpha = (a_1, a_2, a_3, ..., a_n)$ with $a_3 = a_1 + 3a_2$. Let
            $c \in R$, then $c\alpha = (ca_1, ca_2, ca_3, ..., ca_n)$ but $ca_3 =
            c(a_1 + 3a_2) = ca_1 + 3ca_2$ so $c\alpha \in W$

        \end{enumerate}

      \item $W$ is not a subspace. Recall that squaring is not a linear
        operation.
      \item $W$ is not a subspace. Consider $\alpha = (0, 1), \beta = (1, 0)$.
        Then $\alpha + \beta = (1, 1) \not \in W$ so $W$ is not closed under
        addition.
      \item This is not a subspace. Let $c \in R$ be irrational. Then $c \alpha =
        (ca_1, ca_2, ..., ca_n)$, but since $a_2$ is rational and $c$ is
        irrational, $ca_2$ is irrational and so $c\alpha \not\in \{\alpha\}$. Once
        again, $\{\alpha\}$ is not closed under scalar multiplication and so it is
        not a subspace of $R^n$.
    \end{enumerate}
  }


  \prob{2.2.2}
  {
    Let $V$ be the real vector space of all functions $f$ from $R$ into $R$. Which
    of the following sets of functions are subspaces of $V$?

    \begin{enumerate}[label=$\alph*$)]
      \item all $f$ such that $f(x^2) = f(x)^2$
      \item all $f$ such that $f(0) = f(1)$
      \item all $f$ such that $f(3) = 1 + f(5)$
      \item all $f$ such that $f(-1) = 0$
      \item all $f$ which are continuous
    \end{enumerate}
  }
  {
    \begin{enumerate}[label=$\alph*$)]
      \item Not closed under addition. Consider $f(x) = g(x) = x$, then $(f +
        g)(x) = f(x) + g(x) = 2x$.
      \item This is a subspace.
      \item This is not a subspace because it does not contain the \vec{0} vector.
        For instance, suppose that $f(5) = 0$, then $f(3) = 1$. Inversely, if
        $f(3) = 0$, then $f(5) = -1$, thus it is not a vector space.
      \item This is a subspace because $f$ at $-1$ will scale and add, so it is
        closed under addition and scalar multiplication. Additionally, the zero
        function is in $f$.
      \item This is fine. The $0$ function is continuous, and the sum of two
        continuous functions is continuous. Continuous functions can be scaled and
        remain continuous, thus this is a subspace.
    \end{enumerate}
  }

  \newpage

  \section{Homework 2}

  \prob{1}
  {
    Suppose that $W_1$ and $W_2$ are both subspaces of a vector space $V$. Let

    \[
      W_1 + W_2 := \{ \alpha + \beta : \alpha \in W_1 \text{ and } \beta \in W_2 \}
    \]

    and let $W_1 \cap W_2 = \{ \alpha \in V : \alpha \in W_1 \text{ and } \alpha
    \in W_2 \}$.

    \begin{enumerate}
      \item Prove that $W_1 + W_2$ is a subspace of $V$.
      \item Prove that $W_1 \cap W_2$ is a subspace of $V$.
      \item Prove that $W_1 \cap W_2$ is a subset (and hence a subspace) of $W_1
        + W_2$.
    \end{enumerate}
  }
  {
    \begin{enumerate}
      \item Let $\alpha^*, \beta^*$ be arbirary vectors in $W_1 + W_2$, with
        $\alpha^* = \alpha_1 + \beta_1$ and $\beta^* = \alpha_2 + \beta_2$,
        $\alpha_1, \alpha_2 \in W_1$, and $\beta_1, \beta_2 \in W_2$. Finally,
        let $c$ be arbitrary in $F$. Then 

        \begin{align*}
          c \alpha^* + \beta^*
         =&c (\alpha_1 + \beta_1) + (\alpha_2 + \beta_2) \\
         =&c\alpha_1 + c\beta_1 + \alpha_2 + \beta_2 \\
         =&c\alpha_1 + \alpha_2 + c\beta_1 + \beta_2 \\
         =&(c\alpha_1 + \alpha_2) + (c\beta_1 + \beta_2) \\
        \end{align*}

        but $(c\alpha_1 + \alpha_2) \in W_1$ since $W_1$ is a subspace and
        therefore closed. Similarly, $(c\beta_1 + \beta_2) \in W_2$. Thus
        $(c\alpha_1 + \alpha_2) + (c\beta_1 + \beta_2) \in W_1 + W_2$ and so it
        is closed under scalar multiplication and vector addition, and so it is
        a subspace.

      \item Let $W = W_1 \cap W_2$. Now let $\alpha, \beta \in W$, and $c \in
        F$, then by defintion of $W$, $\alpha, \beta \in W_1, W_2$, but then,
        $c\alpha + \beta \in W_1$ since $W_1$ is a vector space, and $c\alpha +
        \beta \in W_2$ for the same reason. Then $c\alpha + \beta \in W$ so $W$
        is closed under scalar multiplication and vector addition, and so it is
        a subspace.

      \item Let $W = W_1 \cap W_2$. Now let $\alpha \in W$, then by definition
        of $W$, $\alpha \in W_1$ and $\alpha \in W_2$. Since $W_2$ is a
        subspace, $\vec{0} \in W_2$, then $\alpha + \vec{0} \in W_1 + W_2$, so
        $W$ is a subset of $W_1 + W_2$.

    \end{enumerate}
  }

  \newpage

  \prob{2.2.7}
  {
    Let $W_1$ and $W_2$ be subspaces of a vector space $V$ such that the
    set-theoretic union of $W_1$ and $W_2$ is also a subspace. Prove that one of
    the spaces $W_i$ is contained in the other.
  }
  {
    % If this was not the case, $W_1 \cup W_2$ would not be closed so it would not
    % be a valid subspace.
    %
    % Suppose for the sake of contradiction that $W = W_1 \cup W_2$ is a subspace,
    % but that $W_1 \not\subset W_2$ and $W_2 \not\subset W_1$. Now let $\alpha
    % \in W_1$, $\alpha \not\in W_2$, and let $\beta \in W_2$, $\beta \not\in
    % W_1$. Then $\alpha + \beta \in W$.
    %
    % WLOG, suppose that $\alpha + \beta \in W_1$, then $\alpha + (-\alpha) +
    % \beta \in W_1$, so $\beta \in W_1$, which is a contradiction.
    %
    % \TODO{} this isn't what im trying to say

    Suppose for the sake of contradiction that neither $W_1$ nor $W_2$ is a
    subset of the other. Let $\alpha \in W_1$, and $\beta \in W_2$ be arbitrary.
    Then, $\alpha + \beta \in W_1 \cup W_2$, and WLOG, suppose that $\alpha +
    \beta \in W_1$. Since $\alpha \in W_1$, $(-\alpha) \in W_1$ so $\alpha +
    (-\alpha) + \beta \in W_1$. Then $\beta \in W_1$, but this is a
    contradiction, so WLOG, $W_1$ is contained in $W_2$.
  }


  \prob{2.2.8}
  {
    Let $V$ be the vector space of all functions from $\R$ into $\R$. Let $V_e$
    be the subset of even functions $f(-x) = f(x)$. Let $V_o$ be the subset of
    odd functions $f(-x) = -f(x)$.

    \begin{enumerate}
      \item Prove that $V_e$ and $V_o$ are subspaces.
      \item Prove that $V_e + V_o = V$.
      \item Prove that $V_e \cap V_o = 0$.
    \end{enumerate}
  }
  {
    \begin{enumerate}
      \item Let $f_1, f_2 \in V_e$ be arbitrary, and $c \in \R$, then

        \begin{align*}
          &(cf_1 + f_2)(-x) \\
         =&cf_1(-x) + f_2(-x) \\
         =&cf_1(x) + f_2(x) \\
         =&(cf_1 + f_2)(x)
        \end{align*}

        so $cf_1(x) + f_2(x) \in V_e$ and therefore $V_e$ is a subspace.

        Similarly for $V_o$, if $g_1, g_2 \in V_o$, then 

        \begin{align*}
          &(cg_1 + g_2)(-x) \\
         =&cg_1(-x) + g_2(-x) \\
         =&-cg_1(x) - g_2(x) \\
         =&-(cg_1(x) + g_2(x)) \\
         =&-(cg_1 + g_2)(x)
        \end{align*}

        $cg_1(x) + g_2(x) \in
        V_o$ since odd functions are closed under addition and scalar
        multiplication.

      \item Let $f$ be an arbirary function in $V$, now consider the two
        functions

        \begin{align*}
          g(x) &= \frac{f(x) + f(-x)}{2} \\
          h(x) &= \frac{f(x) - f(-x)}{2}
        \end{align*}

        Notice,
        \[
          g(x) = \frac{f(x) + f(-x)}{2} = \frac{f(-x) + f(x)}{2}
        \]
        so $g$ is even, and thus $g \in V_e$. Additionally,

        \[
          h(x) = \frac{f(x) - f(-x)}{2} = \frac{-f(-x) + f(x)}{2} = \frac{-(f(-x) - f(x))}{2}
        \]

        so $h$ is odd, and thus $h \in V_o$. Finally, we can see that

        \[
          g(x) + h(x) = \frac{f(x) + f(-x)}{2} + \frac{f(x) - f(-x)}{2} =
          \frac{f(x) + \cancel{f(-x)} + f(x) \cancel{- f(-x)}}{2} = f(x)
        \]

        Thus we can see that $V_e + V_o = V$.

      \item Let $f \in V_e$ be an arbitrary even function that isn't $\vec{0}$,
        and let $x \in \R$ be arbitrary. Then by definition of even functions,
        $f(-x) = -f(x) \ne f(x)$, so $f$ cannot be odd. Therefore, $f \not\in
        V_o$. Thus the only element of $V_e \cap V_o = \vec{0}$.
    \end{enumerate}
  }

  \prob{2.2.9}
  {
    Let $W_1$ and $W_2$ be subspaces of a vector space $V$ such that $W_1 + W_2
    = V$, and $W_1 \cap W_2 = \{0\}$. Prove that for each vector $\alpha$ in
    $V$, there are {\it unique} vectors $\alpha_1$ in $W_1$ and $\alpha_2$ in
    $W_2$ such that $\alpha = \alpha_1 + \alpha_2$.
  }
  {
    Suppose for the sake of contradiction that there is some $\beta_1 \in W_1$
    and $\beta_2 \in W_2$. Then we have

    \begin{align*}
      \alpha =&\alpha_1 + \alpha_2 = \beta_1 + \beta_2 \\
             =&(\alpha_1 - \beta_1) = (\beta_2 - \alpha_2) \\
    \end{align*}

    but $(\alpha_1 - \beta_1) \in W_1$ since it is closed, and $(\beta_2 -
    \alpha_2) \in W_2$ since it is also closed. But since $(\alpha_1 - \beta_1)
    = (\beta_2 - \alpha_2)$, $(\alpha_1 - \beta_1) \in W_1 \cap W_2$ and
    $(\beta_2 - \alpha_2) \in W_1 \cap W_2$ which is a contradiction.
  }

  \prob{2.3.1}
  {
    Prove that if two vectors are linearly dependent, one of them is a scalar
    multiple of the other.
  }
  {
    Suppose that $v, w \in V$ are linearly dependent. Then, by definition,

    \[
      c_1 v + c_2 w = 0
    \]

    For $c_1, c_2 \in F$ and, WLOG, assume that $c_1 \ne 0$. Then

    \begin{align*}
      c_1 v + c_2 w &= 0 \\
              c_1 v &= -c_2 w \\
                  v &= -\frac{c_2}{c_1} w
    \end{align*}

    So $w$ is a scalar multiple of $v$.
  }

  \newpage

  \prob{2.3.5}
  {
    Fund three vectors in $\R^3$ which are linearly dependent, and are such that
    any two of them are linearly independent.
  }
  {
    \[
      \begin{bmatrix}
        1 \\
        0 \\
        1
      \end{bmatrix}
      \,
      \begin{bmatrix}
        0 \\
        1 \\
        0
      \end{bmatrix}
      \,
      \begin{bmatrix}
        1 \\
        1 \\
        1
      \end{bmatrix}
    \]

    Work.
  }

  \prob{2.3.6}
  {
    Let $V$ be the vector space of all $2 \times 2$ matrices over the field $F$.
    Prove that $V$ has dimension $4$ by exhibiting a basis for $V$ which has
    four elements.
  }
  {
    Consider the following basis

    \[
      A = \begin{bmatrix}
        1 & 0 \\
        0 & 0 \\
      \end{bmatrix}
      \,
      B = \begin{bmatrix}
        0 & 1 \\
        0 & 0 \\
      \end{bmatrix}
      \,
      C = \begin{bmatrix}
        0 & 0 \\
        1 & 0 \\
      \end{bmatrix}
      \,
      D = \begin{bmatrix}
        0 & 0 \\
        0 & 1 \\
      \end{bmatrix}
    \]

    $\{A, B, C, D\}$ are observably linearly independent.
  }

  \prob{2.3.7}
  {
    Let $V$ be the vector space of Exercise $6$. Let $W_1$ be the set of
    matrices of the form

    \[
      \begin{bmatrix}
        x & -x \\
        y & x \\
      \end{bmatrix}
    \]

    And let $W_2$ be the set of matrices of the form

    \[
      \begin{bmatrix}
        a & b \\
        -a & c \\
      \end{bmatrix}
    \]

    \begin{enumerate}
      \item Prove that $W_1$ and $W_2$ are subspaces of $V$.
      \item Find the dimensions of $W_1, W_2, W_1 + W_2$, and $W_1 \cap W_2$.
    \end{enumerate}
  }
  {
    \begin{enumerate}
      \item Let $M_1, M_2 \in W_1$, and $c \in F$. Then

        \begin{align*}
           &cM_1 + M_2 \\
          =&c\begin{bmatrix}
            x_1 & -x_1 \\
            y_1 & x_1
          \end{bmatrix}
          +
          \begin{bmatrix}
            x_2 & -x_2 \\
            y_2 & x_2
          \end{bmatrix} \\
          =&\begin{bmatrix}
            cx_1 + x_2 & -(cx_1 + x_2) \\
            cy_1 + y_2 & cx_1 + x_2
          \end{bmatrix}
        \end{align*}

        So $(cM_1 + M_2) \in W_1$ and thus $W_1$ is a subspace of $W$. Similarly
        for $W_2$, let $N_1, N_2 \in W_2$, and $c \in F$. Then

        \begin{align*}
           &cN_1 + N_2 \\
          =&c\begin{bmatrix}
            a_1 & b_1 \\
           -a_1 & c_1
          \end{bmatrix}
          +
          \begin{bmatrix}
            a_2 & b_2 \\
           -a_2 & c_2
          \end{bmatrix} \\
        =&\begin{bmatrix}
           (ca_1 + a_2) & cb_1 + b_2 \\
          -(ca_1 + a_2) & cc_1 + c_2
          \end{bmatrix}
        \end{align*}

        So $(cN_1 + N_2) \in W_2$ and thus $W_2$ is a subspace of $W$.

      \item 
        \begin{enumerate}
          \item $\dim(W_1) = 2$
          \item $\dim(W_2) = 3$
          \item $\dim(W_1 + W_2) = 4$
          \item We know that $\dim(W_1 + W_2) = \dim(W_1) + \dim(W_2) - \dim(W_1
            \cap W_2)$, so $\dim(W_1 \cap W_2) = 1$.
        \end{enumerate}
    \end{enumerate}
  }

  \prob{2.3.10}
  {
    Let $V$ be a vector space over the field $F$. Suppose that there are a
    finite number of vectors $\alpha_1, ..., \alpha_r$ in $V$ which span $V$.
    Prove that $V$ is finite-dimensional.
  }
  {
    If there are a finite number of vectors that spam $V$, then a finite basis
    can be constructed for $V$, and so $V$ is finite-dimensional.
  }

  \prob{2.3.14}
  {
    Let $V$ be the set of real numbers. Regard $V$ as a vector space over the
    field of {\it rational} numbers, with the usual operations. Prove that this
    vector space is {\it not} finite dimensional.
  }
  {
    Let $B = \{\alpha_1, \alpha_2, ..., \alpha_n\}$ be a linearly independent
    set with $\alpha_i \in V$. Then $\Span(B)$ is in bijection with $\Q \times
    \Q \times \cdots \times \Q$. Since the finite cartesian product of countable
    sets is countable, $|\Span(B)|$ is countable. However since $\R$ is
    uncountable, $B$ cannot span $\R$.

    % Consider $a \alpha_i$ for some $a \in \Q$ and $1
    % \le i \le n$.
    %
    %
    % Since $\alpha_i \in \R$ is fixed, and $a \in \Q$,
    % $\{a\alpha_i\}$ is countable. 
    %
    %
    %
    % Since the union of countable sets is countable, $B$ is countable.
    %
    % However if this is the case, $B$ cannot span $\R$ since it is uncountable,
    % and thus is it not a basis. Therefore the vector space of $\R$ over $\Q$ is
    % not finite dimensional.
  }

  \newpage

  \section{Homework 3}

  \vspace{0.7cm}

  \prob{2.4.1}
  {
    Show that the vectors

    \begin{align*}
      &\alpha_1 =(1, 1, 0, 0), &\alpha_2 = (0, 0, 1, 1), \\
      &\alpha_3 =(1, 0, 0, 4), &\alpha_4 = (0, 0, 0, 2), \\
    \end{align*}

    Form a basis for $\R^4$. Find the coordinates of each of the standard basis
    vectors in the ordered basis $\{\alpha_1, \alpha_2, \alpha_3, \alpha_4\}$.
  }
  {
    $(1, 0, 0, 0) = \alpha_3 - 2\alpha_4$, $(0, 1, 0, 0) = \alpha_1 - \alpha_3 +
    2\alpha_4$, $(0, 0, 1, 0) = \alpha_2 - \alpha_4 / 2$, and finally $(0,
    0, 0, 1) = \frac{1}{2}\alpha_4$.

    Since the standard basis of $\R^4$ can be expressed from $\{\alpha_1,
    \alpha_2, \alpha_3, \alpha_4\}$, they must form a basis of $\R^4$.
  }

  \prob{2.4.2}
  {
    Find the coordinate matrix of the vector $(1, 0, 1)$ in the basis of $\C^3$
    consisting of the vectors

    \begin{multicols}{3}
      \[
        (2i, 1, 0)
      \]

      \[
        (2, -1, 1)
      \]

      \[
        (0, 1 + i, 1 - i)
      \]
    \end{multicols}

    In that order.
  }
  {
    We have

    \begin{align*}
      &\begin{bmatrix}
        2i & 2 & 0 \\
        1 & -1 & i + 1 \\
        0 & 1 & 1 - i \\
      \end{bmatrix}^{-1}
      \begin{bmatrix}
        1 \\ 0 \\ 1
      \end{bmatrix} \\
     =\left(\frac{1 + i}{4}\right)
     &\begin{bmatrix}
        -2i & -2 - 2i & -2 + 2i \\
        -1 - i & -2 + 2i & 2 + 2i \\
        i & 2 & 2 - 2i \\
      \end{bmatrix}
      \begin{bmatrix}
        1 \\ 0 \\ 1
      \end{bmatrix} \\
    =&\begin{bmatrix}
        -\frac{1}{2} - \frac{i}{2} \\
        \frac{i}{2} \\
        \frac{3}{4} - \frac{i}{4}
      \end{bmatrix}
    \end{align*}


  }

  \newpage

  \prob{2.4.4}
  {
    Let $W$ be the subspace of $\C^3$ spanned by $\alpha_1 = (1, 0, i)$ and
    $\alpha_2 = (1 + i, 1, -1)$.

    \begin{enumerate}[label=(\alph*)]
      \item Show that $\alpha_1$ and $\alpha_2$ form a basis for $W$.
      \item Show that the vector $\beta_1 = (1, 1, 0)$ and $\beta_2 = (1, i, 1 +
        i)$ are in $W$ and form another basis for $W$.
      \item What are the coordinates of $\alpha_1$ and $\alpha_2$ in the ordered
        basis $\{\beta_1, \beta_2\}$ for $W$?
    \end{enumerate}
  }
  {
    \begin{enumerate}[label=(\alph*)]
      \item 
        % Let $\beta = (a, b, c) \in W$ be arbitrary. Then $\beta$ can be
        % written as a lineary combination of $\alpha_1$ and $\alpha_2$, so
        % $\{\alpha_1, \alpha_2\}$ spans $W$.
        %
        % Secondly let any $c_1, c_2 \in \C$, then
        %
        % \[
        %   c_1 \alpha_1 + c_2 \alpha_2 = (0, 0, 0)
        % \]
        %
        % implies that $c_1 = c_2 = 0$.
        %
        % Thus $\{\alpha_1, \alpha_2\}$ is linearly independent.
        %
        % \TODO{} that's the outline lol

        \[
          c_1 \begin{bmatrix} 1 \\ 0 \\ i \end{bmatrix}
          + c_2 \begin{bmatrix} i + i \\ 1 \\ -1 \end{bmatrix}
          = \begin{bmatrix}
            c_1 + c_2 + c_2 i \\
                  c_2         \\
            c_1 i - c_2
          \end{bmatrix}
          =
          \begin{bmatrix}
            0 \\ 0 \\ 0
          \end{bmatrix}
        \]

        So $c_2 = 0$, but then $c_1 i - c_2 = 0$ implies that $c_1 i = 0$, so
        $c_1 = 0$. Therefore $\alpha_1, \alpha_2$ are linearly independent and
        form a basis.

      \item $\beta_1 = \alpha_2 - i \alpha_1$, and $\beta_2 = (2 - i) \alpha_1 + i \alpha_2$

        Since $\beta_1$, and $\beta_2$ can be written as a non-trivial linear
        combination of $\alpha_1$ and $\alpha_2$, they themselves must be
        linearly independent, and thus form a basis.

      \item We want to solve

        \[
          \begin{bmatrix}
            1 & 1 \\
            1 & i \\
            0 & 1 + i
          \end{bmatrix}
          \begin{bmatrix}
            c_1 \\ c_2
          \end{bmatrix}
          =
          \begin{bmatrix}
            1 \\ 0 \\ i
          \end{bmatrix}
        \]

        Here, $c_1 = \frac{1}{2} - \frac{i}{2}, c_2 = \frac{i}{2} + \frac{1}{2}$
        work. And

        \[
          \begin{bmatrix}
            1 & 1 \\
            1 & i \\
            0 & 1 + i
          \end{bmatrix}
          \begin{bmatrix}
            c_1 \\ c_2
          \end{bmatrix}
          =
          \begin{bmatrix}
            1 + i \\ 1 \\ -1
          \end{bmatrix}
        \]

        Here, $c_1 = \frac{3}{2} - \frac{i}{2}, c_2 = \frac{i}{2} - \frac{1}{2}$
        work.
    \end{enumerate}
  }

  \prob{2.4.6}
  {
    Let $V$ be the vector space over the complex numbers of all functions from
    $\R$ into $\C$, i.e., the space of all complex-valued functions on the real
    line. Let $f_1(x) = 1, f_2(x) = e^{ix}, f_3(x) = e^{-ix}$.

    \begin{enumerate}[label=(\alph*)]
      \item Prove that $f_1, f_2$, and $f_3$ are linearly independent.
      \item Let $g_1(x) = 1$, $g_2(x) = \cos x$, $g_3(x) = \sin x$. Find an
        invertible $3 \times 3$ matrix $P$ such that

        \[
          g_j = \sum_{i = 1}^3 P_{ij} f_i 
        \]
    \end{enumerate}
  }
  {
    \begin{enumerate}[label=(\alph*)]
      \item consider

        \[
          c_1 f_1(x) + c_2 f_2(x) + c_3 f_3(x) = 0
        \]

        For $x = 0, 1, \pi$, we have

        \begin{align*}
          c_1 + c_2 e^{i \cdot 0} + c_3 e^{-i \cdot 0} =&0 \\
          c_1 + c_2 e^{i} + c_3 e^{-i} =&0 \\
          c_1 + c_2 e^{i\pi} + c_3 e^{-i\pi} =&0
        \end{align*}

        Which is equivalent to

        \begin{align*}
          &\begin{bmatrix}
            1 & 1 & 1 \\
            1 & e^i & e^{-i} \\
            1 & e^{-i\pi} & e^{i\pi}
          \end{bmatrix}
          \begin{bmatrix}
            c_1 \\
            c_2 \\
            c_3
          \end{bmatrix}
          =
          \begin{bmatrix}
            0 \\
            0 \\
            0
          \end{bmatrix} \\
        =&\begin{bmatrix}
            1 & 1 & 1 \\
            1 & e^i & e^{-i} \\
            1 & -1 & -1
          \end{bmatrix}
          \begin{bmatrix}
            c_1 \\
            c_2 \\
            c_3
          \end{bmatrix}
          =
          \begin{bmatrix}
            0 \\
            0 \\
            0
          \end{bmatrix}
        \end{align*}

        And we have that

        \begin{align*}
          &\begin{bmatrix}
            1 & 1 & 1 \\
            1 & e^i & e^{-i} \\
            1 & -1 & -1
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            1 & 1 & 1 \\
            1 & e^i & e^{-i} \\
            2 & 0 & 0
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            1 & 1 & 1 \\
            1 & e^i & e^{-i} \\
            1 & 0 & 0
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            0 & 1 & 1 \\
            0 & e^i & e^{-i} \\
            1 & 0 & 0
          \end{bmatrix} \\
          \sim
          &\begin{bmatrix}
            0 & 1 & 1 \\
            0 & e^{2i} & 1 \\
            1 & 0 & 0
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            0 & 1 - e^{2i} & 0 \\
            0 & e^{2i} & 1 \\
            1 & 0 & 0
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            0 & e^{2i} & 1 \\
            0 & 1 - e^{2i} & 0 \\
            1 & 0 & 0
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            0 & e^{2i} & 1 \\
            0 & 1 & 0 \\
            1 & 0 & 0
          \end{bmatrix} \\
          \sim
          &\begin{bmatrix}
            0 & e^{2i} & 1 \\
            0 & e^{2i} & 0 \\
            1 & 0 & 0
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            0 & 0 & 1 \\
            0 & e^{2i} & 0 \\
            1 & 0 & 0
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            0 & 0 & 1 \\
            0 & 1 & 0 \\
            1 & 0 & 0
          \end{bmatrix}
        \end{align*}

        So $\{f_1, f_2, f_3\}$ are linearly independent.

      \item 
        \[
          \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1/2 & -i/2 \\
            0 & 1/2 & i/2 \\
          \end{bmatrix}
        \]
    \end{enumerate}
  }

  \prob{2.4.7}
  {
    Let $V$ be the real vector space of all polynomial functions from $\R$ into
    $\R$ of degree $2$ or less, i.e., the space of all functions $f$ of the form

    \[
      f(x) = c_0 + c_1 x + c_2 x^2
    \]

    Let $t$ be a fixed real number and define

    \begin{multicols}{3}
      \[
        g_1(x) = 1
      \]

      \[
        g_2(x) = x + t 
      \]

      \[
        g_3 (x) = (x + t)^2
      \]
    \end{multicols}

    Prove that $\B = \{g_1, g_2, g_3\}$ is a basis for $V$. If

    \[
      f(x) = c_0 + c_1 x + c_2 x^2
    \]

    What are the coordinates of $f$ in this ordered basis $\B$?
  }
  {
    Let $\B' = \{1, x, x^2\}$ be the standard basis for all polynomials of degree
    $2$ or less. Then $[g_1]_{\B'} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$,
    $[g_2]_{\B'} = \begin{bmatrix} t \\ 1 \\ 0 \end{bmatrix}$, and finally
    $[g_3]_{\B'} = \begin{bmatrix} t^2 \\ 2t \\ 1 \end{bmatrix}$. Now

    \[
      P = \Big[[g_1]_{\B'}, [g_2]_{\B'}, [g_3]_{\B'}\Big] = \begin{bmatrix}
        1 & t & t^2 \\
        0 & 1 & 2t \\
        0 & 0 & 1
      \end{bmatrix}
    \]

    Since $P \sim I$, we see that all $[g_i]_{\B'}$ are linearly independent and
    so they form a basis.

    To find the coordinates of $f(x)$ in $\B$, simply compute

    \[
      P^{-1} \cdot \begin{bmatrix}
        c_0 \\ c_1 \\ c_2
      \end{bmatrix}
    \]
  }

  \prob{3.1.1}
  {
    Which of the following functions $T$ from $\R^2$ into $\R^2$ are linear
    transformations?

    \begin{enumerate}[label=(\alph*)]
      \item $T(x_1, x_2) = (1 + x_1, x_2)$
      \item $T(x_1, x_2) = (x_1, x_2)$
      \item $T(x_1, x_2) = (x_1^2, x_2)$
      \item $T(x_1, x_2) = (\sin x_1, x_2)$
      \item $T(x_1, x_2) = (x_1 - x_2, 0)$
    \end{enumerate}
  }
  {
    \begin{enumerate}[label=(\alph*)]
      \item Not linear. Consider $T(0) + T(0) = (2, 0) \ne 0$
      \item Linear. This is the identity transformation
      \item Not linear. $T(c\alpha + \beta) = ((c\alpha_1 + \beta_1)^2, \alpha_2
        + \beta_2) \ne cT(\alpha) + t(\beta)$
      \item Not linear. $\sin (c_1 x_1 + x_2) \ne c_1 \sin x_1 + \sin x_2$
      \item Linear. $T(c\alpha + \beta) = (c\alpha_1 - \beta_1, 0) = (c\alpha_1,
        0) + (\beta_1, 0) = cT(\alpha) + T(\beta)$
    \end{enumerate}
  }

  \prob{3.1.2}
  {
    Find the range, rank, null space, and nullity for the zero transformation
    and the identity transformation on a finite-dimensional space $V$.
  }
  {
    {\bf Zero Transformation}
    \begin{enumerate}
      \item Range: The 0 vector
      \item Rank: 0
      \item Null Space: $V$
      \item Nullity: $\dim(V)$
    \end{enumerate}

    {\bf Identity Transformation}
    \begin{enumerate}
      \item Range: $V$
      \item Rank: Full, $\dim(V)$
      \item Null Space: The 0 vector
      \item Nullity: 0. Only the 0 vector
    \end{enumerate}
  }

  \prob{3.1.3}
  {
    Describe the range and the null space for differentiation transformation of
    Example $2$. Do the same for the integration transformation of Example $5$.
  }
  {
    {\bf Differentiation Transformation}
    \begin{enumerate}
      \item Range: The set of all polynomials $\mathcal P$.
      \item Null Space: The null space is all constant functions.
    \end{enumerate}

    {\bf Integration Transformation}
    \begin{enumerate}
      \item Range: The set of functions $g(x) = I(f)$ for which $g(0) = 0$,
        since by definition

        \[
          I(f) = \int_0^x f(t) dt
        \]

        And $\int_0^0 f(t) dt = 0$ for any function $f$.

      \item Null Space: The null space is only the function for which

        \[
          \int_0^x f(t) dt
        \]

        For all $x$, which is the zero function.
    \end{enumerate}
  }

  \prob{3.1.4}
  {
     Is there a linear transformation $T$ from $\R^3$ into $\R^2$ such that
     $T(1, -1, 1) = (1, 0)$ and $T(1, 1, 1) = (0, 1)$?
  }
  {
    % Yes, since $(1, -1, 1)$ and $(1, 1, 1)$ are linearly independent, and $(1,
    % 0)$ and $(0, 1)$ are also linearly indepent, there exists a linear
    % transformation between them.
    %
    % \TODO{} find it.

    \[
      \frac{1}{2}
      \begin{bmatrix}
        1 & -1 & 0 \\
        1 & 1 & 0 \\
      \end{bmatrix}
    \]

    Works. Notice

    \begin{multicols*}{2}
      \[
        \frac{1}{2}
        \begin{bmatrix}
          1 & -1 & 0 \\
          1 & 1 & 0 \\
        \end{bmatrix}
        \begin{bmatrix}
          1 \\ -1 \\ 1
        \end{bmatrix}
        =
        \begin{bmatrix}
          1 \\ 0
        \end{bmatrix}
      \]

      \[
        \frac{1}{2}
        \begin{bmatrix}
          1 & -1 & 0 \\
          1 & 1 & 0 \\
        \end{bmatrix}
        \begin{bmatrix}
          1 \\ 1 \\ 1
        \end{bmatrix}
        =
        \begin{bmatrix}
          0 \\ 1
        \end{bmatrix}
      \]
    \end{multicols*}

  }

  \prob{3.1.7}
  {
    Let $F$ be a subfield of the complex numbers and let $T$ be the function
    from $F^3$ into $F^3$ defined by

    \[
      T(x_1, x_2, x_3) = (x_1 - x_2 + 2x_3, 2x_1 + x_2, - x_1 - 2x_2 + 2x_3)
    \]

    \begin{enumerate}[label=(\alph*)]
      \item Verify that $T$ is a linear transformation.
      \item If $(a, b, c)$ is a vector in $F^3$, what are the conditions on
        $a, b$, and $c$ that the vector be in the range of $T$? What is the
        rank of $T$?
      \item If $(a, b, c)$ is a vector in $F^3$, what are the conditions on
        $a, b$, and $c$ that $(a, b, c)$ be in the null space of $T$? What is the
        nullity of $T$?
    \end{enumerate}
  }
  {
    \begin{enumerate}[label=(\alph*)]
      \item Let $\alpha, \beta \in V$ be arbitrary, and $c \in F$ be arbitrary.
        Then

        \begin{align*}
          T(c\alpha + \beta) =&\begin{bmatrix}
            (c\alpha_1 + \beta_1) - (c\alpha_2 + \beta_2) + 2(c\alpha_3 + \beta_3) \\
            2(c\alpha_1 + \beta_1) + (c\alpha_2 + \beta_2) \\
            -(c\alpha_1 + \beta_1) - 2(c\alpha_2 + \beta_2) + 2(c\alpha_3 + \beta_3)
          \end{bmatrix} \\
        =&\begin{bmatrix}
            c\alpha_1 + \beta_1 - c\alpha_2 - \beta_2 + 2c\alpha_3 + 2\beta_3 \\
            2c\alpha_1 + 2\beta_1 + c\alpha_2 + \beta_2 \\
            -c\alpha_1 - \beta_1 - 2c\alpha_2  -2\beta_2 + 2c\alpha_3 + 2\beta_3
          \end{bmatrix} \\
        =&c\begin{bmatrix}
            \alpha_1 - \alpha_2 + 2\alpha_3 \\
            2\alpha_1 + \alpha_2 \\
            -\alpha_1 - 2\alpha_2  + 2\alpha_3 
          \end{bmatrix}
          \begin{bmatrix}
            \beta_1 - \beta_2 + 2\beta_3 \\
            2\beta_1 + \beta_2 \\
            - \beta_1 - 2\beta_2 + 2\beta_3
          \end{bmatrix} \\
        =&cT(\alpha) + T(\beta)
        \end{align*}

        So $T$ is a linear transformation.

      \item We can express $T$ as the following matrix

        \begin{align*}
          \begin{bmatrix}
            1 & -1 & 2 \\
            2 & 1 & 0 \\
            -1 & -2 & 2
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            3 & 0 & 2 \\
            2 & 1 & 0 \\
            -1 & -2 & 2
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            3 & 0 & 2 \\
            2 & 1 & 0 \\
            3 & 0 & 2
          \end{bmatrix}
          \sim
          \begin{bmatrix}
            3 & 0 & 2 \\
            2 & 1 & 0 \\
            0 & 0 & 0
          \end{bmatrix}
        \end{align*}

        So the rank of $T$ is $2$.

      \item
        The conditions for $(a, b, c)$ to be in the null space of $T$ is the
        following

        \begin{align*}
          a =&b - 2c \\
          b =&-2a \\
          c =&b + \frac{1}{2}a
        \end{align*}
    \end{enumerate}
  }

  \prob{3.1.8}
  {
    Describe explicitly a linear transformation from $\R^3$ into $\R^3$ which
    has as its range the subspace spanned by $(1, 0, -1)$ and $(1, 2, 2)$.
  }
  {
    \[
      \begin{bmatrix}
        1 & 1 & 0 \\
        0 & 2 & 0 \\
        -1 & 2 & 0 \\
      \end{bmatrix}
    \]

    Works, but any arragement of the columns of the matrix above will also work.
  }

  \prob{4}
  {
    Suppose $V, W$ are vector spaces over a field $F$ and $T: V \to W$ is any
    linear transformation.

    \begin{enumerate}[label=(\alph*)]
      \item Suppose that $\alpha_1, \alpha_2, \alpha_3 \in V$ and $5 \alpha_1 +
        2 \alpha_2 + \alpha_3 = 0$. Prove that $\{T(\alpha_1), T(\alpha_2),
        T(\alpha_3)\}$ are linearly dependent.

      \item More generally, prove that if $\{\alpha_1, ..., \alpha_k\} \subseteq
        V$ is any linearly dependent set, then $\{T(\alpha_1), ...,
        T(\alpha_k)\}$ is linearly dependent in $W$.

      \item Prove that if $\{\beta_1, ..., \beta_k\} \subseteq W$ are linearly
        independent and $\alpha_1, ..., \alpha_k$ satisfy $T(\alpha_1) =
        \beta_1, ..., T(\alpha_k) = \beta_k$, then $\{\alpha_1, ..., \alpha_k\}$
        is linearly independent in $V$.

      \item Is the following statement True or False? If $\dim(V) = \dim(W)$,
        then for any basis $\B = \{\alpha_1, ..., \alpha_k\}$ of $V$, the set
        $\{T(\alpha_1), ..., T(\alpha_k)\}$ is a basis for $W$. Explain your
        reasoning.
    \end{enumerate}
  }
  {
    \begin{enumerate}[label=(\alph*)]
      \item $\{T(\alpha_1), T(\alpha_2), T(\alpha_3)\}$ is linearly dependent
        because $\{\alpha_1, \alpha_2, \alpha_3\}$ are linearly dependent.
        Simply notice, for example, that $\alpha_3$ can be produced from
        $-5\alpha_1 - 2\alpha_2$

        \[
          5\alpha_1 + 2\alpha_2 + \alpha_3 = 0 \Leftrightarrow -5\alpha_1 - 2\alpha_2 = \alpha_3
        \]

        Then, just apply $T$ and get

        \[
          T(5\alpha_1 + 2\alpha_2 + \alpha_3) = 5T(\alpha_1) + 2T(\alpha_2) +
          T(\alpha_3) = 0
        \]

        By properties of $T$. But then

        \[
          -5T(\alpha_1) - 2T(\alpha_2) = T(\alpha_3)
        \]

        So $\{T(\alpha_1), T(\alpha_2), T(\alpha_3)\}$ are linearly dependent.

      \item This simply follows from the properties of $T$. Consider any
        linearly dependent set $\{\alpha_1, \dots, \alpha_k\} \subseteq V$.
        Then by definition of linear dependence

        \[
          c_1 \alpha_1 + \cdots + c_k \alpha_k = 0
        \]

        With $c_1, c_2, \dots, c_k \in F$ not all $0$. Now

        \[
          T(c_1 \alpha_1 + \cdots + c_k \alpha_k) = T(0) = c_1T(\alpha_1) + \cdots +
          c_k T(\alpha_k) = 0
        \]

        By properties of $T$.

      \item This is the contrapositive of (b). For a proof of (b), see (b)

      \item {\bf False}: Consider $T(\alpha) = \vec{0}$, then for $\alpha_1 \ne
        \alpha_2 \in V$, $T(\alpha_1) = T(\alpha_2) = \vec{0}$.

    \end{enumerate}
  }

\end{document}


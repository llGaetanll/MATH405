\documentclass[12pt]{article}
\usepackage[
    top=10mm,
    bottom=10mm,
    left=10mm,
    right=10mm,
    marginparwidth=0mm,
    marginparsep=0mm,
    % headheight=15pt,
    centering,
    % showframe,
    includefoot,
    % includehead
]{geometry}
\usepackage{tikz-cd}

\renewcommand{\date}[1]{\underline{\bf #1}}

\def\eps{\varepsilon}
\def\range{\text{Range}}

\newcommand{\GCD}{\text{GCD}}
\def\RowSpace{\text{RowSpace}}
\def\ColSpace{\text{ColSpace}}
\def\trace{\text{trace}}
\def\Ann{\text{Ann}}
\def\sgn{\text{sgn}}
\def\B{\mathcal B}
\def\P{\mathcal P}

\def\Tinv{$T$-invariant}


% create a command for TODOs
\newcommand{\TODO}{\color{red}\textbf{TODO}\color{black}}

\newcommand*{\xdash}[1][3em]{\color{darkgray}\rule[0.5ex]{#1}{0.55pt}\color{black}}

% preamble
\input{preamble}


\begin{document}
  % \footnotesize

  \tableofcontents
  
  \newpage

  \date{Fri. Feb 10 2023}

  \section{Vector Spaces}

  Suppose that $V$ is a finite dimensional vector space over $F$, with $\dim(V)
  = n$.

  $V$ may have {\it many different} bases, we know that they all have the same
  size $n$.

  Say $\B = \{\alpha_1, ..., \alpha_n\}$ is a basis fix the ordering of $\B$.

  Fix the ordering of $\B$.

  \theorem {
    For any $\alpha \in V$, there is a unique $n$ tuple $(x_1, ..., x_n) \in F^n$
    such that

    \[
      \alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n
    \]
  }
  {
    Existence is immediate, since $\B$ is a basis, thus $\B$ spans $V$.

    {\bf Uniqueness}

    Say $\alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n$
    and $\alpha = y_1 \alpha_1 + \cdots + y_n \alpha_n$.

    Then we have that

    $x_1 \alpha_1 + \cdots + x_n \alpha_n - y_1 \alpha_1 + \cdots + y_n \alpha_n
    = 0$, so $(x_1 - y_1)\alpha_1 + \cdots + (x_n - y_n)\alpha_n = 0$

    But since $\{\alpha_1, ..., \alpha_n\}$ is linearly independent, all
    coefficients must be $0$.
  }

  What this means is that, for a vector space $V$, there is an associated
  mapping in $F^n$. Notice that we know nothing about the vectors $\alpha_i$.

  We define $[\alpha]_{\B}$ to be the {\it coordinates} of $\alpha$ with
  respect to $\B$.

  {\bf Check}: The mapping $\alpha \mapsto [\alpha]_{\B} \in F^n$ satisfies

  \begin{enumerate}
    \item One to one-ness
    \item Onto-ness
    \item "Additive", for any $\alpha, \beta \in V$, if $\alpha = x_1 \alpha_1
      + \cdots + x_n \alpha_n$ and $\beta = y_1 \alpha_1 + \cdots + y_n
      \alpha_n$. Then

      \[
        [\alpha + \beta]_{\B} = \begin{bmatrix}
          x_1 + y_1 \\
          x_2 + y_2 \\
          \vdots \\
          x_n + y_n \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          x_1 \\
          x_2 \\
          \vdots \\
          x_n \\
        \end{bmatrix}
        +
        \begin{bmatrix}
          y_1 \\
          y_2 \\
          \vdots \\
          y_n \\
        \end{bmatrix}
        =
        [\alpha]_{\B}
        +
        [\beta]_{\B}
      \]
    \item $[c\alpha]_{\B} + c[\alpha]_{\B}$
  \end{enumerate}

  There exists an {\it isomorphism} between $V$ and $F^n$.

  \example{
    Let $\P$ be the space of al polynomials. Let $f(x) = x^3$, and $g(x) =
    x^5$. Then, let

    \[
      V = \Span\{f, g\} = \{\text{all } ax^3 + bx^5 : a, b \in F\}
    \]

    then, $\dim(V) = 2$, since $f$ and $g$ are linearly independent.

    Typical $h(x) \in V$, say $h(x) = 10x^3 - 2x^5$.

    \[
      [h]_{\B} = \begin{bmatrix}
        10 \\
        -2
      \end{bmatrix}
    \]
  }

  \btw {
    $[h]_{\B}$ is the mapping of $h$ to $F^n$. \TODO{} is this right?
  }

  Now let $k(x) = 2x^3 + 4x^5$ and $l(x) = x^3 + 3x^5$. Since $k, l$ are
  linearly independent, they form another basis of $V$.

  \[
    \B' = \{k(x), l(x)\}
  \]

  \subsection{Change of Basis}

  Given $\B = \{\alpha_1, \dots, \alpha_n\}$, and $\B' = \{\alpha_1', ...,
  \alpha_n'\}$ bases for $V$.

  We want to describe the map going from $[\alpha]_{\B} \mapsto
  [\alpha]_{\B'}$.

  \btw{ We want to find $\text{The $\B$ coordinate of $\alpha$} \mapsto
  \text{the $\B'$ coordinate of $\alpha$}$}

  {\bf Step 1}.

  Compute the $\B$ coordinate of $\alpha_1', ..., \alpha_n'$, {\it old}
  coordinates of the {\it new} basis elements. \\

  {\bf Step 2}.

  For an $n \times m$ matrix

  \[
    P = \Big[ [\alpha_1']_{\B}, \dots, [\alpha_n']_{\B} \Big]
  \]

  {\bf Check}: for any $\alpha \in V$

  \[
    [\alpha]_{\B} = P [\alpha]_{\B'}
  \]

  {\bf Ans}: This is what we actually want

  \[
    [\alpha]_{\B'} = P^{-1} [\alpha]_{\B}
  \]

  \date{Mon. Feb 13 2023}

  \TODO{} Missing {\it some} info

  {\bf Want}: Describe the mapping $T: F^n \to F^n$

  \[
    T([\alpha]_{\B_\text{old}}) = [\alpha]_{\B'_{\text{new}}}
  \]

  \btw {
    If we switch the basis for some reason, we want to see what the new
    coordinates are.
  }

  {\bf To do this}: For each $\alpha_j'$, compute
  $[\alpha_j']_{\B_\text{old}}$. Let

  \[
    P = \Big[[\alpha_1']_{\B_\text{old}} \cdots [\alpha_n']_{\B_{\text{old}}}\Big]
  \]

  be an $n \times n$ matrix.

  {\bf Claim}: For any $\alpha \in V$

  \[
    P \cdot [\alpha]_{\B'_\text{new}} = [\alpha]_{\B_\text{old}}
  \]

  {\bf How?}

  \[
    P \cdot [\alpha_1']_{\B'_\text{new}} = P \cdot \begin{bmatrix}
      1 \\
      0 \\
      \vdots \\
      0
    \end{bmatrix}
    = [\alpha_1']_{\B_\text{old}}
  \]

  This is the $1^{st}$ column of $P$, and similarly for all columns.

  {\bf Thus}: For any $\alpha \in V$, 

  \[
    [\alpha]_\text{new} = P^{-1} \cdot [\alpha]_\text{old}
  \]

  \example {
    In practice, we have the following.

    $V = \Span(\{x^3, x^5\})$ subspace of $\P$, the set of all polynomials. Let
    $f(x) = x^3, g(x) = x^5, \B = \{x^3, x^5\}$. Let $h(x) = 10x^3 - 2x^5 \in V$.

    {\bf Question}: What are the coordinates of $h$ with respect to $\B$?

    {\bf Answer}:
    \[
      [h]_{\B} = \begin{bmatrix}
        10 \\
        -2
      \end{bmatrix}
    \]
  }

  Let's now see what happens when we create a new basis $\B'$.

  \example{
    Let $k(x) = 2x^3 + 5x^5$, $l(x) = x^3 + 3x^5$.

    Let $\B' = \{k(x), l(x)\} = \{2x^3 + 5x^5, x^3 + 3x^5\}$ be another basis
    of $V$, still with $\B = \{f(x), g(x)\} = \{x^3, x^5\}$.

    {\bf Question}: What are the coordinates of $h(x) = 10x^3 - 2x^5$ with
    respect to $\B'$ now?

    {\bf Answer}:

    Well we know that $[k(x)]_{\B} = \begin{bmatrix} 2 \\ 5 \end{bmatrix}$ and
    $[l(x)]_{\B} = \begin{bmatrix} 1 \\ 3 \end{bmatrix}$, these are just the
    coordinates of $k$, and $l$ with respect to $\B$.

    So now we can construct our $P$ matrix

    \[
      P = \Big[ [k(x)]_{\B}, [l(x)]_{\B} \Big] = \begin{bmatrix}
        2 & 1 \\
        5 & 3 \\
      \end{bmatrix}
    \]

    notice that $P$'s columns are constructed from $k(x)$ and $l(x)$,
    expressed in terms of our standard basis $\B$.

    {\bf Check}:

    \[
      P^{-1} = \begin{bmatrix}
        3 & -1 \\
        -5 & 2 \\
      \end{bmatrix}
    \]

    Then

    \[
      \begin{bmatrix}
        3 & -1 \\
        -5 & 2 \\
      \end{bmatrix}
      \cdot
      \begin{bmatrix}
        10 \\ -2
      \end{bmatrix}
      =
      \begin{bmatrix}
        32 \\ -54
      \end{bmatrix}
    \]

    {\bf This means}:

    \[
      h(x) = 32k(x) - 54l(x) = 10x^3 - 2x^5
    \]

    Which is what we expect.

  }

  \example {
    Let $V = \R^2$. Standard basis $\B = \{\eps_1, \eps_2\} = \{(1, 0), (0, 1)\}$

    \[
      [(5, 4)]_{\B} = \begin{bmatrix}
        5 \\ 4
      \end{bmatrix}
    \]

    Fix angle $\theta$, Let

    \[
      \B' = \{(\cos(\theta), \sin(\theta)), (-\sin(\theta), \cos(\theta))\}
    \]

    {\bf Question}: What is $\begin{bmatrix} 5 \\ 4
    \end{bmatrix}_{\B'_\text{new}}$?

    {\bf Answer}:

    \begin{enumerate}
      \item Form $P$

        \[
          [(\cos(\theta), \sin(\theta))]_{\B} = \begin{bmatrix} \cos(\theta)
          \\ \sin(\theta) \end{bmatrix}
        \]

        \[
          [(-\sin(\theta), \cos(\theta))]_{\B} = \begin{bmatrix} -\sin(\theta)
          \\ \cos(\theta) \end{bmatrix}
        \]

        Then 

        \[
          P = \begin{bmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
          \end{bmatrix}
        \]

        {\bf Fact}:

        \[
          P^{-1} = \begin{bmatrix}
            \cos(\theta) & \sin(\theta) \\
            -\sin(\theta) & \cos(\theta)
          \end{bmatrix}
        \]

        so we have

        \begin{align*}
          [(5, 4)]_{\B'_\text{new}} =&P^{-1}
          \begin{bmatrix} 5 \\ 4 \end{bmatrix} \\
          =&\begin{bmatrix}
            \cos(\theta) & \sin(\theta) \\
            -\sin(\theta) & \cos(\theta)
          \end{bmatrix}
          \begin{bmatrix} 5 \\ 4 \end{bmatrix} \\
          =&\begin{bmatrix}
            5\cos(\theta) & 4\sin(\theta) \\
            -5\sin(\theta) & 4\cos(\theta)
          \end{bmatrix}
        \end{align*}
    \end{enumerate}
  }

  \section{Linear Transformations}

  Say $V$, $W$ are both vector spaces over the same field $F$.
  
  \definition{
    A {\bf Linear Transformation} $T: V \to W$ is a function satisifying two
    rules

    \begin{enumerate}
      \item For all $\alpha, \beta \in V$,

        \[
          T(\alpha + \beta) = T(\alpha) + T(\beta)
        \]

        Note that the first $+$ is addition in $V$, but the second
        is addition in $W$.

      \item For all $\alpha \in V$ and $c \in F$,

        \[
          T(c\alpha) = cT(\alpha)
        \]
    \end{enumerate}
  }

  The book combines the two definitions above into one, like this,

  \[
    T(c\alpha + \beta) = cT(\alpha) + T(\beta)
  \]

  Let's quickly take some time to understand what $V$ and $W$ are here.
  Suppose we have a transformation $T: V \to W$, then $V$ is the {\bf
  domain} and $W$ is the {\bf codomain}.

  Here, $T$ is just a function, which means that it {\it must} use all of $T$,
  but it {\it does not} have to use all of $W$. For example, the following is
  a perfectly valid transformation.

  \example {
    Let $T: \P^3 \to \P^2$ be the transformation that takes all degree $3$
    polynomials to the space of degree $2$ polynomials, with

    \[
      T(f) = \vec{0}
    \]

    for all $f \in \P^3$.
  }

  Its obvious that there are more degree $2$ polynomials in the world than
  just the $\vec{0}$ polynomial. So here, we say that the $\range(T) =
  \{\vec{0}\}$, and that 

  \[
    \range(T) \subseteq W
  \]

  but maybe we are getting ahead of ourselves.

  \subsection{Basic Facts}

  Suppose that $T: V \to W$ is a linear transformation
  \begin{enumerate}
    \item $T(0) = 0$

      {\bf Proof}:

      \[
        T(0 + 0) = T(0) + T(0) = \vec{0} + \vec{0} = \vec{0}
      \]

      {\bf Note}: $0$ lives in the field, and $\vec{0}$ lives in $W$, the {\bf
      codomain} of the transformation $T$.

      \btw{Always be aware of where the $0$ lives}

    \item For all $\{\alpha_1, ..., \alpha_n\} \subseteq V$, all $\{c_1, ...,
      c_n\} \in F$,

      \[
        c_1 T(\alpha_1) + \cdots + c_n T(\alpha_n)
      \]

      {\bf Proof} Easy induction on $n$, just follows from part (2) of the
      definition.
  \end{enumerate}


  \subsection{Examples}

  Let's look at multiple examples of linear transformations to get an idea of
  how they behave.

  \example{
    We already know that each matrix $A$ has an associated linear
    transformation $T_A$. Let's look at this in more detail now.

    Let $A \in F^{m \times n}$ be an $m \times n$ matrix with entries from a
    field $F$.

    Then, let $T_A: F^n \to F^m$ be defined by

    \[
      T_A(\vec{x}) = A \vec{x}
    \]

    where $\vec{x}$ is a vector in $F^n$.

    Let's check that this is indeed a linear transformation.

    Chose any $\vec{x}, \vec{y} \in F^n$, then

    \begin{enumerate}
      \item \btw{Check that $T_A(\vec{x} + \vec{y}) = T_A(\vec{x}) + T_A(\vec{y})$}

        Let $\vec{x}, \vec{y} \in V$, then

        \[
          T_A(\vec{x} + \vec{y}) = A (\vec{x} + \vec{y}) = A \vec{x} + A \vec{y} = T_A(\vec{x}) + T_A(\vec{y})
        \]

        so this works as we expect.

      \item \btw{Check that $T_A(c\vec{x}) = cT_A(\vec{x})$ for $c \in F$.}

        let $c \in F$, then we have

        \[
          T_A(cX) = A \cdot (cX) = cAX = c T_A(X)
        \]

        which is also what we expect.
    \end{enumerate}

    so we have proved that $T_A$ is a linear transformation!
  }

  \example {
    Consider $\P$ the set of all polynomials $a_0 + a_1x + \cdots + a_n x^n$.

    Let's define $D: \P \to \P$ which takes a function $f \in \P$ to $f' \in
    \P$, where $f'$ is the {\it derivative} of $f$.

    \[
      D(f) = f'
    \]

    {\bf Claim}:

    $D$ is a linear transformation.

    {\bf Proof}: 

    Take two functions $f, g \in \P$, then by definition of $D$, we have

    \[
      D(f + g) = (f + g)' = f' + g' = D(f) + D(g)
    \]

    and for $c \in F$,

    \[
      D(cf) = (cf)' = c \cdot f' = c D(f)
    \]

    so the derivative is a linear transformation!
  }

  \example {
    Let $C(\R)$ be the set of all continuous functions $f: \R \to \R$.

    Let's define $I: C(\R) \to C(\R)$ which takes a function $f \in C(\R)$ to
    $F \in C(\R)$, where $F$ is the {\it antiderivative} of $f$.

    \[
      I(f) = \int_0^x f(t)dt
    \]

    \btw {
      Note that the integral exists because you can always integrate a
      continuous function.
    }

    The result is also continuous and differentiable by the Fundamental
    Theorem of Calculus.

    \[
      D(I(f)) = f
    \]

    Is the {\bf Fundamental Theorem of Calculus}.

    Therefore $I(f)$ really {\it is} continuous, $I(f) \in C(\R)$.

    {\bf Claim}:

    $I$ is a linear transformation.

    {\bf Proof}:

    Take two functions $f, g \in \P$, then by definition of $I$, we have

    \begin{align*}
      I(f + g) =&\int_0^x (f(t) + g(t))dt \\
               =&\int_0^x f(t)dt + \int_0^x g(t)dt \\
               =&I(f) + I(g)
    \end{align*}

    and

    \[
      I(cf) = \int_0^x cf(t)dt = c\int_0^x f(t)dt = cI(f)
    \]

    so the integral is a linear transformation!
  }

  \newpage

  \date{Fri. Feb 15 2023}

  Recall: A linear transformation $T: V \to W$ is a function between two
  vector spaces over the same field $F$, satisifying
  \begin{enumerate}
    \item For all $\alpha, \beta \in V$,

      \[
        T(\alpha + \beta) = T(\alpha) + T(\beta)
      \]

      Note that the first $+$ is addition in $V$, but the second
      is addition in $W$.

    \item For all $\alpha \in V$ and $c \in F$,

      \[
        T(c\alpha) = cT(\alpha)
      \]
  \end{enumerate}

  For all $\alpha_1,..., \alpha_k \in V$,  and $c_1, ..., c_k \in F$, it
  breaks nicely into

  \[
    T(c_1 \alpha_1 + \cdots + c_k \alpha_k) = c_1 T(\alpha_1) + \cdots + c_k
    T(\alpha_k)
  \]

  \example {
    $I^*: C(\R) \to \R$ (all continuous functions from $\R$ to $\R$)

    \[
      I^*(f) = \int_0^1 f(x)dx
    \]

    \[
      I^*(x^2) = \int_0^1 x^2 dx = \frac{x^3}{x} \Big|_0^1 = \frac{1}{3}
    \]

    Note that the output of $I*$ is just a number here. Additionally, $I^*$ is
    linear: you can split integrals up for polynomials, and you can take
    constants outside.
  }

  For any $V, W$, we also have

  \[
    X: V \to W
  \]

  Is the zero transformation. It takes any $\alpha \in V$ to the $0$ of $W$.
  We'll use this to prove theorems about linear transformations later.

  \theorem {
    Let's prove existence and uniqueness of linear transformations.

    \begin{enumerate}
      \item Linear Transformations $T: V \to W$ are {\bf determined} by their
        behavior on a basis $\B$ of $V$. More precisely,

        Suppose that $\B = \{\alpha_1, ..., \alpha_n\}$ is a basis for $V$ and
        suppose that $T, U: V \to W$ are both linear transformations (and they
        agree on a basis), such that

        \[
          T(\alpha_1) = U(\alpha_1), T(\alpha_2) = U(\alpha_2), ..., T(\alpha_n) =
          U(\alpha_n)
        \]

        Then $T = U$

      \item For {\bf any map} $T_0: \B \to W$, there s a unique linear
        transformation $T: V \to W$ with $T \supseteq T_0$. In other words,

        Let $\B = \{\alpha_1, ..., \alpha_n\}$ be {\bf any basis} for $V$
        and let $\beta_1, ..., \beta_n$ be {\bf any vectors} in $W$.

        Then, there is a {\bf unique} linear transformation $T: V \to W$ such
        that

        \[
          T(\alpha_1) = \beta_1, T(\alpha_2) = \beta_2, ..., T(\alpha_n) = \beta_n
        \]
    \end{enumerate}
  }
  {
    \begin{enumerate}
      \item {\bf Uniqueness}: Chose any $\alpha \in V$, since $\B$ is a basis,

      \btw{
          Will show that $T = U \Leftrightarrow$ For any $\alpha \in V$,
          $T(\alpha) = U(\alpha)$
      }

      \[
        \alpha = c_1 \alpha_1 + \cdots + c_n \alpha_n
      \]
      
      for some {\bf unique} $c_1, ..., c_n \in F$.

      Since $T$ is a linear transformation,

      \[
        T(\alpha) = c_1 T(\alpha_1) + \cdots + c_n T(\alpha_n)
      \]

      Likewise with $U$,

      \[
        U(\alpha) = c_1 U(\alpha_1) + \cdots + c_n U(\alpha_n)
      \]

      But, since $T(\alpha_1) = U(\alpha_1), ..., T(\alpha_n) = U(\alpha_n)$,
      $T(\alpha) = U(\alpha)$.

      \btw{
        Essentially, if $T, U$ work the same for all $\alpha_i$, then their
        sum will obviously be the same, and so they'll give the same result
        for the same $\alpha$.
      }

      Note that this theorem {\it still} works for infinite dimensional
      vector spaces.

    \item {\bf Existence}: Chose any $\alpha \in V$. \btw{ We must define
      $T(\alpha)$}

      Since $\B$ is a basis, we can write

      \[
        \alpha = c_1 \alpha_1 + \cdots + c_n \alpha_n
      \]

      which is unique.

      Define 

      \[
        T(\alpha) := c_1 \beta_1 + \cdots + c_n \beta_n \in W
      \]

      {\bf Check}: Is $T$ linear?

      Say $\gamma = d_1 \alpha_1 + \cdots + d_n \alpha_n$, $\delta = e_1
      \alpha_1 + \cdots + e_n \alpha_n$.

      In $V$, we have that $\gamma + \delta = (d_1 + e_1) \alpha_1 + \cdots +
      (d_n + e_n) \alpha_n$.

      By our definition of $T$, we have

      \begin{align*}
        T(\gamma + \delta) =&(d_1 + e_1) \beta_1 + \cdots + (d_n + e_n) \beta_n \\
        =&(d_1 \beta_1 + \cdots + d_n \beta_n) + (e_1 \beta_1 +
        \cdots + e_n \beta_n) \\
        =&T(\gamma) + T(\delta)
      \end{align*}

      {\bf Check}: $T(c\gamma) = cT(\delta)$

      So such a tranformation $T$ exists. Additionally by part (1), it is
      unique.

    \end{enumerate}
  }

  Let $T: V \to W$ be a linear transformation.

  \definition{
    $\range(T) = \{ T(\alpha) : \alpha \in V\} \subseteq W$ is the set of all
    vectors in $W$ hit by $T$.

    {\bf Fact}: $\range(T)$ is a {\bf subspace} of $W$.

    \begin{enumerate}
      \item $0$ is in it. This is because $T(0) = 0$, obviously.
      \item {\bf Combinations of $\alpha_i$ are in it}

        Say that $\beta_1, \beta_2 \in \range(T)$. \btw {must show that $\beta_1 +
        \beta_2 \in \range(T)$ }

        Since $\beta_1 \in \range(T)$, there is some $\alpha_1 \in V$ such that

        \[
          T(\alpha_1) = \beta_1
        \]

        similarly for $\beta_2$. Now $T(\alpha_1 + \alpha_2) = T(\alpha_1) +
        T(\alpha_2) = \beta_1 + \beta_2$, since $T$ is linear. So $T(\alpha_1 +
        \alpha_2) = \beta_1 + \beta_2$ so $\beta_1 + \beta_2 \in \range(T)$ \btw{
        since $\alpha_1, \alpha_2 \in V$ means that $\alpha_1 + \alpha_2 \in V$,
        because it's a vector space! }

      \item {\bf Scaling Works:}
        Say $\beta \in \range(T)$, and $c \in F$.  Chose $\alpha \in V$ such that
        $T(\alpha) = \beta$. Then $T(c\alpha) = cT(\beta) c \beta$, therefore
        $c\alpha \in \range(T)$.
    \end{enumerate}

    In other books this space is also called the \textbf{image} of $T$.
  }

  % \date{Fri. Feb 17 2023}
  %
  % Recall $T: V \to W$ is a linear transformation over $F$
  %
  % see grayson for notes today

    
  \definition{
    The \textbf{Null Space} of $T: V \to W$ is the set

    \[
      \Null(T) = \{\alpha \in V \vert T(\alpha) = \vec{0}\} \subseteq V
    \]

    \btw{
      In other words, this is the set of all vectors $\alpha$ in $V$ that, after
      a transformation $T$ is applied, go to $\vec{0}$. Note that $\vec{0}$ here
      is the zero of the vector space $W \subseteq V$.
    }

    This is also sometimes called the \textbf{Kernel} of $T$.
  }

  \theorem{
    Let $T: V \to W$ be a linear transformation. $\Null(T)$ is a subspace of $V$.
  }
  {
    Let $\alpha, \beta \in \Null(T)$ and $c \in F$. Then,
    \[
      T(c\alpha + \beta) = cT(\alpha) + T(\beta) = c\vec{0} + \vec{0} = \vec{0}
      \Ra c\alpha + \beta \in \Null(T)
    \]
  }

  It's pretty easy to see from this (and it should make sense) that the Null
  Space for a transformation $T$ is itself a vector space.

  \definition{
    The \textbf{Nullity} of $T$ is the dimension of the Null space of $T$.
  }

  \newpage

  \definition{
    The \textbf{Rank} of $T$ is the dimension of $\range(T)$. Is this is equal
    to the dimension of $W$, $T$ is said to have \textbf{full rank}.
  }

  Note again that this comes back to our definition of $W$ for our
  transformation $T$. Earlier, we saw that $W$ was the {\it codomain} of $T$. If
  you think about how functions behave, this is like having a {\it surjective}
  function.

  \example{
    Let $\P_2$ be the set of all polynomials of degree 2 or less over a field
    $F$. Then, we have $\dim(\P_2)= 3$.
    
    Consider the linear transformation $D: \P_2 \to \P_2$, the differentiation
    operator. Then
    
    \[
      \range(D) = \Span(\{D(1), D(x), D(x^2)\}) = \Span(\{1, 2x\}) \Rightarrow \rank(D) = 2
    \]

    In other words, the Range of $D$ is the Span of a basis of $\P_2$ (in this
    case $\{1, x, x^2\}$) after being evaluated through $D$, so $\{1, 2x\}$. So
    the rank of $D$ here is $2$.

    For the Null Space of $D$, we have that

    \[
      \Null(D) = \{c \in F\} \Rightarrow \nullity(D) = 1
    \]

    The Null Space is the set of all constant functions since those are the
    function that, on $D$, go to $\vec{0}$.
  }

  \subsection{The Rank-Nullity Theorem}
  \Theorem{
    Let $V$ be a vector space with $\dim V = n$. Let $T: V \to W$.
    \[ \rank(T) + \nullity(T) = \dim V = n \] 
  }
  {
    First, choose $\{\constants{\alpha}{k}\}$ to be a basis for $\Null(T)$. This
    set is necessarily linearly independent in $V$. So, we can choose an
    additional $\{\alpha_{k+1}, \cdots, \alpha_n\}$ so that
    $\{\constants{\alpha}{n}\}$ is a basis of $V$.
    
    Certainly, $k \leq n$, since $\Null(T)$ is a subspace of $V$.
    
    We claim $A = \{T(\alpha_{k+1}), \cdots, T(\alpha_{n})\}$ is a basis for
    $\range(T)$. From this we have our theorem.
    
    Clearly, $A \subseteq \range(T)$. We also have, that since
    $\{\constants{\alpha}{n}\}$ is a basis of $V$, $\{T(\alpha_i)\}$ spans
    $\range(T)$.
    
    However, $T(\alpha_1) = T(\alpha_2) = \cdots = T(\alpha_k) = \vec{0}$, since
    they are in the null space, and hence do not contribute to the span. Thus,
    $A$ spans $\range(V)$. Now we need only show $A$ is \lind. We choose
    constants such that

    \[
      c_{k+1}T(\alpha_{k+1}) + \cdots + c_nT(\alpha_n) = \vec{0}
    \]

    Let 

    \[
      \alpha^* = c_{k+1}\alpha_{k+1} + \cdots + c_n\alpha_n \in V
    \]

    We then have
    
    \[
      T(\alpha^*) = c_{k+1}T(\alpha_{k+1}) + \cdots + c_nT(\alpha_n) = \vec{0} \Rightarrow \alpha^* \in \Null(T)
    \]

    So, we then have that, since $\alpha^*$ is in the null space,

    \[
      \alpha^* = \lincomb{d}{\alpha}{k} = c_{k+1}\alpha_{k+1} + \cdots + c_n\alpha_n
    \]

    \[
      \lincomb{d}{\alpha}{k} - c_{k+1}\alpha_{k+1} - \cdots - c_n\alpha_n = \vec{0} \in V
    \]

    But since $\{\constants{\alpha}{n}\}$ is a basis of $V$, all the constants
    are zero, and in particular all of the $c_i$ are zero. So,
    $\{T(\alpha_{k+1}), \cdots, T(\alpha_n)\}$ is linearly independent and is
    thus a basis of $\range(T)$.
  }

  Now that we have the rank-nullity theorem, we can analyze transformations and
  their matrices.

  \definition{
    Let $A$ be a matrix in $F^{m \times n}$.
    
    The \textbf{Column Space} is the vector space spanned by the $n$ columns of
    $A$. This is precisely $\range(T_A)$.
    
    The \textbf{Row Space} is the vector space spanned by the $m$ rows of $A$.
  }

  \theorem{
    Let $A$ be a matrix, that when row-reduced has $n$ unknowns and $r$ non-zero
    rows. $\nullity(T_A) = n - r$
  }{
    This follows from the fact that elementary row operations preserve the row
    space, and that solving a linear system in $r$ equations with $n$ unknowns
    will have $n - r$ degrees of freedom.

    \TODO{} I guess I can believe this but some more info would be nice.
  }

  \note{
    Let $A$ be a matrix. Then the following are equal
    \begin{itemize}
      \item The dimension of the row space of $A$
      \item The dimension of the column space of $A$
      \item The number of nonzero rows in the row-reduced form of $A$
      \item $\rank(T_A)$	
    \end{itemize}

  }{
    This follows immediately from the above and the Rank-Nullity Theorem.
  }

  \date{Mon. Feb 20 2023}

  Suppose that $A$ is an $m \times n$ matrix. Now suppose that we row reduce
  $A$, let's call this matrix $A^{rr}$. Then we have that

  \[
    \RowSpace(A) = \RowSpace(A^{rr})
  \]

  And we know that $\rank(A)$ is the number of non-zero rows of $A^{rr}$ which
  we call $r$.

  Moreover, the solution set of the homogeneous system $A\vec{x} = \vec{0}$ has
  dimension $n - r$, where $n$ is the number of columns subtract the number of
  redundant equations.

  Now, we know that for a matrix $A$, there is an associated linear
  transformation $T_A: F^n \to F^m$.

  Last time, we also saw that 
  \begin{enumerate}
    \item $\range(T_A) = \ColSpace(A)$, 
    \item $\Null(T_A)$ is the solution set of $A \vec{x} = \vec{0}$.
  \end{enumerate}

  Now we can put everything together. Recall the Rank-Nullity theorem, then we
  have that, for any linear transformation $T_A$,

  \begin{enumerate}
    \item $\rank(T_A) + \nullity(T_A) = \dim(F^n) = n$
    \item $\rank(T_A) := \dim(\range(T_A))$
    \item $\nullity(T_A) = \dim(\Null(A)) = n - r$, which is exactly the dimension of
      the set of all solutions to the homogeneous.
    \item Finally we have that
      \begin{align*}
        \rank(A) &= \dim(\RowSpace(A)) = \dim(\ColSpace(A)) \\
                 &= \dim(\RowSpace(A^{rr})) \\
                 &= \rank(T_A) \\
                 &= r
      \end{align*}
  \end{enumerate}

  Recall also that $\nullity(T_A) = \dim(\Null(T_A)) = n - r$.

  Consider a matrix $A$ where

  \[
    A = \begin{bmatrix}
      1 & 2 & 3 & 4 \\
      2 & 4 & 6 & 8 \\
      1 & 0 & 1 & 1
    \end{bmatrix}
  \]

  Then

  \[
    A^{rr} = \begin{bmatrix}
      1 & 0 & 1 & 1 \\
      0 & 1 & 1 & 3/2 \\
      0 & 0 & 0 & 0
    \end{bmatrix}
  \]

  Is the row reduced matrix.

  A basis for the row space of $A$ is
  \[
    \{(1, 0, 1, 1), (0, 1, 1, 1/3)\}
  \]

  but another is

  \[
    \{(1, 2, 3, 4), (1, 0, 1, 1)\}
  \]

  We have $T_A: \R^4 \to \R^3$, and $\rank(T_A) = 2$.

  Basis for $\range(T_A)$ equals the basis for $\text{Col Space}(A)$

  % Note: doing row reductions {\it affects} the columns, 

  There are many more linear transformations than the ones given by a matrix,
  for instance the derivative or integrals.

  Let $T: V \to W$ be a linear transformation. From here there are two questions
  we can now ask.

  \begin{enumerate}
    \item Is $T$ onto?

      It is if and only if $\range(T) = W$. We saw this earlier.
      In terms of dimension, this means that $\rank(T) = \dim(W)$.

      Note that here, $V, W$ must be {\bf finite dimensional}.

    \item Is $T$ one to one?

      This requires some more work.
  \end{enumerate}

  \theorem{
    $T: V \to W$ is one to one if and only if $\Null(T) = \{\vec{0}\}$.

    \btw{In other words, the Null space must only contain the zero vector.}
  }
  {
    Assume that $T$ is one to one. We know that $T(\vec{0}_V) = \vec{0}_W$.
    Chose any $\alpha \in \Null(T)$, then $T(\alpha) = \vec{0}_W$, by definition
    of being in the Null Space. Since $T$ is one to one, $\alpha$ must equal
    $\vec{0}_V$. \\

    Now assume that $\Null(T)$ is just $\vec{0}_W$. To see that $T$ is one to
    one, chose any $\alpha, \alpha' \in V$, with $T(\alpha) = T(\alpha')$. Then
    $T(\alpha - \alpha') = T(\alpha) - T(\alpha')$ by linearity, but then since
    $\alpha = \alpha'$, $T(\alpha - \alpha') = \vec{0}$ so $T(\alpha - \alpha')$
    must be in the Null space of $T$, and since $\Null(T) = \{\vec{0}\}$, and
    $\alpha - \alpha' = 0$, so $\alpha = \alpha'$ and thus $T$ is one to one.
  }

  \definition{
    $T$ is called {\bf non-singular} if $T$ is one to one.

    This is just another term for something we already know.
  }

  \theorem {
    Now suppose that $T: V \to W$ is a linear transformation with $\dim(V) =
    \dim(W)$. Then $T$ is one to one if and only if $T$ is onto.
  }
  {
    By the Rank-Nullity theorem from last time, we have that

    \[
      \rank(T) + \nullity(T) = \dim(V)
    \]

    Now, assume that $T$ is one to one, then $\nullity(T) = 0$, but then $\rank(T)
    = \dim(V) = \dim(W)$.

    Now conversely, assume that $T$ is onto. Then

    \[
      \rank(T) = \dim(W) = \dim(V)
    \]

    Therefore $\nullity(T) = 0$, and so $T$ is one to one.
  }

  We are now starting to get a pretty good understanding of linear
  transformations, but suppose that we now want to combine them.

  \subsection{Combining Linear Transformations}

  Say $T: V \to W$ and $U: W \to Y$ are linear transformations over $F$.

  \btw{ then $U \circ T: V \to Y$ is a function. }

  {\bf Check the following}:

  \begin{enumerate}
    \item $U \circ T$ is a linear transformation.

      \btw{
        You know how to do this, just check that they scale and add as we
        expect.
      }

    \item If both $T$ and $U$ are one to one, then the composition is also one
      to one.

    \item If both $T$ and $U$ are onto, the composition is also onto.
  \end{enumerate}

  \note{
    $T \circ U$ would {\bf not} be a linear transformation, assuming that $Y$
    and $V$ are not the same vector space.

    \btw {Linear transformations don't commute nicely like that.}
  }

  % \TODO{} Mention somewhere that a linear transformation is nothing but a
  % function, a nice function (additionally, its just a set too)

  Let's now look at $T$ again.

  \definition{
    A linear transformation $T: V \to W$ is called {\bf invertible} if there is
    a linear transformation $U: W \to V$ such that

    \begin{enumerate}
      \item $U \circ T: V \to V$ is the identity from $V$. In other words

        \[
          U(T(\alpha)) = \alpha
        \]

        For any $\alpha \in V$.

      \item $T \circ U: W \to W$

        \[
          T(U(\alpha)) = \alpha
        \]

        For any $\alpha \in W$.

    \end{enumerate}
  }

  \note {
    It might be interesting for you to prove that, if one of the above applies,
    the other automatically applies as well.
  }

  If $T$ is invertible, we call such a $U$ $T^{-1}$, the inverse
  transformation of $T$.

  \note{
    Inverse transformations are unique, if they exist.

    \btw{We didn't talk about this in class but it {\it has} to be true.}
  }

  {\bf Proposition}: If $T: V \to W$ is an {\it invertible} linear
  transformation if and only if $T$ is both one to one and onto.

  \note {
    If $T$ has an inverse, then it must be the case that $\dim(V) = \dim(W)$.

    If this is surprising, just consider that this follows from the fact that
    $T$ must be both one to one, and onto in order to have an inverse.
  }
  
  \newpage

  \date{Wed. Feb 22 2023}

  {
    \it Today was exam review. As such, everything for today is written as
    examples.
  }

  {\bf What to expect}

  \begin{enumerate}
    \item Short answer {\bf True} / {\bf False}. Then write a sentence
      explaining your choice, doesn't need to be a proof.
    \item Matrix stuff. Row Space, Col Space, Col Space, Rank of Matrix,
      Solution set of homogeneous system, etc...
    \item Linear Transformations. Polynomials, $\{e^{ix}, e^{-ix}\}$ stuff,
      etc...
    \item Full Proofs of statements. Rather straightforward, just involving
      linear independence, or spanning, or dimensions, etc...
  \end{enumerate}

  The exam {\it is} 50 minutes, problems {\it will} be reasonable.
  At this point, we have definitely covered everything that will be on exam 1.

  \section{Example Problems}

  \example {
    {\bf Problem 3.1.4}

    Want: A linear transformation $T: \R^3 \to \R^2$ such that $T(1, -1, 1) =
    (1, 0)$, and $T(1, 1, 1) = (0, 1)$

    Firstly, {\it is there one}?

    The two vectors passed to $T$ are linearly independent, so {\bf they can be
    expanded to a basis}. Say for example,

    \[
      \B = \{(1, -1, 1), (1, 1, 1), (0, 0, 1)\}
    \]

    Where $\B$ is a basis of $\R^3$. Now, there will be a {\it unique} linear
    transformation that will take it to any two points in $\R^2$ (even if those
    two points are not linearly independent.)

    {\bf Fundamental Fact}:

    If $V, W$ are vector spaces, and $\B = \{\alpha_1, \dots, \alpha_n\}$ is a
    basis of $V$, then for any $\beta_1, ...,\beta_n \subseteq W$, there is {\it
    exactly} one linear transformation $T$ for which

    \[
      T(\alpha_1) = \beta_1, \dots, T(\alpha_n) = \beta_n
    \]

    {\sc Note}: $\beta_i$ do {\it not} have to be linearly independent! For
    instance consider the $0$ transformation, then $\beta_1 = \cdots = \beta_n =
    \vec{0} \in W$.

    {\bf Question}: How do we define such a $T$?

    Take any $\alpha^* = c_1 \alpha_1 + \cdots + c_n \alpha_n$. Then by
    linearity,

    \[
      T(\alpha^*) = c_1 \beta_1 + \cdots + c_n \beta_n
    \]

    {\sc Note}: This shows that such a $T$ {\it does} exist, but it's not {\it
    constructive}. We don't {\it actually} know what it is; we only know what
    properties it has, and that it exists.

    {\sc Note}: Before we expanded $\{(1, -1, 1), (1, 1, 1)\}$ to a basis, there
    were an {\it infinite} family of linear transformations. This is because we
    got to choose the last vector of $\B$, in our case $(0, 0, 1)$. If we have
    chosen, say, $(0, 1, 0)$ instead, then we would have gotten an entirely
    different transformation $T$.
  }

  \example {
    {\bf Problem 2.4.6a}

    {\bf Question}: How do we show that $e^{ix}$ and $e^{-ix}$ are linearly
    independent?

    Let $f_1(x) = 1, f_2(x) = e^{ix}, f_3(x) = e^{-ix}$.

    {\bf Claim}: $f_1, f_2, f_3$ are {\it linearly independent}.

    Recall what this means. For {\it any} $x$,

    \[
      c_1 f_1(x) + c_2 f_2(x) + c_3 f_3(x) = \vec{0}
    \]

    Implies that $c_1, c_2, c_3$ must all be $0$. Where $\vec{0}$ is the zero
    function: $z(x) = 0$ for all $x$.

    \[
      c_1 + c_2 e^{ix} + c_3 e^{-ix} = \vec{0}
    \]

    Now let $x = -100i$, then $e^{i(-100i)} = e^{100}$, then $e^{-i(-100i)} =
    e^{-100}$, so we have

    \[
      c_1 + c_2 e^{100} + c_3 e^{-100} = \vec{0}
    \]

    Therefore, $c_1, c_2, c_3$ {\it must} all be $0$, since clearly all the
    functions are positive for $x = -100i$. This shows that the functions are
    linearly independent.

    Since $c_1 f_1(x) + c_2 f_2(x) + c_3 f_3(x) = \vec{0}$ {\it has} to work for
    {\it any} value of $x$, if you just find {\it one} counterexample (like $x =
    -100i$ above), that shows that they are linearly independent.

    % \TODO{} write above better
  }

  \example {
    {\bf Question}: Suppose that the field $F$ is $\C$. Is 

    \[
      W = \{g_1(x) = 1, g_2(x) = \sin x, g_3(x) = \cos x\}
    \]

    Linearly independent?
    
    {\bf Answer}: Yes!

    Suppose that for some $x = \theta$ we have

    \begin{align*}
      c_1 g_1(x) + c_2 g_2(x) + c_3 g_3(x) =& \vec{0} \\
             c_1 + c_2 \sin x + c_3 \cos x =& \vec{0}
    \end{align*}

    Then

    \[
      c_2 \sin x + c_3 \cos x = - c_1
    \]
    
    Then look at $\theta + \pi$,

    \[
      c_2 \sin (\theta + \pi) + c_3 \cos (\theta + \pi) = - c_1
    \]

    So then we have the set of equations

    \[
      c_2 \sin \theta + c_3 \cos \theta = c_2 \sin(\theta + \pi) + c_3 \cos(\theta + \pi)
    \]

    But since $\sin \theta = -\sin(\theta + \pi)$ and $\cos \theta = -
    \cos(\theta + \pi)$, we have

    \begin{align*}
      c_2 \sin \theta + c_3 \cos \theta  &= -c_2 \sin \theta - c_3 \cos \theta \\
      2c_2 \sin \theta + 2c_3 \cos \theta  &= 0
    \end{align*}

    But in order for this to be true for all $\theta$, $c_1$ and $c_2$ must both
    be $0$.

    {\sc Note}: We know that $\sin^2 x + \cos^2 x = 1$, but we're not allowed to
    square $g_2$ and $g_3$ here. That's not a linear operation, so it's not
    relevant. However, it {\it is} true that $\{1, \cos^2 x, \sin^2 x\}$ {\it
    are} linearly dependent.

    {\sc Note}: $f_2 \in \Span(\{g_1, g_2, g_3\})$.

    \begin{align*}
      e^{-ix} =&\cos(-x) + i \sin(-x) \\
      e^{-ix} =&\cos(x) = i \sin(x)
    \end{align*}

    So $f_1 \in \Span(\{1, \sin x, \cos x\})$, but $\dim(W) = 3$, and we already
    know that $f_1, f_2, f_3$ are linearly independent from the previous
    example, so we just found another basis for $W$

    \[
      \{f_1, f_2, f_3\} = \Span(W)
    \]
  }

  \example {
    {\bf Problem 2.4.4d}

    Let $W = \Span(\{(1, 0, i), (1 + i, 1, -1)\})$.

    {\bf Question}: Is this set linearly independent?

    Yes, neither is $0$, or a multiple of the other.

    {\bf Question}: What is $\dim(W)$?

    $2$

    So a basis could be

    \[
      \B = \{(1, 0, i), (1 + i, 1, -1)\}
    \]

    The vectors themselves.

    {\bf Question}: Let $\beta_1 = (1, 1, 0)$. Is $\beta_1 \in W$?

    Let's try to make it.

    \[
      (1, 1, 0) = c_1 (1, 0, i) + c_2 (1 + i, 1, -1)
    \]

    Need: $1 = c_1 + c_2 (i + i)$, and $1 = c_1 (0) + c_2 (1)$ so $c_2 = 1$,
    and finally, $0 = c_1 (i) + c_2 (-1)$.

    So $\beta_1 = (-i)(1, 0, -i) + 1 (1 + i, 1, -1)$
  }
  
  \example {
    {\bf 4d on Homework 3}

    If $\dim(V) = \dim(W)$, and $\B = \{ \alpha_1, \dots, \alpha_k\}$ is a basis
    for $V$, must

    \[
      T(\alpha_1) + \cdots + T(\alpha_k)
    \]

    Be a basis for $W$?

    {\bf Answer}: No!

    Let $T$ be the $0$ transformation, then everything is mapped to $0$.
  }

  {\sc Note}: Here it's worth reiterating the difference between the {\it
  codomain} and the {\it range} of $T$.

  Suppose that $T: V \to W$ is a linear transformation that maps vectors from
  $V$ to $W$. Then, we say that $V$ is the {\it domain} of $T$, and that $W$ is
  the {\it codomain} of $T$.

  We know that a linear transformation is just a function, and one thing that we
  know about functions is that they {\it must} use their entire domain (so
  everything from $V$ has to map {\it somewhere} in $W$), but they {\it don't}
  have to use all of their codomain.

  The nuance here is the word {\it "somewhere"}. Suppose that we have the
  function $f: \R \to \R$ with $f(x) = 5$. Then the domain of $f$ is $\R$ and so
  is the codomain. Here, we can see that $5$ is {\it somewhere} in the codomain
  of $f$, but it's certainly {\it not all} of it. Then, we say that the {\it
  range} of $f$ is $\{5\}$. This is all just by definition.

  Let's look at another example. Suppose that $g: \R \to \Z$ with $g(x) =
  \lfloor x \rfloor$ this time. In this case, $g$ {\it does} use up all of its
  codomain, since every value in $\R$ will be mapped to a value in $\Z$.

  Linear transformation behave in exactly the same way. When we talk about
  $\dim(W)$, we're talking about the size of a basis of $W$, but $T$ makes no
  promises about mapping elements from $V$ to all of it.

  \date{Mon. Feb 27 2023}
  
  {\it From this point, the course is going to become much more abstract.}

  Fix $V, W$ vector spaces over the same field $F$.

  Let $L(V, W)$ consist of all linear transformations $T: V \to W$.

  \theorem {
    $L(V, W)$ is a vector space.
  }
  {
    Say that $T, U$ are each linear transformations. Let $T + U: V \to W$ be
    defined by

    \[
      (T + U)(\alpha) = T(\alpha) + U(\alpha)
    \]

    For $T \in L(V, W)$ and $c \in F$. Let $cT: V \to W$ be the linear
    transformation

    \[
      (cT)(\alpha) = cT(\alpha)
    \]

    It's easy to check that these linear transformations satisfy the properties
    of being a vector space.
  }

  \theorem {
    Suppose that $\dim(V) = n$ and $\dim(W) = m$, then $\dim(L(V, W)) = nm$.
  }
  {
    Recall that a linear transformation $T: V \to W$ is determined by what it
    does to a basis of $V$.

    Chose a basis $\B = \{\alpha_1, \dots, \alpha_n \}$ of $V$, and $\B' =
    \{\beta_1, \dots, \beta_n \}$ of $W$.

    For any $1 \le p \le m$ and $1 \le q \le n$,

    Let $E^{pq}: V \to W$ be determined by

    \[
      E^{pq}(\alpha_i) = \begin{cases}
        \beta_j & \text{ if } i = q \\
        0       & \text{ if } i \ne q
      \end{cases}
      = \delta_{iq} \cdot \beta_j
    \]

    Where $\delta_{iq}$ is the ``Kroneker $\delta$ function" defined by

    \[
      \delta_{iq} = \begin{cases}
        1 & \text{ if } i = q \\
        0 & \text{ if } i \ne q
      \end{cases}
    \]

    {\bf Claim}: $E^{pq}: 1 \le p \le m, 1 \le q \le n$ is a basis for $L(V,
    W)$.

    {\bf Proof}: \btw {Why does $\{E^{pq}\}$ span $L(V, W)$?}

    For $1 \le p \le m, 1 \le q \le n$, $E^{pq}(\alpha_q) = \beta_p$ but
    $E^{pq}(\alpha_{q'}) = 0$ for all $q' \ne q$.

    Choose any $T \in L(V, W)$, i.e. $T: V \to W$ is a linear transformation.
    \btw{What does this $T$ do to $V$?}

    For $1 \le q \le n$, say $T(\alpha_q) = A_{1q \beta_1} + A_{2q \beta_2} +
    \cdots + A_{mq \beta_m}$, for some $A_{1q}, \dots, A_{mq}$

    \btw {Here, we're building an $m \times n$ matrix!}

    {\bf Subclaim}:

    \[
      T = \sum_{p = 1}^m \sum_{q = 1}^n A_{pq} E^{pq}
    \]

    \btw {The subclaim shows that $T$ is in the span of $E^{pq}$}

    {\bf Proof of Subclaim}

    Let $U = \sum_{p = 1}^m \sum_{q = 1}^n A_{pq} E^{pq}$. \btw {We're going to
    show that $T$, and $U$ do the same thing to every basis element. Linear
    transformations are equal if and only if they agree on a basis.}
    
    Fix $1 \le q \le n$

    \begin{align*}
      U(\alpha_q) =&\sum_{p = 1}^m \sum_{q = 1}^n A_{pq} E^{pq}(\alpha_q) \\
                  =&\sum_{p = 1}^m  A_{pq} \beta_p \\
                  =&A_{1q} \beta_1 + A_{2q} \beta_2 + \cdots + A_{mq} \beta_m  \\
                  =&T(\alpha_q)
    \end{align*}

    So we see that $T$ and $U$ agree on every $\alpha \in \B$, so $T = U$.

    \btw {Now we need to show that they are linearly independent.}

    {\bf Subclaim 2}: $E^{pq}$ are linearly independent.

    Chose $\{c_{pq}\} \in F$ such that

    \[
      \sum_{p = 1}^m \sum_{q = 1}^n c_{pq} E^{pq} = 0
    \]

    \btw {Now we must show that all $c_{pq}$ must be $0$.}

    Fix any $\le q \le n$. Then

    \begin{align*}
      U(\alpha_q) =&\sum_{p = 1}^{m} \Big( \sum_{q = 1}^{n} c_{pq} E^{pq} (\alpha_q) \Big) & \\
      =&\sum_{p = 1}^{m} c_{pq} \beta_p & \text{By cheatsheet}
      =&0 &
    \end{align*}

    Since $U$ is the zero transformation. Thus

    \[
      c_{1q} \beta_1 + c_{2q} \beta_2 + \cdots + c_{mq} \beta_m = 0
    \]

    Since $\{\beta_1, \dots, \beta_m\}$ is a basis for $W$.

    This means that $c_{1q} = c_{2q} = \cdots = c_{mq} = 0$.

    This holds for every $1 \le q \le n$, therefore all $c_{pq}$ must be $0$, so
    $\{E^{pq}\}$ is linearly independent.
  }


  \date{Wed. Mar 1 2023}

  \definition{
    Suppose that $V, W$ are vector spaces over the same field $F$. An {\bf
    isomorphism} is a linear transformation $T$ that has an inverse $U: W \to V$
    satisfying

    \[
      U \circ T = I_V
    \]
    
    and 

    \[
      T \circ U = I_W
    \]

    Write $T^{-1}$ for this $U$ if it exists.

    Note that $T$ here is {\it necessarily} a bijection.

    Note that $V$ and $W$ must be over the {\bf same field} for $T$ to be an
    isomorphism.
  }

  \definition{
    The vector spaces $V, W$ are called {\bf isomorphic} if there exists an {\it
    isomorphism} $T$ from $V$ to $W$. 

    Note that there may be {\it many different} isomorphisms.
  }

  What does this all mean? Well if $V$ and $W$ are isomorphic, then, even if
  they are very different, they will behave in very similar ways.

  The following hold

  \begin{itemize}
    \item $\dim V = \dim W$
    \item If $V' \subseteq V$ is a subspace, then $T(V') = W' \subseteq W$ is a
      subspace of $W$ with $\dim(V') = \dim(W')$. Basically if $V$ and $W$ are
      isomorphic, then $V'$ and $W'$ are also isomorphic.
  \end{itemize}

  {\bf Recall}: A linear transformation $T: V \to W$ is an isomorphism if and
  only if $T$ is both one to one and onto (in other words, if $T$ is a bijection)

  {\bf Specical Case}: If we're lucky and $\dim(V) = \dim(W)$, then there's an
  easier test. If $T: V \to W$ is a linear transformation, then the following
  are equivalent

  \begin{itemize}
    \item $T$ is an isomorphism
    \item $T$ is one to one
    \item $T$ is onto
  \end{itemize}

  Let's look at examples!

  \example {
    Let $V$ be a vector space over $F$ of dimension $\dim(V) = n$. Let $\B =
    \{\alpha_1, \dots, \alpha_n\}$ be an ordered basis for $V$. The coordinate
    transformation

    \[
      C_{\B}: V \to F^n
    \]
    
    is an isomorphism, sending $\alpha \mapsto [\alpha]_{\B}$. In other words it
    translates $\alpha$ to the language of $\B$.

    If $V = \P^2$, $\B = \{1, x, x^2\}$, for any $f \in \P^2$, $f(x) = a_0 + a_1
    x + a_2 x^2$,

    \[
      C_{\B}(f) = [f]_{\B} = \begin{bmatrix} a_0 \\ a_1 \\ a_2 \end{bmatrix} \in F^3
    \]

    is an isomorphism.

    Given any $\begin{bmatrix} a_0 \\ a_1 \\ a_2 \end{bmatrix} \in F^3$, Let
    $\alpha = a_0 + a_1 x + a_2 x^2 \in \P^2$, then $C_{\B} (\alpha) =
    [\alpha]_{\B} = \begin{bmatrix} a_0 \\ a_1 \\ a_2 \end{bmatrix}$ is {\bf
    onto}.

    {\bf Conclusion}:

    If $V$ has dimension $n$, then

    \[
      V \cong F^n
    \]

    and we say that $V$ is {\it isomorphic} to $F^n$
  }

  This is great! Now we can compare $V$ directly to some $F^n$.

  {\bf Corrolary}

  If $V, W$ are vector spaces over the same field $F$ and $\dim(V) = \dim(W)$,
  then $V \cong W$.

  This is huge! As long as linear transformations have the same dimension, they
  behave in the same way.

  {\bf Proof}

  There exists a $C_{\B}: V \to F^n$ and $C_{\B'}: W \to F^n$, so there {\it must} exist a
  linear transformation $C_{\B'}^{-1} \circ C_{\B}$ is a linear transformation
  going from $V \to W$, this is an isomorphism!

  {\bf Check}

  \begin{itemize}
    \item The composition of any 2 isomorphisms {\it is} an isomorphism
    \item If $T$ is an isomorphism going from $V$ to $W$, then $T^{-1}: W \to V$
      is an isomorphism
  \end{itemize}

  Let's recall some things

  For any field $F$ and any $m, n \ge 1$. If $F^{m \times n}$ consists of all $m
  \times n$ matrices over $F$, $F^{m \times n}$ is a vector space of dimension
  $mn$

  \[
    \dim(F^{m \times n}) = mn
  \]

  After all, there are $mn$ free variables in the basis.
  
  Now, let $V, W$ be vector spaces over $F$. Let $\dim(V) = nb$ and $\dim(W) =
  m$. Now let $L(V, W)$ be the set of all linear transformations $T: V \to W$.

  We saw that $L(V, W)$ is a vector space of dimension $mn$, and that

  \[
    \{E^{pq} : 1 \le q \le n, 1 \le p \le m\}
  \]
  
  is a basis for $L(V, W)$. But notice: $L(V, W)$ is a vector space of dimension
  $mn$, but so is $F^{m \times n}$. So they must be isomorphic!

  \[
    L(V, W) \cong F^{m \times n}
  \]

  {\bf Question}: What is a linear transformation giving this isomorphism?

  {\it Sure, they're isomorphic, but how do we get from one to the other?}

  So the input here is just a linear transformation $T: V \to W$ (an element of
  $L(V, W)$), and the output is some $m$ by $n$ matrix (an element of $F^{m
  \times n}$).

  Chose a basis $\B = \{\alpha_1, \dots, \alpha_n\}$ for $V$ with $\dim(V) =
  n$, and a basis $\B' = \{\beta_1, \dots, \beta_m\}$ for $W$ with $\dim(W)$.
  Then there is an isomorphism $C_{\B}$ Taking $V$ to $F^n$.

  But there is also a coordinate isomorphism $C_{\B'}$ from $W$ to $F^m$.

  But recall that $T$ goes from $V$ to $W$, so the diagram commutes. 

  Let $M_{\B'}^{\B}(T) = \left[ [T(\alpha_1)]_{\B'}, [T(\alpha_2)]_{\B'}, \dots,
  [T(\alpha_n)]_{\B'} \right]$ .

  Then, we propose that, for any $\alpha \in V$,

  \[
    M_{\B'}^{\B} (T) \cdot [\alpha]_{\B} = [T(\alpha)]_{\B'}
  \]

  \subsection{Diagram}
  \[
    \begin{tikzcd}[sep=large]
      {\alpha \in V} && { [\alpha]_{\mathcal B} \in F^n} \\
      \\
      {T(\alpha) \in W} && {[T(\alpha)]_{\mathcal B'} \in F^n}
      \arrow["{C_{\mathcal B}}", from=1-1, to=1-3]
      \arrow["T"', from=1-1, to=3-1]
      \arrow["{C_{\mathcal B'}}"', from=3-1, to=3-3]
      \arrow["{M_{\mathcal B'}^{\mathcal B}(T)}", dashed, from=1-3, to=3-3]
    \end{tikzcd}
  \]

  Let's give a concrete example for this.

  \example {
    Let $D: \P_2 \to \P_1$

    \[
      D(a_0 + a_1 x + a_2 x^2) = a_1 + 2a_2 x
    \]

    $\B = \{1, x, x^2\}$ be a basis for $\P_2$, and $\B' = \{1, x\}$ be a basis
    for $P_1$.

    {\bf Question}

    What is the matrix $M_{\B'}^{\B}(D)$? Well

    $D(1) = 0$, $D(x) = 1, D(x^2) = 2x$, then $[D(1)]_{\B'} = \begin{bmatrix} 0
    \\ 0 \end{bmatrix}$, $[D(x)]_{\B'} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$,
    $[D(x^2)]_{\B'} = \begin{bmatrix} 0 \\ 2 \end{bmatrix}$,

    So

    \[
      M_{\B'}^{\B}(D) = \begin{bmatrix}
        0 & 1 & 0 \\
        0 & 0 & 2
      \end{bmatrix}
    \]

    But what does this mean? Well, chose any $f \in P_2$, say

    \[
      f(x) = 5 + 3x - x^2
    \]

    What are the coordinates of $f$ with respect to $\B$?

    \[
      [f]_{\B} = \begin{bmatrix} 5 \\ 3 \\ -1 \end{bmatrix}
    \]

    Then

    \[
      M_{\B'}^{\B}(D) \cdot [f]_{\B} = \begin{bmatrix}
        0 & 1 & 0 \\
        0 & 0 & 2
      \end{bmatrix}
      \begin{bmatrix} 5 \\ 3 \\ -1 \end{bmatrix}
      =
      \begin{bmatrix} 3 \\ -2 \end{bmatrix}
    \]

    But what does this final vector mean? Well, it's just $[D(f)]_{\B'}$, in other
    words, it's the derivative of $f$ with respect to the basis $\B'$!
  }

  Really, all we're doing here is moving around in the diagram. All we need to
  do is to apply $T$ to every $\alpha_i$, living in $V$.

  {\bf Note}: Look at Homework number 13 on 3.4.

  To show that $\{E^{pq}\}$ span $L(V, W)$, chose $T: V \to W$. Think about
  $M_{\B'}^{\B}(E^{pq})$



  \date{Fri. Mar 3 2023}

  Last time, we saw that, given a transformation $T: V \to W$, and bases $\B =
  \{\alpha_1, \dots, \alpha_n\}$ of $V$, and $\B' = \{\beta_1, \dots, \beta_n\}$
  of $W$.

  The matrix of $T$ with respect to $\B, \B'$ is the $m \times n$ matrix

  \[
    M_{\B'}^\B(T) = \left[[T(\alpha_1)]_{\B'}, \dots, [T(\alpha_n)]_{\B'}\right]
  \]

  For any $\alpha \in V$,

  \[
    M_{\B'}^\B(T) \cdot [ \alpha ]_\B = [T(\alpha)]_{\B'}
  \]

  Let's now look at a special case.

  \definition{
    Let $V$ be a vector space over a field $F$, then a {\bf Linear Operator} $T:
    V \to V$ is any linear transformation from $V$ to itself.
  }

  These are extremely applicable, even in the real world. Let's look at some
  examples.

  \example {
    Let $V$ be $\R^2$. Possible linear operators are $T_{\theta} : \R^2 \to
    \R^2$ where $T_{\theta}$ {\it rotates} points by $\theta$ radians.

    Another $U: \R^2 \to \R^2$ would be to {\it stretch} $x$ by a factor of 5,
    and $y$ by a factor of $2$. Notice that the unit square would be stretch by
    a factor of $5 \times 2$ \btw{Some will notice that this is the information
    that the determinant of $T$ encodes!}

    A third example might be in differential equations. Let $V$ be the set of
    all pairs of foxes and rabbits, and $T$ encodes the number of foxes and
    rabbits one generation later.

    Moreover, linear transformations are also used in physics! Let $V$ be an
    ``Electron cloud" in Quantum mechanics. Heisenberg's uncertainty principle
    tells us that, an observation on $V$ is a linear operator!

    \btw{This is beyond the scope of the class, but the point is that this {\it
    is} extremely useful in real life!}
  }

  Previously we defined $L(V, W)$ as being the vector space of all vector spaces
  $V$ and $W$. Now let's define $L(V, V)$ as the space of all linear operators.

  If $\dim(V) = n$, then $\dim(L(V, V)) = n^2$. This is all matrices
  representing $T: V \to V$ will be square.

  For $T, U \in L(V, V)$, then $UT \in L(V, V)$ is the linear operator which
  ``does $T$ first, then $U$." In other words

  \[
    UT(\alpha) = U(T(\alpha)) = U \circ T (\alpha)
  \]

  Conversely

  \[
    TU(\alpha) = T(U(\alpha)) = T \circ U (\alpha)
  \]

  So these operations are read from {\it right to left}.

  \note{
    Typically $UT \ne TU$. The order in which you put on shoes and socks
    matters.
  }

  \example {
    Suppose that $T, U: \R^2 \to \R^2$, with

    \[
      T(\begin{bmatrix} x \\ y \end{bmatrix}) = \begin{bmatrix} 5x \\ 2y \end{bmatrix}
    \]

    and 

    \[
      U(\begin{bmatrix} x \\ y \end{bmatrix}) = \begin{bmatrix} y \\ x \end{bmatrix}
    \]

    swaps $x$ and $y$. So then

    \[
      UT(\begin{bmatrix} x \\ y \end{bmatrix}) = U(\begin{bmatrix} 5x \\ 2y
    \end{bmatrix}) = \begin{bmatrix} 2y \\ 5x \end{bmatrix}
    \]

    and 

    \[
      TU(\begin{bmatrix} x \\ y \end{bmatrix}) = T(\begin{bmatrix} y \\ x
    \end{bmatrix}) = \begin{bmatrix} 5y \\ 2x \end{bmatrix}
    \]

    which are not the same.
  }

  Let's look at what it means to raise a linear transformation to a power.


  Let $T^2: V \to V$, then

  \[
    T^2(\alpha) = T(T(\alpha))
  \]

  \[
    T^{10}(\alpha) = T( \cdots T(T(\alpha)) \cdots )
  \]

  10 times.

  \[
    (T - U)(T + U) = T^2 + TU - UT + U^2
  \]

  But note that $TU$ and $UT$ {\it cannot} be canceled out here, since they
  might not be the same!

  We often call the identity operator $I: V \to V$. It just ``does nothing".

  \[
    I(\alpha) = \alpha
  \]

  for any $\alpha \in V$.

  \note {
    $I$ commutes with everything!

    \[
      T \circ I = I \circ T
    \]

    for any linear operator $T$.
  }

  If $T$ is invertible, $T^{-1}$ ``undoes" $T$

  \[
    T^{-1} T = T T^{-1} = I
  \]

  But note the domain and codomain of $T$, they must be the same! 
  \btw {If this isn't the case, the inverse might only work in {\it one
  direction}}.

  Let's simplify
  this more

  We say that $T$ is invertible
  \begin{itemize}
    \item if and only if $T$ is onto
    \item if and only if $T$ is one to one
  \end{itemize}

  \btw {If you have one of these facts, the others come for free!}

  \definition {
    Fix $V$ a vector space of dimension $n$, and fix {\it one} basis $\B =
    \{\alpha_1, \dots, \alpha_n\}$ of $V$. Then,

    \[
      M_{\B}^{\B} (T) = \Big[[T(\alpha_1)]_\B, \dots, [T(\alpha_n)]_\B \Big]
    \]
  }

  Is an $n \times n$ matrix. Let's look at what it does.

  Let's take some $\alpha \in V$, then

  \[
    M_{\B}^{\B} (T) \cdot [\alpha]_\B = [T(\alpha)]_\B
  \]

  Where $[\alpha]_\B$ and the result are $n \times 1$ column vectors. What this
  matrix does then, is take a vector $\alpha \in V$, written in the basis of
  $\B$, and output the result of applying $T$ to $\alpha$, still in the basis of
  $\B$.

  \note{
    The book refers to $M_{\B}^{\B} (T)$ as $[T]_\B$. So we would have

    \[
      [T]_\B [\alpha]_\B = [T(\alpha)]_\B
    \]
  }

  If you input the $\B$ coordinates of $\alpha$, you get the $\B$ coordinates of
  $T(\alpha)$. What this means is that we have an isomorphism $T$ going from
  $L(V, V)$ to $F^{n \times n}$

  \[
    \begin{tikzcd}[sep=large]
      {L(V, V)} && { F^{n \times n}} \\
      \arrow["T", from=1-1, to=1-3]
    \end{tikzcd}
  \]

  In fact, there are many different isomorphisms $L(V, V) \to F^{n \times n}$.
  Fix any basis $\B = \{\alpha_1, \dots, \alpha_n\}$ of $V$, then we get an
  isomorphism using $\B$

  \usetikzlibrary{decorations.pathmorphing}
  \[
    \begin{tikzcd}[sep=large]
      {T: V \to V} && { [T]_{\B}} \\
      \arrow["\text{Isomorphism}", leftrightsquigarrow, from=1-1, to=1-3]
    \end{tikzcd}
  \]

  This isomorphism takes $T$ and writes it {\it in the language} of $\B$. We'll
  see later what this isomorphism actually is, because we can construct it!

  In the mean time, let's invoke a basis $\B' = \{\alpha_1', \dots, \alpha_n'\}$
  of $V$, different of $\B$.

  {\bf Question}: How are $[T]_\B$ and $[T]_{\B'}$ related?

  We actually already know how to do this from section $2.4$. We had a ``change of
  basis" matrix. Let's look at it again.

  Given bases $\B = \{\alpha_1, \dots, \alpha_n\}$ and $\B' = \{\alpha_1',
  \dots, \alpha_n'\}$ of $V$, the book gave us the matrix

  \[
    P = \Big[[\alpha_1']_\B, \dots, [\alpha_n']_\B \Big]
  \]

  In other words, $P$ is just the $\B$ representation of the basis vectors of
  $\B'$, written as the column vectors of a matrix.

  \TODO{} Question: How do we get the $\B$ representation of $\alpha_i'$? What
  basis is $\alpha_i'$ written in when it's in the basis of $\B' = \{\alpha_1',
  \dots, \alpha_n'\}$? I presume it's the standard basis, but is this correct?

  So for any $\alpha \in V$,

  \[
    P[\alpha]_{\B'} = [\alpha]_\B
  \]

  What this means is that if we have a vector $\alpha \in V$, written using the
  coordinates of $\B'$, we can translate it {\it to} the coordinates of $\B$ by
  multiplying it by $P$.

  \note {
    Recall what we mean when we talk about $[\alpha]_\B$. This just means ``The
    coordinates of $\alpha$, written using the basis $\B$".
  }

  In our notation $P$ is just $M_{\B}^{\B'} (I)$. It's the matrix that
  translates {\it from} $\B'$ {\it to} $\B$. Why do we pass in the identity to
  $M$? Well, recall the definition of $M_{\B}^{\B'}(T)$

  \[
    M_{\B}^{\B'} (T) = \Big[[T(\alpha_1')]_{\B}, \dots, [T(\alpha_n')]_{\B} \Big]
  \]

  So if the transformation is the identity, it becomes

  \[
    M_{\B}^{\B'} (I) = \Big[[\alpha_1']_{\B}, \dots, [\alpha_n']_{\B} \Big]
  \]


  Then, in our notation, we have

  \[
    M_{\B}^{\B'} (I) [\alpha]_{\B'} = [\alpha]_{\B}
  \]

  {\bf Question}: How are $M_{\B}^{\B'}(I)$ and $M_{\B'}^{\B}(I)$ related?

  Well, we know that

  \[
    M_{\B}^{\B'}(I) [\alpha]_{\B'} = [\alpha]_{\B}
  \]

  So

  \[
    \left(M_{\B}^{\B'}(I)\right)^{-1} [\alpha]_{\B} = [\alpha]_{\B'}
  \]

  So they are inverses of each other!

  \note {
    We can do this because $M_{\B}^{\B'}(I) \cdot [\alpha]_{\B'}$ is nothing
    more than matrix multiplication. We have a vector $\alpha$ written in the
    basis of $\B'$, and we multiply it by the matrix $M_{\B}^{\B'}(I)$.

    But how do we know that $M_{\B}^{\B'}(I)$ has an inverse? This is a fair
    question and the answer might not be immediately obvious. Recall once more
    then definition of $M_{\B}^{\B'}(I)$, we have

    \[ M_{\B}^{\B'} (I) = \Big[[\alpha_1']_{\B}, \dots, [\alpha_n']_{\B} \Big]
    \]

    First, notice that each $\alpha_i'$ is a vector of size $n$, since
    $\dim(V) = n$, so here, we're working with an $n \times n$ matrix. Secondly,
    we know that $\{\alpha_1', \dots, \alpha_n'\}$ form a basis of $V$, so they
    {\it must} be linearly independent. But if they're linearly independent,
    that means that each column of $M_{\B}^{\B'} (I)$ is linearly independent,
    so this is a full rank matrix and so it must have an inverse.
  }

  \example {
    Given $[T]_\B$, we want $[T]_{\B'}$. This is a 3 step process.

    \btw { We want $[T]_{\B'} [\alpha]_{\B'} = [T(\alpha)]_{\B'}$}

    We first start with a vector $\alpha$ in the language of $\B'$, notated as
    $[\alpha]_{\B'}$

    \begin{enumerate}
      \item Change $[\alpha]_{\B'}$ to $[\alpha]_\B$

        We first multiply $[\alpha]_{\B'}$ by $M_{\B}^{\B'}(I)$, essentially
        translating it from the basis of $\B'$ to $\B$

        \[
          M_{\B}^{\B'}(I) \cdot [\alpha]_{\B'} = [\alpha]_{\B}
        \]

      \item Apply $[T]_{\B} \cdot [\alpha]_\B = [T(\alpha)]_\B$

        We then apply the transformation $[T]_{\B}$ to this.

        \[
          [T]_\B \cdot \Big( M_{\B}^{\B'}(I) \cdot [\alpha]_{\B'} \Big) = [T(\alpha)]_{\B}
        \]

        $[T]_{\B}$ is the linear operator working with vectors {\it in the language} of
        $\B$, which is why we {\it can't} apply it to a vector $[\alpha]_{\B'}$.
        We first had to translate $\alpha$ to the language of $[T]_\B$.

      \item Change $[T(\alpha)]_{\B}$ to $[T(\alpha)]_{\B'}$

        We now have a vector $[T(\alpha)]_\B$ which is the result of applying
        $T$ to $\alpha$, all in the language of $\B$. In order for us to get it
        in the language of $\B'$, we only have to multiply it by
        $M_{\B'}^{\B}(I)$, the matrix which takes us from $\B$ to $\B'$.

        \[
          M_{\B'}^{\B}(I) \cdot \Bigg ([T]_\B \cdot \Big(M_{\B}^{\B'}(I) \cdot
          [\alpha]_{\B'} \Big) \Bigg) = [T(\alpha)]_{\B'}
        \]
    \end{enumerate}
  }

  \date {Mon. Mar 6 2023}

  Recall our discussion about linear operators last time.

  \[
    T\left(\begin{bmatrix} x \\ y \end{bmatrix}\right) = \begin{bmatrix} -y \\ x\end{bmatrix}
  \]

  There is a nice visual intuition for these. In this case, this operation flips
  values over the $x$ axis.

  Let's look at other examples.

  \[
    T_F \left(\begin{bmatrix} x \\ y \end{bmatrix}\right) = \begin{bmatrix} y \\ x \end{bmatrix}
  \]

  This linear transformation (or more precisely, linear operator) flips values
  across the $y = x$ line.

  Let's define a basis $S = \left\{\begin{bmatrix} 1 \\ 0 \end{bmatrix},
  \begin{bmatrix} 0 \\ 1 \end{bmatrix}\right\}$, the standard basis in $\R^2$.

  {\bf Question}: What is the matrix associated with $T_F$? Well, $T_F$ sends
  the first basis vector to the second, and the second to the first, so we get
  the linear transformation

  \[
    [T_F]_S = \begin{bmatrix}
      0 & 1 \\
      1 & 0
    \end{bmatrix}
  \]

  Now, we can compute where a vector ends up on $T$ by multiplying it by its
  associated matrix.

  \[
    \begin{bmatrix}
      0 & 1 \\
      1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      5 \\ 3
    \end{bmatrix}
    =
    \begin{bmatrix}
      3 \\ 5
    \end{bmatrix}
  \]

  Let $\B = \left\{\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ -1
  \end{bmatrix}\right\}$ be a different basis (we know they form a basis because the two
  vectors are linearly independent)

  {\bf Want}: $[T]_\B$

  We want $[T]_\B$, and we know that $[T]_\B = \left[[T(\alpha_1)]_\B,
  [T(\alpha_2)]_\B\right]$.

  $T(\alpha_1) = T\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}\right) = \begin{bmatrix} -2 \\
  1 \end{bmatrix}$. Notice that the coordinates of the output vector is with
  respect to the standard basis $S$. We want it expressed in terms of $\B$. But
  we know how to do that

  \[
    \begin{bmatrix}
      -2 \\
      1
    \end{bmatrix}
    =
    c_1
    \begin{bmatrix}
      1 \\ 2
    \end{bmatrix}
    + c_2
    \begin{bmatrix}
      1 \\ -1
    \end{bmatrix}
  \]

  By inspection, we see that $-1 = 3c_1$, so $c_1 = -\frac{1}{3}$, and so $c_2 =
  -\frac{5}{3}$.

  So we have that

  \[
    \begin{bmatrix}
      -2 \\ 1
    \end{bmatrix}_S
    =
    \begin{bmatrix}
      -1 / 3 \\ -5 / 3
    \end{bmatrix}_\B
  \]

  Similarly, we can find that

  \[
    \begin{bmatrix}
      1 \\ 1
    \end{bmatrix}_S
    =
    \begin{bmatrix}
      2 / 3 \\ 1 / 3
    \end{bmatrix}_\B
  \]

  Putting everything together, we have

  \[
    [T]_\B = \left[[T(\alpha_1)]_\B, [T(\alpha_2)]_\B\right]
    = \begin{bmatrix}
      -1 / 3 & 2 / 3 \\
      -5 / 3 & 1 / 3
    \end{bmatrix}
  \]

  So the two matrices $\begin{bmatrix}
      -1 / 3 & 2 / 3 \\
      -5 / 3 & 1 / 3
    \end{bmatrix}$ and $\begin{bmatrix}
      0 & -1 \\
      1 & 0
    \end{bmatrix}$ represent the {\it same} operator in $\R^2$. Hopefully at
    this point, we can see that certain matrices are easier to work with than
    others, even if they do the same thing.

  {\bf} Comic Relief

  Let $c \in \R$

  {\bf Claim}: $T - cI$ is invertible.

  With respect to the standard basis $S$,

  \[
    T - cI = \begin{bmatrix} 
      0 & -1 \\ 
      1 & 0 
    \end{bmatrix} 
    - c 
    \begin{bmatrix} 
    1 & 0 \\ 
    0 & 1 
    \end{bmatrix}
  \]

  So for any $2 \times 2$ matrix $\begin{bmatrix}
    a & b \\ c & d
  \end{bmatrix}$, it's invertible if and only if $ad - bc \ne
  0$.

  \definition {
    Two $n \times n$ matrices are {\bf similar} if they represent the same linear
    transformation, but with respect to different bases.
  }

  Let $\B' = \left\{\beta_1 = \begin{bmatrix} a \\ b \end{bmatrix}, \beta_2 = \begin{bmatrix} c \\ d
  \end{bmatrix}\right\}$ be {\it any} basis for $\R^2$.

  {\bf Find} $[T]_{\B'}$

  Well we have

  \[
    [T]_{\B'} = \left[[T(\beta_1)]_{\B'}, [T(\beta_2)]_{\B'}\right]
  \]

  and we have that

  \[
    [T(\beta_1)]_S = T(\begin{bmatrix}
      a \\ b
    \end{bmatrix})
    =
    \begin{bmatrix}
      -b \\ a
    \end{bmatrix}_S
  \]

  \[
    [T(\beta_2)]_S = T(\begin{bmatrix}
      c \\ d
    \end{bmatrix})
    =
    \begin{bmatrix}
      -d \\ c
    \end{bmatrix}_S
  \]

  {\bf Need}: $[T(\beta_1)]_{\B'}, [T(\beta_2)]_{\B'}$

  For this, we need $M_{\B'}^S (I)$, but this is kind of a pain, instead, let's
  ask:

  What is $M_{S}^{\B'} (I)$? Well this is easy:

  \[
    M_{S}^{\B'} (I) = \left[
      [\beta_1]_S, [\beta_2]_S
    \right]
    =
    \begin{bmatrix}
      a & c \\
      b & d
    \end{bmatrix}
  \]

  But then, finding the inverse is easy!

  \[
    M_{\B'}^{S} (I) = \left(M_{S}^{\B'} (I)\right)^{-1}
    =
    \frac{1}{ad - bc}
    \begin{bmatrix}
      d & -c \\
      -b & a
    \end{bmatrix}
  \]

  Notice: $ad - bc$ will never be zero, if they were, and $ad = bc$, the rows
  would not be linearly independent, so $\B$ would not be a basis.

  Finally, we have

  \begin{align*}
    [T]_{\B'} =&M_{\B'}^S \cdot [T]_S \cdot M_S^{\B'} (I) \\
    =&
    \frac{1}{ad - bc}
    \begin{bmatrix}
      d & -c \\
      -b & a
    \end{bmatrix}
    \left(
    \begin{bmatrix}
      0 & -1 \\ 1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      a & c \\ b & d
    \end{bmatrix}
    \right) \\
    =&
    \frac{1}{ad - bc}
    \begin{bmatrix}
      d & -c \\
      -b & a
    \end{bmatrix}
    \left(
    \begin{bmatrix}
      -b & -d \\ a & c
    \end{bmatrix}
    \right) \\
    =&
    \frac{1}{ad - bc}
    \begin{bmatrix}
      -db - ca & -d^2 - c^2 \\
      b^2 + a^2 & bd + ac
    \end{bmatrix}
    = [T]_{\B'}
  \end{align*}

  Now notice: $-d^2 - c^2 = 0$ only if $c = d = 0$. Secondly, $b^2 + a^2 = 0$
  only if $a = b = 0$

  For any $2 \times 2$ matrix, you can look at the trace, the sum of the
  diagonal.

  Later, we will study similar matrices and we will show that $\text{trace}(A) =
  \text{trace}(A')$ if $A, A'$ are similar. (The converse does not hold)

  \TODO{} Get Notes for March 8

  \date{Fri. Mar 10 2023}

  For $n \times n$ matrices

  The $\trace(A)$ is defined as the sum of the main diagonal. In other words,
  the trace is a linear functional from $F^{n \times n} \to F$, and we then have
  the following operations.

  \begin{itemize}
    \item $\trace(A + B) = \trace(A) + \trace(B)$
    \item $\trace(cA) = c(\trace(A))$
  \end{itemize}

  We also have the following fact, proved on the homework.

  \[
    \trace(AB) = \trace(BA)
  \]

  And the following corollary

  If $A$ and $A'$ are similar, then

  \[
    \trace(A) = \trace(A')
  \]

  The proof is that, if $A' = P^{-1} A P$, then $\trace(A') = \trace((P^{-1} A) P)
  = \trace((A P^{-1}) P) = \trace(A)$

  We also have another corollary,

  For any square matrices in fields of characteristic $0$, we have that

  $AB - BA = I$ is impossible, proved on the homework. The proof is that, if
  matrices are equal, then they have the same trace, but

  \[
    \trace(I) = n \ne \trace(AB - BA) = \trace(AB) - \trace(BA) = 0
  \]

  Recall the dual space.

  If the $\dim(V) = n$ with basis $\B = \{\alpha_1, \dots, \alpha_n\}$, and
  $\dim(V^*) = n$ with basis $\{f_1, \dots, f_n \}$ with the following property

  \[
    f_i(\alpha_j) = \begin{cases}
      1 \text{ if $i = j$} \\
      0 \text{ if $i \ne j$}
    \end{cases}
  \]

  \definition {
    The Annihilator of $S$ denoted by $\Ann(S)$ or $S^0$, defined by

    \[
      \Ann(S) = S^0 = \{f \in V^* : f(\beta) = 0 \text{ for every $\beta \in S$ } \}
    \]

    By this definition, we can see that $S^0$ is a subspace of $V^*$ for any $S
    \subseteq V$.
  }

  Note that if $S \subseteq T$, then $\Ann(T) \supseteq \Ann(S)$. Why?

  Choose some $f \in \Ann(T)$, then $f(\beta) = 0$ for all $\beta \in T$. Since
  $S \subseteq T$, this implies that all $S = 0$ for all $\beta \in S$.
  Therefore \TODO{}

  Let's look at some examples

  \example {
    $\Ann(\{0\}) = \{f \in V^* : f(0) = 0\} = V^*$ since $V^*$ is a vector
    space.

    $\Ann(V) = \{f \in V^* : f(\beta) = 0 \text{ for all $\beta \in V$ } \}$ is
    the zero transformation in $V^*$.
  }

  \theorem {
    If $\dim(V)$ is finite, say $n$, then for any subspace $W \subseteq V$

    \[
      \dim(W) + \dim(\Ann(W)) = n
    \]
  }
  {
    Let $\{\alpha_1, \dots, \alpha_k\}$ be a basis for $W \subseteq V$.
    This basis can be extended to a basis of $V$. Let $\B = \{\alpha_1, \dots,
    \alpha_n\}$ be an extension to a basis for all of $V$.

    Let $\B^* = \{f_1, \dots, f_n\}$ be the dual basis of $V^*$.

    {\bf Claim}: $\{f_{k + 1}, \dots, f_{n}\}$ form a basis for $\Ann(W)$.

    Given the claim, the theorem is proved, since the numbers would add up.

    There are three steps.
    \begin{enumerate}
      \item Step 1 is to show that $\{f_{k + 1}, \dots, f_n\} \subseteq \Ann(W)$.

        Chose any $\beta \in W$, say $\beta = c_1 \alpha_1 + \cdots + c_k \alpha_k$.
        Now chose any $i \ge k + 1$. Then $f_i(\beta) = f_i(c_1 \alpha_1 + \cdots +
        c_k \alpha_k) = c_1 f(\alpha_1) + \cdots + c_k f(\alpha_k)$. But since $i
        \ge k$

        \[
          c_1 f(\alpha_1) + \cdots + c_k f(\alpha_k) = 0
        \]

        so what we've shown is that $f_i(\beta) = 0$, therefore $f_i \in \Ann(W)$.

      \item Step 2: $\{f_{k + 1}, \dots, f_n\}$ are linearly independent.

        This is obvious since this is how they were defined, all $f_1, \dots, f_n$
        is a basis for $V^*$.

      \item $\{f_{k + 1}, \dots, f_n\}$ spans $\Ann(W)$.

        Chose any $g \in \Ann(W) \subseteq V^*$. Since $\B^*$ spans $V^*$

        \[
          g = d_1 f_1 + \cdots + d_n f_n
        \]

        for some $d_1, \dots, d_n$

        then for any $j$,

        \[
          g(\alpha_j) = d_1 f_1(\alpha_j) + \cdots + d_n f_n (\alpha_j) = d_j
          (1) = d_j
        \]

        But $g \in \Ann(W)$, therefore for $1 \le j \le k$

        \[
          g(\alpha_j) = 0
        \]

        since $\alpha_1, \dots, \alpha_k$ are in $W$.

        Therefore, $d_1 = 0, d_2 = 0, \dots, d_k = 0$, therefore $g \in
        \Span(\{f_{k + 1}, \dots, f_n\})$
    \end{enumerate}
  }

  \date {Mon. Mar 13 2023}


  Last time, we saw that we have a vector space $V$ and a subspace $W
  \subseteq V$ with $\dim(V) = n$ and $\dim(W) = k$.

  Let $\{\alpha_1, \dots, \alpha_k\}$ be a basis for $W$. Extend to a basis of
  $\B = \{\alpha_1, \dots, \alpha_n\}$ of $V$.

  Let $\B^* = \{f_1, \dots, f_n\}$ be a basis for $\Ann(W)$ so $\dim(V) = n =
  \dim(W) + \dim(\Ann(W))$, then we have that $\dim(\Ann(W)) = n - k$


  {\bf Corrolary}

  If $W \subseteq V$ is a subspace $\beta \in V \backslash W$, the nthere is
  some $f: V \to F \in V^*$  such that $f \in \Ann(W)$, but $f(\beta) \ne 0$

  {\bf Proof}

  Write $\beta = c_1 \alpha_1 + \cdots + c_n \alpha_n$

  {\bf Claim}. At least one of $c_{k + 1}, c_{k + 2}, \dots, c_n \ne 0$. If not,
  $\beta \in c_1 \alpha_1 + \cdots + c_k \alpha_k$ implies that $\beta \in W$.

  Say that $c_i \ne 0$ with $i \ge k + 1$. Then, $f_i \in \Ann(W)$, but
  $f_i(\beta) = c_i \ne 0$.


  {\bf Corrolary}

  If $W_1, W_2$ are both subspaces of $V$, then $W_1 = W_2$ if and only if
  $\Ann(W_1) = \Ann(W_2)$

  {\bf Proof}

  The forwards direction is obvious, if they are the same, they have the same
  annihilator space.

  For the backwards direction, there are two cases

  \begin{enumerate}
    \item There is some $\beta \in W_1 \backslash W_2$.

      Apply the first corollary to $W_2$, then there is some $f \in \Ann(W_2)$
      with $f(\beta) \ne 0$, therefore $f \in \Ann(W_1)$ and so $\Ann(W_2) \ne
      \Ann(W_1)$.

    \item There is some $\beta \in W_2 \backslash W_1$

      Apply the first corollary to $W_1$, the argument is similar.
  \end{enumerate}

  Say that $f: V \to F$ is any linear functional, but $f \ne 0$. Then
  $\nullity(f) = \dim(V) - 1$.

  {\bf Proof}. $\range(f) \subseteq F'$, but $\range(F) \ne \{0\}$ therefore the
  range of $F$ must equal $F$. So $\rank(f) = 1$.

  \[
    \nullity(f) = n - 1
  \]

  \definition {
    A {\bf Hyperspace} is a subspace $W \subseteq V$ with $\dim(W) = \dim(V) -
    1$.

    We say that $W$ has ``co-dimension" $1$.
  }

  $\Null(f)$ is a hyperspace.

  {\bf Corollary}

  Let $W \subseteq V$ with $\dim(W) = k$, $\dim(V) = n$, and $k < n$.

  Then $W$ is the {\bf intersection} of $n - k$ hyperspaces.

  \btw {
    What this is saying is that you can shrink down from $V$ to $W$ in $n - k$
    steps.
  }

  {\bf Proof}

  {\it Claim}.

  \[
    W = \Null(f_{k + 1}) \cap \Null(f_{k + 1}) \cap \cdots \cap \Null(f_{n})
  \]

  \btw {Note that $f_{k + 1}$ can't be $0$ because they are part of a basis.}

  {\it Proof}. Chose $\beta \in W$. For $i \ge k + 1$, $f_i \in \Ann(W)$,
  therefore $f_i (\beta) = 0$, and so $\beta \in \Null(f_i)$, and so $\beta$ is
  in the intersection.

  Next, chose $\beta \in \Null(f_{k + 1}) \cap \Null(f_{k + 2}) \cap \cdots \cap
  \Null(f_{n})$. Then

  \[
    f_{k + 1}(\beta) = 0, f_{k + 2}(\beta) = 0, \dots, f_{n}(\beta) = 0, 
  \]

  But since $\{f_{k + 1}, \dots, f_n\}$ is a basis of $\Ann(W)$, we have that for
  any $g \in \Ann(W)$, $g(\beta) = 0$. What we can conclude from this is that
  $\beta$ must be in $W$.

  \btw {
    There is no $g$ that kills off everything in $W$ but does not kill $\beta$.
  }

  Say $\dim(V) = n$, $\dim(W) = m$, and $T: V \to W$ is any linear
  transformation.

  We now know that each of $V$ and $W$ have dual spaces $V^*$ and $W^*$
  respectively. Note that $V$ and $W$ are not necessarily the same size, so
  their dual spaces might not either.

  \definition{
    The {\bf transpose} of $T$, written $T^t$ is a linear transformation going
    from $W^* \to V^*$, defined as

    \[
      T^t(g) = f
    \]

    Where $g \in W^*$, and $f \in V^*$ is a linear functional defined as $f: V
    \to F$. Take $f$, and apply it to any $\alpha \in V$

    \[
      T^t(g)(\alpha \in V) := g(T(\alpha)) \in F
    \]

    {\bf Check}. $T^t: W^* \to V^*$  is a linear transformation.
  }

  \TODO{} cleanup

  \theorem {
    Say $V, W$ are finite dimensional vector spaces with $\dim(V) = n, \dim(W) =
    m$, and $T: V \to W$ is any linear transformation where $\rank(T) = r$.

    Then, the following are true

    \begin{enumerate}
      \item $\Null(T^t) = \Ann(\range(T))$
      \item $\rank(T^t) = \rank(T) = r$
      \item $\range(T^t) = \Ann(\Null(T))$
    \end{enumerate}
  }
  {
    \begin{enumerate}
      \item $\Null(T^t) = \Ann(\range(T))$ 

        The proof of this is largely definition unwinding.

        \begin{align*}
                          & g \in \Null(T^t) \\
          \Leftrightarrow &T^t(g) = \vec{0} & \text{By definition of Null Space} \\
          \Leftrightarrow &\forall \alpha \in V, (T^t(g))(\alpha) = 0 & \text{Expanding definition further}\\
        \end{align*}

        Let's take a second to understand what's happening here. $T^t$ is a map from
        $W^*$ to $V^*$. So it takes in elements of $W^*$, which are linear
        functionals $g: W \to F$, and produces elements of $V^*$: linear functionals $f: V \to F$.

        What this means is that when we take a linear functional $g: W \to F$,
        and a linear transformation $T^t: W^* \to V^*$, $T^t(g)$ is itself now a
        map going from $V^* \to F$. In other words, $T^t$ took our map $g$ going
        from $W$ to $F$ and transformed it into a map $f$ going from $V$ to $F$.

        Let's keep going.

        \begin{align*}
                          &\forall \alpha \in V, (T^t(g))(\alpha) = 0 \\
          \Leftrightarrow &\forall \alpha \in V, g(T(\alpha)) = 0 & \text{By definition of $T^t$} \\
          \Leftrightarrow &g \in \Ann(\range(T)) & \text{By definition of Annihilator}
        \end{align*}

        So what we have is that, if $g$ is in the Null Space of $T^t$, it also
        must be in the Annihilator of the Range of $T$.

      \item $\rank(T^t) = \rank(T) = r$ 

        \TODO{} Diagram chasing

      \item $\range(T^t) = \Ann(\Null(T))$

        {\bf Check}. $\range(T^t) \subseteq \Ann(\Null(T))$

        Given this, $\dim(\Ann(\Null(T))) = r$. Now we know that $\range(T^t) =
        r$, and $\dim(\Ann(\Null(T))) = r$

        Therefore

        \[
          \range(T^t) = \Ann(\Null(T))
        \]
    \end{enumerate}
  }

  \date{Wed. Mar 15 2023}

  Let $A$ be the $m \times n$ matrix for $T$ with respect to $\B, \B'$, i.e.

  \[
    A \cdot [\alpha]_{\B} = [T(\alpha)]_{\B'}
  \]

  Let $A^t$ be the $n \times m$ matrix for $T^t$ with respect to
  $\left(\B'\right)^*$ and $\B^*$.

  But it's incredibly easy to go from $A$ to $A^t$,

  \[
    \left(A^t\right)_{ij} = \left(A_{ji}\right)
  \]

  For example

  \[
    A = \begin{bmatrix}
      2 & 3 & 1 \\
      4 & 0 & 5
    \end{bmatrix}
  \]

  then

  \[
    A^t = \begin{bmatrix}
      2 & 4 \\
      3 & 0 \\
      1 & 5
    \end{bmatrix}
  \]

  This is kind of profound! How on earth is this whole $T^t$ thing related to
  flipping a matrix on its diagonal?


  Last time, we saw that

  \begin{itemize}
    \item $\rank(T^t) = \rank(T)$
    \item $\range(T^t) = \Ann(\Null(T))$
  \end{itemize}

  What does this mean when

  \[
    V = \R^n \text{ and } W = \R^m
  \]

  with standard bases

  \[
    \B \text{ and } \B'
  \]

  Well we saw that for a linear system

  \begin{align*}
    2x_1 + 3x_2 + 1x_3 \\
    4x_1 + 0x_2 + 5x_3 \\
  \end{align*}

  Each of these lines is a linear functional!

  \[
    f\left(\begin{bmatrix} a \\ b \\ c \end{bmatrix}\right) = 2(a) + 3(b) + 1(c)
  \]

  When we had a linear transformation $T: \R^3 \to \R^2$, say for example

  \[
    T\left(\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}\right) = (2x_1 + 3x_2
    + x_3, 4x_1 + 0x_2 + 5x_3) = \begin{bmatrix}
      2x_1 + 3x_2 + x_3 \\
      4x_1 + 0x_2 + 5x_3
    \end{bmatrix}
  \]

  We have the matrix associated with $T$ is

  \[
    A = \begin{bmatrix}
      2 & 3 & 1 \\
      4 & 0 & 5
    \end{bmatrix}
  \]

  We also saw that $\rank\left(\left(T_A\right)^t\right) = \rank(T_A) =
  \dim(\ColSpace(A))$

  But we know that $\rank\left(\left(T_A\right)^t\right)$ is the same as
  $\dim(\ColSpace((T_A)^t))$ which is the same as $\dim(\ColSpace(A^t))$ which
  is the same as $\dim(\RowSpace(A))$.

  \begin{center}
    \begin{tikzpicture}[>=latex]
      % Define the main vector spaces
      \def\vsA{(-1,0) ellipse (2cm and 2.7cm)}
      \def\vsB{(7,0) ellipse (2cm and 2.7cm)}

      % Draw the main vector spaces
      \draw \vsA node at (-1,3) {$V: \dim(n)$};
      \draw \vsB node at (7,3) {$W: \dim(m)$};

      \node at (-3.3,0) {$\B$};
      \node at (9.3,0) {$\B'$};
      % \draw \bsBp node at (1,2.3) {$\B'$};

      % Define the subspaces
      \def\subA{(-1,-0.4) ellipse (1cm and 1.1cm)}

      % Draw the subspaces
      \draw \subA node at (-1,1) {$\Null(T)$};

      % Define the linear transformation
      \draw[->] (1.2, 0.5) to [bend left=15] node[above] {$T$} (4.8,0.5);
    \end{tikzpicture}
  \end{center}

  \begin{center}
    \begin{tikzpicture}[>=latex]
      % Define the main vector spaces
      \def\vsA{(-1,0) ellipse (2cm and 2.7cm)}
      \def\vsB{(7,0) ellipse (2cm and 2.7cm)}

      % Draw the main vector spaces
      \draw \vsA node at (-1,3) {$V^*: \dim(n)$};
      \draw \vsB node at (7,3) {$W^*: \dim(m)$};

      \node at (-3.3,0) {$\B^*$};
      \node at (9.5,0) {$\left(\B'\right)^*$};
      % \draw \bsBp node at (1,2.3) {$\B'$};

      % Define the subspaces
      \def\subA{(-1,-0.4) ellipse (1cm and 1.1cm)}

      % Draw the subspaces
      \draw \subA node at (-1,1) {$\rank(T)$};

      % Define the linear transformation
      \draw[<-] (1.2, 0.5) to [bend left=15] node[above] {$T^t$} (4.8,0.5);

      \node at (7,0) {$g$};
    \end{tikzpicture}
  \end{center}

  This marks the end of Chapter 3. Chapter 4 will focus on polynomial, and
  chapter 5 on determinants.

  We will study linear operators for some time, as they have a lot of real world
  significance.

  We will find a magic polynomial: the characteristic polynomial, which will
  give us a lot of information about the linear transformation.

  \subsection{Chapter 4}

  {\it Our discussion of polynomials begins}.

  {\bf Note}: We will mostly skip $4.1$ and $4.3$ from the book

  Let $F$ be a field ($\Q$, $\R$, $\C$). Which field we choose is going to
  matter here.

  \definition{
    $F[x]$ is all polynomials of the form

    \[
      a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n
    \]

    Where $a_0, a_1, \dots, a_n \in F$
  }

  We saw this before, we just called it $\P$ before. We also had that $\P$ had
  the basis $\{1, x, x^2, \dots\}$

  {\bf Note}: We can multiply two polynomials together and get a polynomial!

  You can always add, always subtract, always multiply, but not always divide!
  For instance you cant divide by the zero polynomial. There are restrictions
  about when you can divide.

  $f(x) = x^2$ is a perfectly fine polynomial, but $\frac{1}{x^2}$ is not a
  polynomial, since there is no multiplicative inverse. \TODO{} verify

  We say that $F[x]$ is a {\bf ring}.

  {\bf Note}: Every field is a ring, but not every ring is a field.

  As a side note, we can see that $\Q$ is a field, but $\Z$ is not: {\it it} is a ring.

  What we have in mind here, is to study linear operators $T: V \to V$.
  Polynomials are a tool for understanding these $T$s.

  Given, say $f(x) = x^2 + 2x - 3$, we can apply $f$ to $T$, getting $f(T)$

  \[
    f(T) = T^2 + 2T - 3(I)
  \]

  \btw{Remind yourself what $T^2$ means: it's $T \circ T$.}

  Which is itself a different linear transformation going from $V$ to $V$.

  Given such a $T$, we are interested in the set $J$ of all polynomials $f \in F[x]$
  such that $f(T)$ is the zero transformation.

  \[
    J = \Big\{ \text{polynomials } f \in F[x] \text{ such that } f(T) = \vec{0} \Big\}
  \]

  We'll see that, for any $T$, this is an {\bf ideal} of $F[x]$.

  Within such a $J$, there is a particular polynomial called the {\bf
  characteristic polynomial}.

  We'll see later that, how the characteristic polynomial factors will tell us a
  lot of information about $T$.

  But what do we mean by factoring?

  \definition{
    A polynomials $d(x)$ divides $f(x)$ if there is some other $q(x) \in F[x]$
    such that

    \[
      f = d \cdot q
    \]
  }

  We'll later try to factor this all the way down, similarly to the unique prime
  factorization of the natural numbers. We'll see that for a given polynomial
  $p$, we'll get a unique factorization.

  {\bf Danger}: For a particular $f(x)$, whether or not $f$ factors may depend
  on the field $F$. For example

  let $f(x) = x^2 + 1$. In $\R[x]$, $f$ is irreducible. But in $\C[x]$, $f$ does
  factor!

  \[
    x^2 + 1 = (x - i)(x + i)
  \]

  {\bf Notation}: The zero polynomial is always a special case. If we're trying
  to prove something for all polynomials, we tend to handle zero separately.

  For $f \in F[x], f \ne 0$, the {\bf degree} of $a_0 + a_1 x + \cdots + a_m
  x^m$ is the largest $n$ such that $a_i \ne 0$.

  We say that $f$ is {\bf monic} if $a_n = 1$, where $n$ is the degree of $2$.
  For example

  \[
    f(x) = x^2 + 2x - 3
  \]

  $f$ is a monic of degree 2, since the leading coefficient is 1.

  In the integers, we have the idea of the GCD, the largest number that divides
  all members of a set. For example

  \[
    \GCD(24, 36) = 12
  \]

  For polynomials, we can do something similar. Given $\{f_1, f_2, f_3\}$,
  $\GCD(f_1, f_2, f_3)$ is the polynomial $h$ of largest degree such that
  $h$ divides $f_1$, $f_2$, and $f_3$.

  \date{Fri Mar. 17 2023}

  Looking at 3 on the homework

  \Problem{3}
  {
    Suppose that $V, W$ are finite dimensional vector spaces and $T: V \to W$ is
    a linear transformation.

    \begin{enumerate}
      \item Prove that if there is some $f \in W^*$ such that $\Null(T^t) =
        \Span(\{f\})$, then $\range(T) = \Null(\{f\})$

      \item Prove that if there is some $f \in V^*$ such that $\range(T^t) =
        \Span(\{f\})$, then $\Null(T) = \Null(\{f\})$
    \end{enumerate}
  }
  {
    Recall our picture

    \begin{center}
      \begin{tikzpicture}[>=latex]
        % Define the main vector spaces
        \def\vsA{(-1,0) ellipse (2cm and 2.7cm)}
        \def\vsB{(7,0) ellipse (2cm and 2.7cm)}

        % Draw the main vector spaces
        \draw \vsA node at (-1,3) {$V$};
        \draw \vsB node at (7,3) {$W$};

        % Define the subspaces
        \def\subA{(7,-0.4) ellipse (1cm and 1.1cm)}

        % Draw the subspaces
        \draw \subA node at (7,1) {$\range(T)$};

        % Define the linear transformation
        \draw[->] (1.2, 0.5) to [bend left=15] node[above] {$T$} (4.8,0.5);
      \end{tikzpicture}
    \end{center}

    \begin{center}
      \begin{tikzpicture}[>=latex]
        % Define the main vector spaces
        \def\vsA{(-1,0) ellipse (2cm and 2.7cm)}
        \def\vsB{(7,0) ellipse (2cm and 2.7cm)}

        % Draw the main vector spaces
        \draw \vsA node at (-1,3) {$V^*$};
        \draw \vsB node at (7,3) {$W^*$};

        % Define the subspaces
        \def\subA{(7,-0.4) ellipse (1cm and 1.1cm)}

        % Draw the subspaces
        \draw \subA node at (7,1) {$\Null(T^t)$};

        % Define the linear transformation
        \draw[<-] (1.2, 0.5) to [bend left=15] node[above] {$T^t$} (4.8,0.5);
      \end{tikzpicture}
    \end{center}

    From class, we saw that $\Ann(\range(T)) = \Null(T^t)$.

    For part a, since we have that $\Null(T^t) = \Span(\{f\})$ for some $f \in
    W^*$. Thus we have that

    \begin{equation}
      \Ann(\range(T)) = \Span(\{f\})
    \end{equation}

    {\bf Claim}: $\range(T) = \Null(\{f\})$

    {\bf Proof}: Chose any $\beta \in \range(T)$, by (1), we must have that
    $f(\beta) = 0$, because $f$ is in the Annihilator of the Range of $T$, that
    means that $f$ kills anything in the Range of $T$, but $\beta$ is in the
    range of $T$, so $f(\beta) = 0$. But this means that

    \[
      \beta \in \Null(\{f\})
    \]

    so we have that $\range(T) \subseteq \Null(\{f\})$

    For the other direction, its easier to just count the dimension.

    Say that $f \ne 0$. Since $f: W \to F$ is a linear transformation, 

    \[
      \dim(\Null(f)) = m - 1
    \]

    By the Rank Nullity Theorem (since the output is of dimension 1) (but how do
    we know its not less than $m - 1$, oh because it outputs a single number,
    the dimensions just work out)

    Since $\Null(T^t) = \Span(f)$ by assumption, $\dim(\Null(T^t)) = 1$

    By Rank Nullity of $T^t$,

    \[
      \dim(\range(T^t)) + \dim(\Null(T^t)) = m
    \]

    So $\dim(\range(T^t)) = m - 1$ but from the diagram chase from class we saw
    that $\dim(\range(T^t)) = \dim(\range(T))$

    3b is similar to this
  }

  Let's look at other problem

    \Problem{4.2.3}
  {
    Let $A$ be an $n \times n$ diagonal matrix over the field $F$, i.e., a
    matrix satisfying $A_{ij} = 0$ for $i \ne j$. Let $f$ be the polynomial over
    $F$ defined by

    \[
      f = (x - A_{11}) \cdots (x - A_{nn})
    \]

    What is the matrix $f(A)$?
  }
  {
    Let's start by asking: what would $(A - a_{11}(I))$ look like? It would
    simply be $A$ without the top left entry. What about $(A - a_{22}(I))$? That
    would be the matrix with an entry in each element of the diagonal, except
    for the second.

    When we multiply all these guys together, we get zeroes along the diagonal,
    since each matrix removes an element from the diagonal.

    All in all, this is then the zero matrix!
  }

  Let's look at ideals. We want to factor polynomials, and prove theorems about
  factorization, so we want to do this carefully.

  \subsubsection{Euclidean Algorithm}

  {\bf Lemma}: For any polynomials $f, d \in F[x]$ with $d \ne 0$ and $\deg(d)
  \le \deg(f)$, there is a polynomial $g \in F[x]$ such that

  \[
    \deg(f - gd) < \deg(f) \text{ or } f - gd = 0
  \]

  {\bf Proof}: 

  Write $f = a_mx^m + (\text{stuff of degree $< m$})$, similarly, write $d = b_n
  x^n + (\text{stuff of degree $< n$})$. By assumption, $n \le m$.

  Let $g(x) = (\frac{a_m}{b_n} x^{m - n})$. Then $f - gd$ has
  degree $<m$ or is equal to zero.

  {\bf Note}: we use the fact that we're in a field to do this division.

  \theorem {
    Suppose that $f$ and $d$ are two different polynomials in $F[x]$ and $d \ne
    0$. There are $q, r \in F[x]$ such that

    \[
      f = d \cdot q + r
    \]

    Where $q$ is the quotient, and $r$ is the remainder.
    Additionally, $\deg(r) < \deg(d)$ or $r = 0$.
  }
  {
    {\it By induction.}

    Fix any $d \in F[x]$ with $d \ne 0$. Let $(*)_m$: For every $f \in F[x]$ of
    degree less than $m$ (or $f = 0$), there are $q, r \in F[x]$ such that $f =
    d \cdot q + r$ with $\deg(r) < \deg(d)$ or $r = 0$.

    So the theorem says that $(*)_m$ holds for all $m$, we assume that it holds
    for $m$ and we will prove that $(*)_{m + 1}$ holds. (This is just induction)

    To do this, chose $f \in F[x]$ with $\deg(f) = m$. Now there are two cases.

    \begin{enumerate}
      \item If $m < \deg(d)$, then we're trivially done! Simply take $f = r$, and
        $q = 0$, and $\deg(r) < \deg(d)$.

      \item If $m \ge \deg(d)$. But then, we can use the Lemma. We apply it to
        get some $g(x) \in F[x]$ such that $f - dg = 0$ or $\deg(f - dg) <
        \deg(f) = m$. But now we can apply $(*)_m$ to $f - dg$, since its degree
        is strictly smaller than $m$. So we get $q, r \in F[x]$ such that

        \[
          (f - dg) = dq + r
        \]

        Where $r = 0$ or $\deg(r) < \deg(d)$. Now let $q^* = g + q$, then $f = d
        \cdot q^* + r$, where $\deg(r) < \deg(d)$ or $r = 0$. So we're done.
    \end{enumerate}
  }

  Let's look at a fun Corollary from this.

  {\bf Corollary}: For any $f \in F[x]$ and $c \in F$. $(x - c)$ is a factor of
  $f$ if and only $f(c) = 0$ (if $c$ is a root of $f$)

  {\bf Proof}:
  Say $f(x) = (x - c)h(x)$ for some $h(x) \in F[x]$, then just apply $c$, so we
  have

  \[
    f(c) = (c - c)h(c) = 0
  \]

  More interestingly is the other direction. Assume that $f(c) = 0$. Apply the
  theorem to $f$ and $d(x) = (x - c)$, and we get $q, r \in F[x]$ such that

  \[
    f(x) = (x - c) q(x) + r(x)
  \]

  with $\deg(r) < \deg(x - c)$ or $r = 0$. But since $\deg(x - c) = 1$, that
  tells us that $r$ must be a constant (possibly zero).

  Plugging in $c$, we get

  \[
    f(c) = (c - c) q(c) + r(c) = r(c)
  \]

  But $f(c) = 0$, so $r(c) = 0$, but since $r$ was a constant function, $r$ must
  have been the zero polynomial this whole time. Rolling back, this tells us
  that there was no remainder in $f(x)$, so $(x - c)$ is a factor!

  \[
    f(x) = (x - c) q(x)
  \]

  \definition{
    An {\bf Ideal} $I \subseteq F[x]$ satisfies

    \begin{enumerate}
      \item $0 \in I$
      \item if $f, f' \in I$, then $f + f' \in I$
      \item If $f \in I$ and $g \in F[x]$, then $fg \in I$
    \end{enumerate}
  }

  {\bf Ex}: $\{0\}$ is an ideal, called the zero ideal.

  Another fact, maybe more surprising, is the following. Suppose that $I$ is an
  ideal, and $1 \in I$, then $I = F[x]$ by (3)
  

  \date{Mon Mar. 27 2023}

  \TODO{} Talk about the Euclidean Algorithm for $F[x]$.

  $F[x]$ is a {\it commutative ring} if, for any $f, g \in F[x]$

  \[
    f \cdot g = g \cdot f
  \]

  Say $S = \{f_1, \dots, f_n\} \subseteq F[x]$ , the ideal generated by $S$ is

  \[
    I(S) = \{f_1 g_1 + \cdots + f_n g_n : g_1, \dots, g_n \in F[x] \}
  \]

  For example, $I = \{f \in F[x] : f(2) = 0 \text{ and } f(4) = 0 \}$

  In a commutative ring, we want to know what the ideals look like. For $F[x]$
  as with the integers, they turn out to be incredibly nice. We'll even show
  that every ideal is generated by a single polynomial

  \theorem{
    In $F[x]$, every ideal $I \subseteq F[x]$ is generated by a single
    polynomial $d \in F[x]$.

    $I = I(\{d\}) = d \cdot F[x]$ is the ideal generated by $d$.
  }
  {
    We say that $F[x]$ is a ``principal ideal domain" \btw{ ``Principal" because there is one guy that
    controls everything.} This is also true of the integers.

    Chose any ideal $I$. If $I = \{0\}$, take $d = 0$. If $I$ has a non-zero
    element, chose $d \in I - \{0\}$ of {\it least possible degree}.

    {\bf Claim}. $I = d \cdot F[x]$

    {\bf Proof}.

    $\supseteq$ is clear. Chose any $g(x) \in F[x]$ Since $I$ is an ideal, and
    $d \in I$, $d \cdot g \in I$ by definition of ideal.

    $\subseteq$. Chose any $f \in I$. Since $d \ne 0$, apply the Euclidean
    Algorithm to get $f = d q + r$ where $r = 0$ or $\deg(r) < \deg(d)$. But
    notice that $d \in I$, so $d \cdot q \in I$, $f \in I$, and so $f - d \cdot
    q \in I$ by definition of $I$, but that means $r \in I$. But $r$ must be
    $0$, since we chose $d$ to be of smallest degree.

    This all means that $f$ can be written as $d \cdot q$.
  }

  {\bf Aside}: Why do we care about all this? I thought I signed up for 405, not
  403

  Ideals show up in Linear Algebra.

  Let $A$ be any $n \times n$ matrix. Let $I = \{f \in F[x] : f(A) = 0\}$. This
  is an ideal which depends on a matrix. Now we know that this ideal $I$ is
  generated by one single polynomial.

  \btw {End of Aside}

  To find $d$ for a particular ideal $I$, we need to find the {\bf gcd}: the
  Greatest Common Divisor. How do we find a gcd?

  In $\Z$, what we did was use the Euclidean Algorithm!

  \definition{
    A non-constant $f \in F[x]$ is {\bf irreducible} if $f \ne g \cdot h$ where
    $\deg(g), \deg(h) \ge 1$.

    These are the ``primes" of $F[x]$.
  }

  As in the integers, every non-constant $f \in F[x]$ can be written as $f = c
  \cdot p_1^{k_1} \cdot p_2^{k_2} \cdots p_l^{k_l}$, where $c$ is a constant,
  and $p_1, \dots, p_l$ are {\it monic, irreducible} polynomials.

  \example {
    Let $f(x) = 3(x^2 + 1)^2 (x - 3)^4 (x + 2)$ and $g(x) = -4 (x^2 + 1) (x -
    3)^2 (x + 2)^5$ in $\R[x]$.

    The gcd of $f$ and $g$ is now trivial!

    \[
      \gcd(f, g) = (x^2 + 1) (x - 3)^2 (x + 2)
    \]

    In fact, this gcd is our polynomial $d$. This is always the case.
  }

  \definition{
    $f, g$ are {\bf relatively prime} if

    \[
      \gcd(f, g) = 1
    \]

    In other words, they are relatively prime if they share not factors.
  }

  Which polynomials are irreducible depends on $F$. For example

  $x^2 + 1$ is irreducible in $\Q[x]$, $\R[x]$, but not $\C[x]$. If you allow
  complex numbers, we can factor it into

  \[
    (x^2 + 1) = (x - i)(x + i)
  \]

  Similarly, $(x^2 - 2)$ is irreducible in $\Q[x]$ but not $\R[x]$.

  \[
    (x^2 - 2) = (x - \sqrt{2})(x + \sqrt{2})
  \]

  In $\C[x]$, the only monic, irreducible polynomials are of the form $x - a$.

  \btw {This is it. This is all that we needed from chapter 4. We'll go back to
  linear algebra land now.}

  \date {Wed Mar. 28 2023}

  \section{The Determinant Function}

  \TODO{} Write notes for that day

  (My computer broke so I couldn't write them on the day, sorry about that)

  \date {Fri Mar. 31 2023}

  \note {
    We're interested in the determinant function! On Wednesday we showed the
    existence of the function, and today we're going to show the uniqueness.
  }

  Fix a commutative ring $K$

  For each $n \ge 1$, a {\bf determinant function}a $D: (K^n)^n \to K$ satisfies
  three things

  \begin{enumerate}
    \item $D$ is {\bf $n$ linear}
    \item $D$ is {\bf alternating}
    \item $D(I_{n \times n}) = 1$
  \end{enumerate}

  What we're aiming for is: for every $n$, there is {\bf exactly one}
  determinant function.

  For the moment, just trust that this is important! We'll see later exactly
  what we can do with the determinant and it's properties.

  \subsection{Existence}

  Last time, we showed this for $n = 1$.

  \[
    D(a) = a \text{ for all $a \in K$}
  \]

  is a determinant function.

  For $n = 2$, we had

  \[
    D\left(\begin{bmatrix} a & b \\ c & d \end{bmatrix}\right) = ad - bc
  \]

  But we want this for an arbitrary $n$, how do we do this?

  We do it by induction.

  \subsection{General Method}

  Let's look at how we can get from an $(n - 1) \times (n - 1)$ determinant
  function $D$, to a $n \times n$ determinant function $E$

  This is sometimes called ``expansion by minors".

  For any $1 \le i, j \le n$ for any $n \times n$ matrix $A$.

  $A[i | j]$ is the $(n - 1) \times (n - 1)$ matrix formed by {\it deleting}
  the $i^\text{th}$ row and $j^\text{th}$ column.

  \TODO{} draw examples with barred matrices

  We also define $D_{ij}(A) = D(A[i | j])$, where $A$ is an $n \times n$ matrix.

  \definition {
    For any $1 \le j \le n$, let $E_j: (K^n)^n \to K$ defined as

    \[
      E_j(A_{n \times n}) =
      \sum_{i = 1}^n \underbrace{(-1)^{i + j}}_{\in K}
      \underbrace{A_{ij}}_{\in K}
      \underbrace{D_{ij}(A)}_{\in K}
    \]
  }

  Of course, we already know how to take an $n \times n$ determinant of a
  matrix, so this shouldn't feel completely arbitrary to the reader.

  \theorem{
    For every $1 \le j \le n$, $E_j$ is a determinant function on $n \times n$
    matrices.
  }
  {
    \TODO{} He didn't write the proof for this in class but the book probably
    has it in $5.2$
  }

  \example {
    Let
    \[
      A = \begin{bmatrix}
        (x - 2) & x^2 & x^3 \\
        0 & (x - 2) & x^2 + 5 \\
        0 & 0 & x + 6
      \end{bmatrix}
    \]

    Compute $E_i(A) = \sum_{i = 1}^n (-1)^{i + 1} A_{i1} D_{i1}(A)$

    $i = 1$

    $A[1 | 1] = \begin{bmatrix} x - 4 & x^2 + 5 \\ 0 & x + 6 \end{bmatrix}$

    So $D_{1,1}(A) = (x - 4)(x + 6) - (x^2 + 5) \cdot 0 = (x - 4)(x + 6)$

    So $(-1)^{1 + 1} A_{1, 1} D_{1, 1} (A) = 1(x - 2) (x - 4) (x + 6)$

    $i = 2$

    $A[2 | 1] = \begin{bmatrix} x^2 & x^3 \\ 0 & x + 6\end{bmatrix}$

    $D_{2,1}(A) = x^2 (x + 6) - x^3 \cdot 0$

    $(-1)^{2 + 1} \cdot A_{2, 1} D_{2, 1} (A) = (-1)^3 \cdot 0 (x^2) (x + 6) = 0$

    $i = 3$

    $A[3 | 1] = \begin{bmatrix} x^2 & x^3 \\ x - 4 & x^2 + 5 \end{bmatrix}$

    \TODO{}
    $(-1)^{3 + 1} \cdot A_{3, 1} D_{3, 1} (A) = (-1)^4 \cdot 0 \cdot * = 0$

    So $E_1(A) = (x - 2)(x - 4)(x + 6) + 0 + 0 = (x - 2)(x - 4)(x + 6)$

    \note {
      When we write $0$, it's important to keep in mind where it comes from. The
      answer is that it comes from $K$, moreover, it's a polynomial in this case.
    }
  }

  \TODO{} Ok to me I dont see why this process creates a determinant function,
  it's what we did in 240, but why does THIS process create a function with the
  3 properties above? unless I'm missing something, THAT wasn't explained.

  The answer is Theorem 1 in 5.2, it will not be proved in class because it's
  pretty involved.

  \theorem {
    We've now proved that for every $n \ge 1$, a determinant function $D:
    (K^n)^n \to K$ exists.
  }
  {
    The proof is by induction on $n$.
  }


  \subsection{Uniqueness}

  \theorem {
    For any $n \ge 1$, there is {\bf exactly one} determinant function $D:
    (K^n)^n \to K$.
  }
  {
    Fix $n \ge 1$. Suppose $A = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n
    \end{pmatrix}$ is the $n \times n$ matrix with rows $\alpha_1, \dots,
    \alpha_n$.

    Let $D(A) = D(\alpha_1, \dots, \alpha_n)$. Use the fact that $D$ is linear
    with $i = 1$. (after all, it's linear in all terms since it's $n$ linear)

    We know that $\alpha_1 = (A_{1,1}, A_{1,2}, \dots, A_{1,n})$. We can also
    write it as $\alpha_1 = (A_{1,1} \eps_1, A_{1,2} \eps_2, \dots, A_{1,n}
    \eps_n)$. Recall that $\eps_1 = (1, 0, \dots, 0), \eps_2 = (0, 1, 0, \dots,
    0), \dots$ \btw{$\eps_i$ is the $i$th row of the identity matrix.}

    Then we have

    \begin{align}
       &D(\alpha_1, \alpha_2, \dots, \alpha_n) \\
      =&D((A_{1,1} \eps_1 + \cdots + A_{1,n} \eps_n), \alpha_2, \dots, \alpha_n) \\
      =&A_{1, 1} D(\eps_1, \alpha_2, \dots, \alpha_n) + A_{1, 2} D(\eps_2, \alpha_2,
      \dots, \alpha_n) + \cdots + A_{1, n} D(\eps_n, \alpha_2, \dots, \alpha_n) \\
      =&\sum_{j = 1}^n A_{1, j} D(\eps_j, \alpha_2, \dots, \alpha_n)
    \end{align}

    For (4), recall that $A_{i, j}$ is just a number in $K$, so we can factor it
    from $D$ by $n$ linearity.

    Let's look at $D(\eps_j, \alpha_2, \dots, \alpha_n)$ in more detail.

    \begin{align}
       &D(\eps_j, \alpha_2, \dots, \alpha_n) \\
      =&D(\eps_j, (A_{2,1} \eps_1 + \cdots + A_{2,n} \eps_n), \alpha_3, \dots, \alpha_n) \\
      =&A_{2, 1} D(\eps_j, \eps_1, \alpha_2, \dots, \alpha_n) + A_{2, 2}
      D(\eps_j, \eps_2, \alpha_3, \dots, \alpha_n) + \cdots + A_{2, n} D(\eps_j, \eps_n, \alpha_3, \dots, \alpha_n) \\
      =&\sum_{k = 1}^n A_{2, k} D(\eps_j, \eps_k, \alpha_3, \dots, \alpha_n)
    \end{align}

    We can keep exploding these terms on and on. Since $D$ is $n$ linear, we end
    up with this

    \begin{align*}
      D(A) =&D(\alpha_1, \dots, \alpha_n) \\
      =& \sum_{(k_1, \dots, k_n)} A_{1, k_1} A_{2, k_2} \cdots A_{n, k_n} D(\eps_{k_1}, \eps_{k_2}, \dots, \eps_{k_2}) \\
      =& \sum_{k_1 = 1}^n \sum_{k_2 = 1}^n \cdots \sum_{k_n = 1}^n A_{1, k_1} A_{2, k_2} \cdots A_{n, k_n} D(\eps_{k_1}, \eps_{k_2}, \dots, \eps_{k_2})
    \end{align*}

    This is a lot of products.

    For $n = 3$: Sum of $3^3 = 27$ products.

    For $n = 10$: Sum of $10^{10} =$ 10 billion products.
  }

  I should get paid for texing this.



  \date{Mon Apr. 3 2023}

  {\it We're in the midst of determining that there is a unique determinant for
  any $n \ge 1$. Today, we will finish proving uniqueness.}

  Let's recall what this means.

  $K$ is any commutative ring, $n \ge 1$. For any $n \times n$ matrix $A$ with
  {\bf rows} $\alpha_1, \dots, \alpha_n$, we can either write 

  \begin{itemize}
    \item $D: K^{n \times n} \to K$ for which we have $D(A)$
    \item $D: (K^n)^n \to K$ for which we have $D(\alpha_1, \dots, \alpha_n)$
  \end{itemize}

  These mean the same thing, and they are just different ways of thinking about
  what we're doing.

  We also had the following properties for the determinant

  \begin{enumerate}
    \item $D$ is {\bf $n$ linear}
    \item $D$ is {\bf alternating}

      With this property, we ensure that if two rows are the same, the
      determinant is zero.

    \item {\bf ``normalization"}, $D(I) = D(\eps_1, \eps_2, \dots, \eps_n) = 1$ 
  \end{enumerate}

  We want to show that {\bf there is only one} function which satisfies these
  three properties.

  Last time, we saw that if $D$ is $n$ linear, we had this property

  \begin{align*}
    D(A) = D(\alpha_1, \dots, \alpha_n) =& \sum_{(k_1, \dots, k_n)} A_{1, k_1} A_{2, k_2} \cdots A_{n, k_n} D(\eps_{k_1}, \eps_{k_2}, \dots, \eps_{k_2}) \\
    =& \sum_{k_1 = 1}^n \sum_{k_2 = 1}^n \cdots \sum_{k_n = 1}^n A_{1, k_1} A_{2, k_2} \cdots A_{n, k_n} D(\eps_{k_1}, \eps_{k_2}, \dots, \eps_{k_2})
  \end{align*}

  In other words, this is the sum of $n^n$ products.

  The question now is, what happens to $D(\eps_{k_1}, \eps_{k_2}, \dots,
  \eps_{k_n})$ ?

  Let's assume that $D$ is $n$ linear and alternating. Then, by definition, we
  have that $D(\eps_{k_1}, \eps_{k_2}, \dots, \eps_{k_n}) = 0$ {\bf unless}
  $k_1, \dots, k_n$ are all distinct!

  \definition {
    A {\bf permutation} of $\{1, \dots, n\}$ is a one to one (therefore onto)
    function $\sigma: \{1, \dots, n\} \to \{1, \dots, n\}$.

    \btw {This function just shuffles things around}

    We know that it's onto simply by pigeonhole principle! (Also this isn't
    exactly true for infinite sets)
  }

  \TODO{} clean up and explain examples.

  \example {
    $\sigma(1) = 2$

    $\sigma(2) = 4$

    $\sigma(3) = 5$

    $\sigma(4) = 1$

    $\sigma(5) = 3$
  }

  Let $S_n$ be the set of all permutations $\sigma: \{1, \dots, n\} \to \{1,
  \dots, n\}$.

  {\bf Question}: How big in $S_n$? Well,

  \begin{itemize}
    \item How many places can you send $1$ to? $n$ places of course
    \item What about 2? Well you fixed a place for 1, so $n - 1$ places are left
      for 2.
    \item For 3, we have $n - 2$ places left, and so on...
  \end{itemize}

  So $S_n$ is $n!$ big.

  If $D$ is both $n$ linear and alternating, then
  \[
    D(\alpha_1, \dots, \alpha_n)
    = \sum_{\sigma \in S_n} A_{1, \sigma(1)} A_{2, \sigma(2)} \cdots A_{n,
  \sigma(n)} D(\eps_{\sigma(1)}, \eps_{\sigma(2)}, \dots, \sigma_{\sigma(n)})
  \]

  \TODO{} explain notation here

  \example {
    $\tau(1) = 1$

    $\tau(2) = 2$

    $\tau(3) = 5$

    $\tau(4) = 4$

    $\tau(5) = 3$
  }

  \definition {
    A {\bf transposition} $\tau_{i, j}$ is the permutation swapping only $i$ and
    $j$.
  }

  Given any 2 permutations $\sigma$ and $\tau$, we can ``compose them".

  \[
    \sigma \circ \tau \in S_n
  \]

  \example {
    Given

    $\sigma(1) = 2$

    $\sigma(2) = 4$

    $\sigma(3) = 5$

    $\sigma(4) = 1$

    $\sigma(5) = 3$

    and

    $\tau(1) = 1$

    $\tau(2) = 2$

    $\tau(3) = 5$

    $\tau(4) = 4$

    $\tau(5) = 3$


    $\sigma \circ \tau(1) = 2$

    $\sigma \tau(2) = 4$

    $\sigma \tau(3) = 3$

    $\sigma \tau(4) = 1$

    $\sigma \tau(5) = 5$
  }

  Every $\sigma \in S_n$ has an ``inverse".

  \example {
    Following from the examples from earlier, we have

    $\sigma^{-1}(1) = 4$

    $\sigma^{-1}(2) = 1$

    $\sigma^{-1}(3) = 5$

    $\sigma^{-1}(4) = 2$

    $\sigma^{-1}(5) = 3$
  }

  \note {
    If $\tau$ is a transposition, $\tau \circ \tau$ is the same as doing
    nothing!

    \[
      \tau \circ \tau = I
    \]

    Transpositions are called ``idempotent" from this property.
  }

  \note {
    As a sidenote, $S_n$ is a {\bf group} with operation of composition and
    identity element being the ``do nothing" permutation.
  }

  {\bf Fact}: In $S_n$, every $\sigma \in S_n$ is a composition of
  {\it transpositions}.

  We might see $\sigma = \tau_k \circ \tau_{k - 1}, \circ \cdots \circ \tau_1$
  where each $\tau_j$ is a transposition.

  {\bf Proof}: Fix any permutation $\sigma \in S_n$. Choose any $a \in \{1,
  \dots, n\}$. Now keep applying $\sigma$ to $a$.

  \[
    a, \sigma(a), \sigma(\sigma(a)), \dots, \sigma^k(a) = a
  \]

  Eventually, we'll get back to $a$, since there's only finitely many ways to
  permute finitely many things.

  The cycle $C = \{a, \sigma(a), \dots, \sigma^{k - 1}(a)\}$ is a subset of
  $\{1, \dots, n\}$.

  Choose any $b \in \{1, \dots, n\} \setminus C$. Then

  \[
    C_b = \{b, \sigma(b), \dots, \sigma^{k' - 1}(b)\}
  \]

  So $\{1, \dots, n\}$ is partitioned into {\bf cycles}.

  \[
    \sigma = \tau_{3, 5} \circ \tau_{4, 1} \circ \tau_{1, 2} (1)
  \]

  What we get is that each cycle is a product of transpositions.

  Even though the permutations might not be the shortest, their parity will
  always be right.

  \definition{
    A permutation $\sigma$ is called {\bf even} if $\sigma$ is the product of of
    an {\bf even number} of transpositions.

    $\sigma$ is odd otherwise.
  }

  \note {
    The identity is even. Any transposition is $\tau$ is odd.

    In our example, $\sigma$ is odd.

    \[
      \sigma = \tau_{3, 5} \circ \tau_{4, 1} \circ \tau_{1, 2} (1)
    \]

    The number of even transpositions is $n! / 2$ and so is the number of odd
    transpositions.
  }

  Now if $\sigma$ is even, we have

  \[
    1 \cdot D(\eps_{\sigma(1)}, \eps_{\sigma(2)}, \dots, \sigma_{\sigma(n)})
  \]

  \btw {
    We can get from $\sigma$ to the identity in an even number of flips, but
    every time we flip, we switch the sign.
  }

  But if $\sigma$ is odd, we have

  \[
    -1 \cdot D(\eps_{\sigma(1)}, \eps_{\sigma(2)}, \dots, \sigma_{\sigma(n)})
  \]

  \btw {
    We can get from $\sigma$ to the identity in an odd number of flips, but
    every time we flip, we switch the sign.
  }

  \definition {
    We define $\sgn(\sigma)$ as a function $\sgn: S_n \to \pm 1$

    \[
      \sgn(\sigma) = \begin{cases}
        1 & \text{ if $\sigma$ is even} \\
        -1 & \text{ if $\sigma$ is odd}
      \end{cases} 
    \]

    Which represents the parity of permutation $\sigma$.
  }

  {\bf Finally},
  
  If $D$ is $n$ linear and alternating, then

  \[
    D(\alpha_1, \dots, \alpha_n) = \sum_{\sigma \in S_n} \sgn(\sigma) A_{1,
    \sigma(1)} \cdots A_{n, \sigma(n)} \cdot D(\eps_1, \dots, \eps_n)
  \]

  {\bf Corollary}:

  There is {\it exactly one} determinant function

  \[
    \det(A) = \det(\alpha_1, \dots, \alpha_n) = \sum_{\sigma \in S_n}
    \sgn(\sigma) A_{1, \sigma(1)} A_{2, \sigma(2)} \cdots A_{n, \sigma(n)}
  \]

  Since $D(\eps_1, \dots, \eps_n) = 1$ by ``normalization".

  \date{Wed. Apr 5 2023}

  {\bf Last Time}: For any $n$ linear, alternating $D$, we had

  \[
    D(\alpha_1, \dots, \alpha_n) = \sum_{\sigma \in S_n} \sgn(\sigma) A_{1,
    \sigma(1)} \cdots A_{n, \sigma(n)} D(I)
  \]

  And if we insist that $D(I) = 1$, it shrinks down.

  We define

  \[
    \det(\alpha_1, \dots, \alpha_n)
    = D(\alpha_1, \dots, \alpha_n) = \sum_{\sigma \in S_n} \sgn(\sigma) A_{1,
    \sigma(1)} \cdots A_{n, \sigma(n)} D(I)
  \]

  so $\det(I) = 1$

  \btw{
    This is not even by definition, we just see it as a byproduct of the
    definition.
  }

  \subsection{Properties of the Determinant}

  \begin{enumerate}
    \item $\det(AB) = \det(A) \cdot \det(B)$

      {\bf Proof}

      Fix an $n \times n$ matrix $B$. Say that $A = \begin{pmatrix}
        \alpha_1 \\ \vdots \\ \alpha_n
      \end{pmatrix}$ is any $n \times n$ matrix.

      {\bf Check}

      \[
        AB = \begin{pmatrix}
        \alpha_1 \cdot B \\ \vdots \\ \alpha_n \cdot B
      \end{pmatrix}
      \]

      \btw {
        These are the rows of $AB$
      }

      Now, we define $D(A) = \det(AB) = \begin{pmatrix}
        \alpha_1 \cdot B \\ \vdots \\ \alpha_n \cdot B
      \end{pmatrix}$.

      {\bf Check}: $D$ is $n$ linear and alternating.

      % \QUESTION{} How do we check if something is $n$ linear? Like I know that
      % it factors in that weird way or whatever. Do we just check that?

      % What about checking for alternating?

      Therefore, by what we had last time, we have that
      \[
        D(A) = \underbrace{\Big( \sum_{\sigma \in S_n} \sgn(\sigma) A_{1, \sigma(1)} \cdots
        A_{n, \sigma(n)} \Big)}_{\det(A)} D(I)
      \]

      But now notice

      \[
        D(I) = \det(I \cdot B) = \det(B)
      \]

      So we have

      \[
        D(A) = \Big( \sum_{\sigma \in S_n} \sgn(\sigma) A_{1, \sigma(1)} \cdots
        A_{n, \sigma(n)} \Big) D(I) = \det(A) \cdot \det(B)
      \]

    \item $\det(A^t) = \det(A)$

      Recall $A^t$ is defined as $(A^t)_{i, j} := A_{j, i}$. Again, by
      definition of the determinant, we have

      \[
        \det(A^t) = \sum_{\sigma \in S_n} \sgn(\sigma) (A^t)_{1, \sigma(1)} \cdots
        (A^t)_{n, \sigma(n)}
      \]

      Now fix any $\sigma \in S_n$, we have

      \[
        (A^t)_{i, \sigma(i)} := A_{\sigma(i), i}
      \]

      Then we have

      \[
        \det(A^t) = \sum_{\sigma \in S_n} \sgn(\sigma) A_{\sigma(1), 1} \cdots
        A_{\sigma(n), n}
      \]

      But remember that every permutation has an inverse! If we have

      \[
        i = \sigma^{-1}(j)
      \]

      Then,

      \[
        A_{\sigma(i), j} = A_{i, \sigma^{-1}(j)}
      \]

      Additionally, the sign doesn't change, since

      \[
        \sigma = \tau_1 \tau_2 \cdots \tau_k
      \]

      \[
        \sigma^{-1} = \tau_k \tau_{k - 1} \cdots \tau_1
      \]

      \btw {
        We know these are inverses, because look what happens when we compose
        them.

        \[
          \sigma \sigma^{-1} = \tau_1 \tau_2 \tau_3 \cdots (\tau_k \tau_k) \cdots \tau_3 \tau_2 \tau_1 = \text{id}
        \]

        So we have

        \[
          \sgn(\sigma^{-1}) = \sgn(\sigma)
        \]
      }

      So finally, we have

      \begin{align*}
        \det(A^t) =&\sum_{\sigma \in S_n} \sgn(\sigma^{-1}) A_{1, \sigma^{-1}(1)} \cdots A_{n, \sigma^{-1}(n)} \\
        =&\sum_{\sigma \in S_n} \sgn(\sigma) A_{\sigma(1), 1} \cdots A_{\sigma(n), n} = \det(A)
      \end{align*}

    \item $\det(cA) = c^n \det(A)$
    \item If $A$ is upper or lower triangular, then

      \[
        \det(A) = \prod_{i = 1}^n A_{i, i}
      \]

      We can expand by minors for this

      % \begin{align*}
      %   \det(A) =&(-1)^{1 + 1} A_{1, 1} \det(A[1 | 1])
      % \end{align*}

      \TODO{} Write proof

      To show this for the lower triangular case, simply notice that the
      transpose of an upper triangular matrix is lower triangular, and then use
      (2)

    \item If $A$ has a zero row, or zero column, then $\det(A) = 0$

      The proof of this is that every single one of the products in

      \[
        D(A) = \sum_{\sigma \in S_n} \sgn(\sigma) A_{1, \sigma(1)} \cdots A_{n, \sigma(n)} \Big) D(I)
      \]

      Is going to be zero. So if a row is all zeroes, the determinant is zero.

      To show this for the columns, we use the transpose fact again.

    \item Let $r + s = n$. Suppose that we can decompose $A$ as follows

      Let $A = \begin{bmatrix}
        B & C \\
        0 & D
      \end{bmatrix}$

      Where
      \begin{itemize}
        \item $B: r \times r$
        \item $D: s \times s$
        \item $0$ is the $s \times r$ zero matrix
        \item $C: r \times s$
      \end{itemize}

      Then $\det(A) = \det(B) \det(D)$. You can see this if you once again
      expand by minors.

    \item $A: n \times n$. Let's look at elementary row operations.

      \[
        A \to A^\text{rr}
      \]

      {\bf Question}: What do these row operations do to the determinant?

      \begin{itemize}
        \item {\bf Swap two rows}: $\det(A)$ switches signs
        \item {\bf Multiply $\alpha_i$ by $c \ne 0$}: $\det(A)$ gets multiplied
          by $c$.

        \item {\bf Replace $\alpha_j$ by $c \alpha_i + \alpha_j$}: You can check
          that $det(A)$ is unchanged
      \end{itemize}

      So

      \[
        \det(A) = 0 \Leftrightarrow \det(A^\text{rr}) = 0
      \]
  \end{enumerate}


  \date{Fri. Apr 7 2023}

  \btw {
    Exam 2 is next Friday. We'll be covering new material today though.
  }

  Today, we're going to use our results about polynomials and determinants to better
  understand linear operators (remember, these are linear transformations going
  from $V$ to $V$) over a field $F$.

  \[
    T: V \to V
  \]

  It's been a few weeks since we've been in the world of linear operators, and
  it's time we jumped back in.

  In 6.2, 6.3, we'll define two polynomials that help describe the behavior of
  $T$. One is the {\it characteristic polynomial}, the other is the {\it minimal
  polynomial}.

  \xdash[6em]

  {\bf Recall}: For any linear operator $T: V \to V$, and for any basis $\B$ of
  $V$, $[T]_\B$ is the matrix of $T$ with respect to $\B$. Additionally, for any
  $\alpha \in V$,

  \[
    [T]_\B [\alpha]_\B = [T(\alpha)]_\B
  \]

  As we vary $\B$, we get {\it different} matrices $[T]_{\B'}$, where $\B'$ is
  another basis for $V$.

  Additionally, recall the definition of {\it similar matrices}: two matrices
  $A, B: n \times n$ are similar if they represent the same linear
  transformation (possibly under different bases.)

  \[
    B = P^{-1} A P
  \]

  for some invertible matrix $P$.

  {\bf Fact}: If $A, B$ are similar $n \times n$ matrices, then

  \[
    \det(A) = \det(B)
  \]

  {\bf Proof}: Say $B = P^{-1} A P$, then

  \[
    \det(B) = \det(P^{-1} A P) = \det(P^{-1}) \det(A) \det(P) = \det(P^{-1} P)
    \det(A) = 1 \cdot \det(A)
  \]

  \definition {
    For a linear operator $T: V \to V$, {\it define}

    \[
      \det(T) = \det(A)
    \]

    for any/some $n \times n$ matrix $A$ representing $T$ for some basis $\B$.

    \[
      A = [T]_\B
    \]
  }

  Let $A$ be any $n \times n$ matrix over a field $F$.

  Then $\det(A) \ne 0$ if and only if $A$ is invertible.

  {\bf Proof}: Row reduce $A \to A^\text{rr}$. From last time, we saw that
  $\det(A)$ might not equal $\det(A^\text{rr})$, but

  \[
    \det(A) \ne 0 \Leftrightarrow \det(A^\text{rr}) \ne 0
  \]

  If $A$ is invertible, then $A^\text{rr}$ has no completely zero rows (or
  columns). Moreover, all the rows will contain ones, so $\det(A^\text{rr}) =
  1$. So again, $\det(A) \ne 0$.

  Conversely, if $A$ is not invertible, then there {\it is} a fully zero row, so
  $\det(A^\text{rr}) = 0$, so $\det(A) = 0$.

  {\bf Thus}: $T: V \to V$ is invertible (the book calls this ``non-singular")
  if and only if $\det(T) \ne 0$.

  This is great, but there is more.

  \definition {
    A linear operator $T: V \to V$ is {\bf diagonalizable} if there is some
    basis $\B$ of $V$ such that $[T]_\B$ is {\bf diagonal}.

    i.e. $A_{i, j} = 0$ whenever $i \ne j$.
  }

  Similarly

  \definition {
    An $n \times n$ matrix $A$ is {\bf diagonalizable} if it is {\bf similar}
    to a diagonal matrix.

    i.e. if and only if $T_A$ is diagonalizable.
  }

  Suppose that $T$ is diagonalizable and $\B =\{\alpha_1, \dots, \alpha_n\}$ is
  a basis such that

  \TODO{}

  $T$ is invertible if and only if {\bf none} of $c_1, \dots, c_n$ are 0. More
  generally,

  \begin{itemize}
    \item $\rank(T)$ is the number of non-zero $c_i$s.
    \item $\nullity(T)$ is the number of zero $c_i$s.
  \end{itemize}

  Let's look at some examples.

  \example {
    Is

    \[
      \begin{bmatrix}
        1 & 0 \\
        0 & 2 
      \end{bmatrix}
    \]

    diagonalizable?

    {\bf Answer}: Yes

    We'll also see that

    \[
      \begin{bmatrix}
        1 & 4 \\
        0 & 1
      \end{bmatrix}
    \]
    
    is {\bf not} diagonalizable.
  }

  Take $A$ to be the matrix

  \[
    A = \begin{bmatrix}
      0 & 1 \\
      -1 & 0
    \end{bmatrix}
  \]

  Then there is {\bf no} matrix of the form

  \[
    \begin{bmatrix}
      c_1 & 0 \\
      0 & c_2
    \end{bmatrix}
  \]

  With $c_1, c_2 \in \R$ similar to $A$, but in $\C$, there is!

  What this tells us is that, whether a matrix is diagonalizable might depend on
  the field that we're using.

  \section{Eigenvalues}

  \definition {
    An element $c \in F$ is an {\bf eigenvalue} of $T$ if there is some non-zero
    $\alpha \in V$ such that

    \[
      T(\alpha) = c \alpha
    \]

    In such a case, $\alpha$ is called the ``eigenvector".
  }

  \note {
    For any $T: V \to V$, and $c \in F$, let $W_c = \{\alpha \in V : T(\alpha) =
    c \alpha\}$.

    {\bf Claim}: $W_c$ is always non-empty (after all, the zero vector is in it)

    \[
      T(0) = c \cdot 0
    \]
  }

  More interestingly though, $W_c$ is a {\bf subspace} of $V$.

  \[
    = \{\alpha \in V : (c I - T)(\alpha) = 0\} = \Null(cI - T)
  \]

  \QUESTION{} woah woah woah what's happening here?

  \note {
    For most $c \in F$, $W_c = \{0\}$.
  }

  So $c \in F$ is an eigenvalue of $T$ if and only if $\Null(cI - T) \ne \{0\}$
  if and only if $(c I - T)$ is {\bf not invertible} if and only if $\det(c I -
  T) = 0$.

  \subsection{Finding Eigenvalues}

  To find the eigenvalues of $T$, let $A$ be a matrix of $T$ with respect to
  $\B$. The {\bf characteristic polynomial} of $A$ is

  \[
    \det(x I - A)
  \]

  \Lemma {
    In fact, if $A$ and $B$ are similar, they have the same characteristic
    polynomial.
  }
  {
    Say $B = P^{-1} A P$. Then

    \begin{align*}
       &x I - B \\
      =&x I - (P^{-1} A P) \\
      =&x (P^{-1} P) - P^{-1} A P \\
      =&P^{-1} (x I) P - P^{-1} A P \\
      =&P^{-1} (x I - A) P
    \end{align*}
  }

  \definition {
    The characteristic polynomial of $T$ is

    \[
      \det(x I - A)
    \]

    for some/every matrix $A$ representing $T$.
  }


  \TODO{} write down examples of characteristic polynomial for the two matrices
  from earlier.


  \date{Mon. Apr 10 2023}

  \btw {
    Exam is Friday. Everything in class is fair game to be covered. 

    5.4, 5.5, not the adjoint of $A$, or Cramer's Rule.

    Material for today is also fair game.
  }

  We study linear operators $T: V \to V$ where $V$ is a vector space where
  $\dim(V) = n$.
  
  Last time, we learned about the {\bf characteristic polynomial}, defined as

  \[
    f(x) = \det(xI - A)
  \]

  Where $A$ is any $n \times n$ matrix representing $T$. We also saw that $f$
  does not depend on our choice of $A$. If $B$ is similar to $A$, $\det(xI - A)
  = \det(xI - B)$.

  \begin{align*}
    xI - A =&\begin{bmatrix}
      x & 0 & \cdots & 0 & 0 \\
      0 & x & \cdots & 0 & 0 \\
      \vdots & \vdots & \ddots & \vdots & \vdots \\
      0 & 0 & \cdots & x & 0 \\
      0 & 0 & \cdots & 0 & x \\
    \end{bmatrix}
    -
    \begin{bmatrix}
      A_{1,1} & 0 & \cdots & 0 & 0 \\
      0 & A_{2, 2} & \cdots & 0 & 0 \\
      \vdots & \vdots & \ddots & \vdots & \vdots \\
      0 & 0 & \cdots & A_{n - 1, n - 1} & 0 \\
      0 & 0 & \cdots & 0 & A_{n, n} \\
    \end{bmatrix} \\
      =&
    \begin{bmatrix}
      x - A_{1,1} & 0 & \cdots & 0 & 0 \\
      0 & x - A_{2, 2} & \cdots & 0 & 0 \\
      \vdots & \vdots & \ddots & \vdots & \vdots \\
      0 & 0 & \cdots & x - A_{n - 1, n - 1} & 0 \\
      0 & 0 & \cdots & 0 & x - A_{n, n} \\
    \end{bmatrix}
  \end{align*}

  \note {
    $\det(xI - A)$ is a {\bf monic} of degree $n$
  }

  \QUESTION{} Why is it monic?

  \[
    \det(xI - A) = \sum_{\sigma \in S_n}
  \]

  Sometimes, $T$, might not have eigenvalues. Consider for example

  \[
    \begin{bmatrix}
      0 & -1 \\
      -1 & 0
    \end{bmatrix}
  \]

  which doesn't have eigen values over $\R$. (but it does over $\C$.)

  \Proposition {
    Any $T: V \to V$ where $V$ is a vector space over $\C$ has {\bf at least
    one} eigenvalue.
  }
  {
    Let $f(x)$ be the characteristic polynomial. By the Fundamental Theorem of
    Algebra, $f$ has a root $c$. Therefore $c$ is an eigenvalue of $T$.
  }

  For each eigenvalue $c \in F$ of $T: V \to V$, the corresponding {\bf
  Eigenspace} $W_c = \{\alpha \in V : T(\alpha) = c \alpha\} = \Null(cI - T)$
  is a subspace of $V$.

  % {\bf Claim}: This space gives a decomposition of $V$.

  Say $c_1 \ne c_2$, then $W_{c_1} \cap W_{c_2} = \{0\}$ Since $T(0) = 0 = c_1
  \cdot 0 = c_2 \cdot 0$.

  Then $T(\alpha) = c_1 \alpha$, and $T(\alpha) = c_2 \alpha$, we have that
  $(c_1 - c_2) \alpha = 0$. Since $c_1 \ne c_2$, $\alpha$ must be $0$.

  $W_{c_i}$ only share $\{0\}$.

  Moreover, we have the following theorem.

  \Theorem {
    If $c_1, \dots, c_k$ are distinct eigenvalues of some transformation $T: V
    \to V$. Then for any $\beta_i \in W_{c_i} \setminus \{0\}$ for $1 \le i \le
    k$. We have that

    \[
      \{\beta_1, \dots, \beta_k\}
    \]

    are linearly independent in $V$.
  }
  {
    By induction on $k$.

    \induction {
      $k = 1$. So $\beta_1 \in W_{c_1}$. We obviously see that $\{\beta_1\}$ is
      linearly independent.
    }
    {
      Assume that the result holds for $k$.
    }
    {
      Chose $\beta \in W_{c_1} \setminus \{0\}, \dots, \beta_{k + 1} \in W_{c_{k
      + 1}} \setminus \{0\}$.

      We must show that $\{\beta_1, \dots, \beta_{k + 1}\}$ is linearly
      independent.

      Chose $a_1, \dots, a_{k + 1} \in F$, such that

      \[
        a_1 \beta_1 + \cdots a_{k + 1} \beta_{k + 1} = 0 \,\,\,\,\,\,\,\,\,\,\,\,(*)
      \]

      \begin{enumerate}
        \item Multiply (*) by $c_{k + 1}$

          \[
            c_{k + 1} a_1 \beta_1 + \cdots c_{k + 1}  a_{k + 1} \beta_{k + 1} = 0
          \]

        \item Apply $T$ to (*)
          \begin{align*}
            &T(a_1 \beta_1 + \cdots + a_{k + 1} \beta_{k + 1}) = T(0) = 0
            =&T(a_1 \beta_1) + \cdots + T(a_{k + 1} \beta_{k + 1})
            =&c_1 \beta_1 + \cdots + c_{k + 1} \beta_{k + 1}
          \end{align*}
      \end{enumerate}

      Subtract ($2$) from ($1$).

      \[
        (c_{k + 1} - c_1) \alpha_1 \beta_1 + \cdots (c_{k + 1} - c_k) \alpha_k
        \beta_k = 0
      \]

      By the Inductive Hypothesis, $\{\beta_1, \dots, \beta_k\}$ are linearly
      independent.
    }
  }

  Given $T: V \to V$, is $T$ diagonalizable? In other words, is there a basis
  $\B = \{\alpha_1, \dots, \alpha_n\}$ consisting of eigenvectors?

  \[
    [T]_\B = \begin{bmatrix}
      c_1 & \cdots & 0 \\
      \vdots & \ddots & \vdots \\
      0 & \cdots & c_n \\
    \end{bmatrix}
  \]

  where $\alpha_i \in W_{c_i}$, in other words $T(\alpha_i) = c_i \alpha_i$ for
  all $i$.

  \Corollary {
    If $T$ has $n$ distinct eigenvalues, then $T$ is diagonalizable. Since
    $\{\alpha_1, \dots, \alpha_n\}$ are linearly independent by the theorem,
    they form a basis.
  }
  {
    No
  }

  The characteristic polynomial doesn't tell you the dimension of the subspace

  Let $T: V \to V$ be any linear operator. Let $I$ be the collection of all $f
  \in F[x]$ such that $f(T) = 0$. Then $I$ is an ideal.

  $L(V, V)$ is a vector space of $\dim$ $n^2$, so $\{1, T, T^2, \dots,
  T^{n^2}\}$ are linearly dependent.

  Say $c_0 + c_1 T + c_2 T^2 + \cdots + c_{n^2} T^{n^2} = 0$ with not all $0$.

  Let $g(x) = c_0 + c_1 x + c_2 x^2 + \cdots + c_{n^2} x^{n^2}$, then $g(T) =
  0$.

  \Definition {}{
    The {\bf minimum polynomial} of $T$ $m(x)$ is the non-zero element of $I$ of
    {\it least degree}.

    By Chapt. 4

    \[
      I = m(x) F[x]
    \]
  }

  {\bf Fact}: Cayley-Hamilton Theorem. The min polynomial divides the
  characteristic polynomial.

  For instance if $f$ is the characteristic polynomial, then

  \[
    f(T) = 0
  \]

  For the matrix $\begin{bmatrix} 1 & 4 \\ 0 & 1 \end{bmatrix}$ is $m(x) = (x -
  1)^2$. For the identity, its $(x - 1)$.

  \date{Mon. Apr 17 2023}

  Suppose that $T: V \to V$ is a linear operator, where $V$ is a vector space
  over $F$ of dimension $n$.

  {\bf Goal}: Decompose $T$ and $V$ into smaller understandable bits. To do
  this, the tools we have are the characteristic polynomial, and the minimal
  polynomial.

  We're going to need some notions.

  \begin{enumerate}
    \item $T$-invariant subspaces. This is section 6.4 f the book, and what
      we're going to talk about today.
    \item Direct sum decompositions of $V$. (Chapter 6.6)
    \item Direct sum decompositions into $T$ invariant subspaces. (Chapter 6.7)
    \item The primary decomposition theorem. (Chapter 6.8)
  \end{enumerate}

  Let's do a simple example.

  \example {
    Say $\{c_1, \dots, c_k\}$ are the eigenvalues of $T$. Then for each
    eigenvalue $c_i$, we have a subspace $W_{c_i}$ such that

    \begin{align*}
      W_{c_i} =&\{\alpha \in V \mid T(\alpha) = c_i \alpha\} \\
              =&\{\alpha \in V \mid (T - c_i I)(\alpha) = 0 \} \\
    \end{align*}

    If we let $W = W_{c_1} + W_{c_2} + \cdots + W_{c_k}$, we'll see that $W$ is
    a subspace of $V$, and more.
  }

  The minimal polynomial of $T$ $m(x)$ will really help us here.

  \note {
    Recall what the minimal polynomial $m(x)$ is: it's the least polynomial that
    annihilates the matrix associated with $T$.
  }

  {\bf Check}: If $A$ is any matrix representing $T$, then $m(T) = 0$ if and
  only if $m(A) = 0$.

  Let's start easy.

  \Definition{}
  {
    A subspace $W \subseteq V$ is {\bf $T$-invariant} if

    \[
      T(\alpha) \in W \text{ for all $\alpha \in W$}
    \]
  }

  If $W$ is $T$-invariant, we get a {\bf restriction operator} $T_W: W \to W$
  such that

  \[
    T_W(\alpha) = T(\alpha)
  \]

  Two boring examples.

  \example {
    \begin{itemize}
      \item $V$ is always $T$-invariant
      \item $\{0\}$ is always $T$-invariant
    \end{itemize}
  }

  Let's look at more interesting examples.

  \example {
    Say $c$ is an eigenvalue of $T$. Then

    \[
      W_c = \{\alpha \in V \mid T(\alpha) = c \alpha\}
    \]

    is $T$-invariant.

    {\bf Proof}.

    Take any $\alpha \in W_c$. Then we know that $T(\alpha) = c\alpha$ but now,
    is $c \alpha \in W_c$? Well

    \[
      T(c \alpha) = c T(\alpha) = c (c \alpha)
    \]

    Which is in $W_c$.

    Therefore $W_c$ is $T$-invariant.
  }

  Let's look at another example

  \example {
    Let $F[x]$  be the vector space of all polynomials with coefficients in $F$.

    For each $n$, let $P_n$ be the set of all polynomials with degree at most
    $n$. So $P_n$ is a subspace of $F[x]$.

    Let $D: F[x] \to F[x]$ be the differentiation operator.

    \qna {
      Is $P_n$ $D$-invariant?
    }
    {
      Yes. When you differentiate a polynomial, the degree only gets lower, so
      it's in $P_n$.
    }
  }

  \example {
    Let $W = W_{c_1} + \cdots + W_{c_k}$. Is $W$ \Tinv?

    Take any $\beta \in W$. Then, $\beta$ is of the form

    \[
      \beta = a_1 \alpha_1 + a_2 \alpha_2 + \cdots + a_k \alpha_k
    \]

    Where each $\alpha_i \in W_{c_i}$ and each $a_i \in F$. So

    \begin{align*}
      T(\beta) =&T(a_1 \alpha_1 + a_2 \alpha_2 + \cdots + a_k \alpha_k) \\
               =&a_1T(\alpha_1) + \cdots + a_kT(\alpha_k) \\
               =&a_1 (c_1 \alpha_1) + \cdots + a_k (c_k \alpha_k) \\
    \end{align*}

    So $T(\beta) \in W$.
  }

  Let's look at a big source of examples.

  \example {
    Given $T: V \to V$. Let $U: V \to V$ be such that

    \[
      TU = UT
    \]

    Then, $\range(U)$ and $\Null(U)$ are \Tinv.

    {\bf Proof}.

    Choose any $\alpha \in \range(U)$, then we must show that $T(\alpha) \in
    \range(U)$.

    Choose any $\beta \in V$ such that $U(\beta) = \alpha$. Then

    \[
      T(\alpha) = T(U(\beta)) \underset{(TU = UT)}{=} U(T(\beta))
    \]

    Therefore, $T(\alpha) \in \range(U)$ by definition of the range!

    Now choose any $\alpha \in \Null(U)$, we must show that $T(\alpha) \in
    \Null(U)$.

    {\bf Compute} $U(T(\alpha)) = T(U(\alpha)) = T(0) = 0$. Therefore $T(\alpha)
    \in \Null(U)$.
  }

  Thus: If $g(x) \in F[x]$, then $\range(g(T))$ and $\Null(g(T))$ and both
  \Tinv.
  
  {\bf Proof}. By our previous example, we only need to show that $g(T)$
  commutes with $T$

  \[
    g(T) \cdot T = T \circ g(T)
  \]

  {\bf Obvious}: Since the polynomials $g(x) \cdot x = x \circ g(x)$.

  Say $T: V \to V$ is a linear operator and $W \subseteq V$ is \Tinv. Choose
  $\B_W = \{\alpha_1, \dots, \alpha_r\}$ a basis for $W$. Let $\B = \{\alpha_1,
  \dots, \alpha_r, \alpha_{r + 1}, \dots, \alpha_n\}$ be a basis for $V$,
  extending $\B_W$.

  Let $\B = [T]_{\B_W}$ be the $r \times r$ matrix representing $T_W$.

  Then
  \[
    [T]_\B = \begin{bmatrix}
      B & C \\
      0 & D
    \end{bmatrix}
    = [T(\alpha_1), T(\alpha_2), \dots, T(\alpha_n)]
  \]

  Where $D$ is an $(n - r) \times (n - r)$ matrix. Since for $1 \le i \le r$,

  \[
    T(\alpha_i) = T_W(\alpha_i) \in W = \Span(\alpha_1, \dots, \alpha_r)
  \]

  But recall the determinant of a block matrix of this form!

  \[
    \det\left([T]_\B\right) = \det\left(\begin{bmatrix}
      B & C \\
      0 & D
    \end{bmatrix}\right) =\det(B) \cdot \det(D)
  \]

  {\bf Similarly}: $\det(xI - T) = \det(xI - T_W) \cdot \det(xI - D)$. Therefore
  the characteristic polynomial of $T_W$ divides the characteristic polynomial
  of $T$.

  {\bf Similarly}: The minimum polynomial of $T_w$ divides the minimal
  polynomial of $T$. Notice  \[
    \left([T]_\B\right)^k = \begin{bmatrix}
      B^k & C^k \\
      0 & D^k
    \end{bmatrix}
  \]

  ``If you don't believe the above, just trust me that it works" -Laskowski
  (kinda)

  The minimal polynomial $m(x)$ gives that $m(T) = 0$. In particular:
  $m(T)(\alpha) = 0$.

  Fix $\alpha \in V$, and $W \subseteq V$ \Tinv.

  \Definition{} {
    The {\bf stuffer ideal}

    \[
      I = \{g(x) \in F[x] \mid g(T)(\alpha) \in W\}
    \]

    $I$ (which really depends on $\alpha, W$) is an ideal of $F[x]$.
  }

  \Definition {}
  {
    The {\bf $T$-conductor} of $\alpha$ into $W$ is the monic $g(x) \in I$ of {\bf
    least degree}.
  }

  Clearly, the $T$-conductor of $\alpha \in W$ {\bf divides} the minimal
  polynomial $m(x)$.

  {\bf Proof}.

  Since $m(T)(\alpha) = 0 \in W$, we get that $m(x) \in I$, but $g(x)$ divides
  every polynomial in $I$.

  \date{Wed. Apr 19 2023}

  \TODO{} Missed class, sorry


  \date{Mon. Apr 24 2023}

  Let's look again at direct sums.

  Suppose that $V = W_1 \oplus \cdots \oplus W_k$. We have that, for any $\gamma
  \in V$, we can write

  \[
    \gamma = \beta_1 + \beta_2 + \cdots + \beta_k
  \]

  with $\beta_i \in W_i$. And this decomposition is unique!


  \Definition {} {
    A set $\{E_1, \dots, E_k\}$ of {\bf coordinate operators} on $V$ has each
    $E_i$ is a linear operator such that $3$ properties hold.

    \begin{enumerate}
      \item $E_i^2 = E_i$, i.e. $E_i(E_i(\gamma)) = E_i(\gamma)$ for all $\gamma
        \in V$.

      \item $E_i E_j = 0$ whenever $i \ne j$

      \item $E_1 + E_2 + \cdots + E_k = I$
    \end{enumerate}
  }

  We have the following theorem.

  \Theorem {

    \begin{enumerate}[label=(\Alph*)]
      \item If $V = W_1 \oplus \cdots \oplus W_k$ then, letting $E_i(\gamma)$ be the
        unique ``coordinate" $\beta_i \in W_i$ gives $\{E_1, \dots, E_k\}$ is a set
        of coordinate operators.

      \item If $\{E_1, \dots, E_k\}$ is a set of coordinate operators, and if
        $W_i = \Range(E_i)$, then

        \[
          V = W_1 \oplus \cdots \oplus W_k
        \]
    \end{enumerate}
  }
  {
    \begin{enumerate}[label=(\Alph*)]
      \item 

        \begin{enumerate}[label=\arabic*.]
          \item Define $E_i(\beta_1 + \cdots + \beta_k) = \beta_i$ where $\beta_i \in W_i$ for each $i$. Then

            \[
              E_i(E_i(\beta_1 + \cdots + \beta_k)) = E_i(\beta_i) = \beta_i
            \]

            So $E_i^2 = E_i$, so $(1)$ holds.

          \item Chose $i \ne j$, then

            \[
              E_j(E_i(\beta_1 + \cdots + \beta_k)) = E_j(\beta_i) = 0
            \]

            So $E_i E_j = 0$, and $(2)$ holds.

          \item What is $(E_1 + E_2 + \cdots + E_k)(\beta_1 + \cdots + \beta_k)
            = \beta_1 + \cdots + \beta_k$

            Well, 

            \begin{align*}
              &E_1(\beta_1 + \cdots + \beta_k) + E_2(\beta_1 + \cdots + \beta_k) + \cdots + E_k(\beta_1 + \cdots + \beta_k) \\
              =&E_1 + \cdots E_k = I
            \end{align*}
        \end{enumerate}

      \item Assume $\{E_1, \dots, E_k\}$ is a set of coordinate operators, and
        let $W_i = \Range(E_i)$.

        {\bf Claim}. $V = W_1 \oplus \cdots \oplus W_k$.

        We first show that $V = W_1 + \cdots + W_k$. Choose any $\gamma \in V$.

        By $(3)$

        \[
          (E_1 + \cdots + E_k)(\gamma) = I(\gamma) = \gamma
        \]

        Now we need to show that this sum is unique.

        Say $\gamma = \beta_1 + \cdots + \beta_k$ with each $\beta_i \in W_i$.

        For each $i$, choose $\alpha_i \in V$ such that $E_i(\alpha_i) =
        \beta_i$.

        Now choose any $1 \le j \le k$, we know that

        \begin{align*}
          &E_j(\gamma) = E_j(\beta_1 + \cdots + \beta_k) \\
         =&E_j(E_1(\alpha_1) + \cdots + E_k(\alpha_k)) \\
         =&E_j(E_1(\alpha_1)) + \cdots + E_j(E_j(\alpha_j)) + \cdots + E_j(E_k(\alpha_k)) \\
         =&E_j(E_j(\alpha_j)) = E_j(\alpha_j) = \beta_j
        \end{align*}

        So we've shown that it is unique: There is only one way of writing this,
        given a $\gamma$, the only choice for $\beta_j$ is $E_j(\alpha_j)$, all
        the other things are zero.

    \end{enumerate}
  }

  Now, suppose that $T: V \to V$ is a linear operator. We would like to write
  $V$ as $W_1 \oplus \cdots \oplus W_k$ where each $W_i$ is $T$-invariant.

  Suppose $V = W_1 \oplus \cdots \oplus W_k$ where each $W_i$ is $T$-invariant.
  We want to understand what $T$ is. We're going to be looking at ``Divide and
  Conquer".

  Specifically, let's say that $A_i$ is the matrix for $T_{W_i}: W_i \to W_i$
  ($T$ restricted to $W_i$) with respect to some basis $\B_i$. 

  Then, let $\B = \B_1 \cup \cdots \cup \B_k$. What is the matrix of $T$ with
  respect of this basis? Well it looks like this.

  \[
    \begin{bmatrix}
      (A_1) & 0 & \cdots & 0 & 0 \\
      0 & (A_2) & \cdots & 0 & 0 \\
      \vdots & \vdots & \ddots & \vdots & \vdots \\
      0 & 0 & \cdots & (A_{k - 1}) & 0 \\
      0 & 0 & \cdots & 0 & (A_k) \\
    \end{bmatrix}
  \]

  Where each $A_i$ is the block matrix for $T_{W_i}$.

  \Lemma {
    Suppose $T: V \to V$ is a linear operator. $\{E_1, \dots, E_k\}$ are a set
    of coordinate operators, each of which is $h_i(T)$ for some polynomial in
    $h_i \in F[x]$.

    Then, $W_i = \Range(E_i)$ is $T$-invariant!!
  }
  {
    We did this! If $T$ and $U$ commute, $UT = TU$, then $\Range(U)$ and
    $\Null(U)$ are $T$-invariant!

    Here, $T$ and $h_i(T)$ commute, simply because $x h_i(x) = h_i(x) \cdot x
    \in F[x]$.
  }

  \NamedTheorem {
    \section*{Primary Decomposition Theorem}
  }
  {
    Suppose that $T: V \to V$ is a linear operator, and $V$ is a vector space
    over $F$ (not necessarily algebraically closed!)

    Suppose that minimal polynomial $m(x)$ factors into

    \[
      m(x) = p_1(x)^{r_1} \cdot p_2(x)^{r_2} \cdot \cdots \cdot p_k(x)^{r_k} 
    \]

    Where $\{p_1, \dots, p_k\}$ are {\bf primes} (monic and irreducible) in
    $F[x]$.

    Then 

    \[
      V = W_1 \oplus \cdots \oplus W_k
    \]

    Where each $W_i = \Null(p_i(x)^{r_i})$, each $W_i$ is $T$-invariant, and the
    minimal polynomial of $T_{W_i}: W_i \to W_i$ is $p_i(x)^{r_i}$.

    \btw {
      Recall that in $\C[x]$, the only primes are linear (because of algebraic
      completeness), but in $\R[x]$ $(x^2 + 1)$ is prime, for example. In the
      rationals, life is even worse!

      $(x^2 - 2)$ is prime in $\Q[x]$.
    }
  }
  {
    We're going to do this by constructing the $E_i$s.

    For each $i$, let

    \[
      f_i = \frac{m(x)}{p_i(x)^{r_i}}
    \]

    {\bf Check}: $\gcd(\{f_1, \dots, f_k\}) = 1$. Therefore, there are $g_1,
    \dots, g_k \in F[x]$ such that

    \[
      f_1(x) g_1(x) + \cdots + f_k(x) g_k(x) = 1
    \]

    We're going to define $E_i = f_i(T) g_i (T)$.

    \TODO{} Proof will be finished (and formalized) on Wednesday.
  }

  \date{Wed. Apr 25 2023}

  \TODO{} missed class, sorry

  \date{Fri. Apr 28 2023}

  Let $V$ be a finite-dimensional vector space over any field $F$. $T: V \to V$
  a linear operator.

  Chose any $\alpha \in V$, and look at the set $\{f(T)(\alpha) : f(x) \in
  F[x]\}$ (apply any polynomial of $T$ to $\alpha$). This is a {\it subspace} of
  $V$, it is called $Z(\alpha, T)$, the {\bf cyclic subspace} of $V$ generated
  by $T$ and $\alpha$.

  Another way of writing this subspace is as the span of $\{\alpha, T(\alpha),
  T^2(\alpha), \dots\}$.

  We will see that for any such $T: V \to V$, there are finitely many
  $\{\alpha_1, \dots, \alpha_r\} \subseteq V$ such that

  \[
    V = Z(\alpha_1, T) \oplus \cdots \oplus Z(\alpha_r, T)
  \]

  Here, what's happening is that $V$ is a direct sum of cyclic subspaces.
  (\QUESTION{} Why cyclic? What's cyclic about them?)

  Suppose $T: V \to V$ and there is some $\alpha \in V$, such that

  \[
    Z(\alpha, T) = V
  \]

  In this case, we say that $T$ is {\bf cyclic}, and $\alpha$ is a {\bf cyclic
  vector} for $T$.

  \example {
    For any $V$, $Z(0, T) = \{f(T)(0) : f(x) \in F[x]\} = \{0\}$. \QUESTION{}
    Wait what? What about with polynomials with constants on them?

    This $0$ is a cyclic vector {\it only when} $V = \{0\}$.
  }

  \example {
    Say $\dim(V) = 1$, and $T: V \to V$ is non-zero. Then every $\alpha \ne 0$
    is a cyclic vector for $T$.

    BORING!!
  }

  \example {
    Suppose $\dim(V) \ge 2$, and $\alpha$ is an eigenvector for $T$. In other
    words

    \[
      T(\alpha) = c(\alpha)
    \]

    for some $c \im F$. For any $f(x) \in F[x]$, we have

    \[
      f(T)(\alpha) = f(c)(\alpha)
    \]

    This is just some constant, times $\alpha$! (excitement. Not factorial)

    So $\dim(Z(\alpha, T)) = 1$, so $Z(\alpha, T)$ cannot equal $V$, since
    $\dim(V) \ge 2$.

    When $\dim(V) \ge 2$, eigenvectors are {\bf not} cyclic vectors.
  }

  \example {
    For any polynomial $f(x) = c_0 + c_1 x + \cdots + c_k x^k$, we have that

    \[
      f(I)(\alpha) = (c_0 + c_1 + \cdots + c_k) (\alpha)
    \]

    Again, we see that $\dim(Z(\alpha, T)) = 1$, so $Z(\alpha, T) \ne V$.

    What we can gather from this is that, when $\dim(V) \ge 2$, The identity
    transformation is {\bf not cyclic}!
  }

  Let's look at an example that is cyclic.

  \example {
    Let $A = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}: \R^2 \to \R^2$.

    {\bf Claim}: $\begin{bmatrix} 1 \\ 0\end{bmatrix}$ is a cyclic vector for
    $T_A$.

    {\bf Proof}. Given some $\begin{bmatrix} a \\ b \end{bmatrix} \in \R^2$, we
    must find some polynomial $g(x) \in \R[x]$ such that

    \[
      g(T) \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} a \\ b \end{bmatrix}
    \]

    Let $g(x) = a + bx$. Then $g(T_A) = a(I) + b (T_A)$, and $g(T_A)
    \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} a \\ b \end{bmatrix}$
  }

  For the rest of the day, we assume that $V$ is finite dimensional, over some
  field $F$, $T: V \to V$ is cyclic, and some $\alpha \in V$ is a cyclic vector.

  When you have this, life is easy!

  Let $I = \{f(x) \in F[x] : f(T)(\alpha) = 0\}$. Then, (you can check that) $I$
  is an ideal. Obviously, $0$ is in it, but also the minimal polynomial of $T$,
  $m(x)$ is also in it!

  Let $p_\alpha(x) \in I$ be the monic polynomial of least degree.

  Write $p_\alpha(x) c_0 + c_1 x + \cdots + c_{k - 1} x^{k - 1} + 1 \cdot x^k$.

  {\bf Fact}.

  Let $\B = \{\alpha, T(\alpha), T^2(\alpha), \dots, T^{k - 1}(\alpha)\}$ is a
  basis for $V$.

  In particular, $\dim(V) = k$.

  {\bf Proof}.

  \begin{enumerate}
    \item $\B$ spans $V$.

      Choose any $\beta \in V$. Chose any $g(x) \in F[x]$ such that

      \[
        g(T)(\alpha) = \beta
      \]

      (Since $Z(\alpha, T) = V$.)

      By the Euclidean Algorithm, there are $q(x)$ and $r(x)$ such that

      \[
        g(x) = p_\alpha(x) q(x) + r(x)
      \]

      where either $r = 0$, or $\deg(r) < \deg(p_\alpha) = k$.

      So, $g(T)(\alpha) = p_\alpha(T) q(T) \alpha + r(T) \alpha$. But notice
      that $p_\alpha(T) q(T) \alpha = 0$ since $p_\alpha(T) \alpha = 0$, that's
      why it's in the ideal! Therefore, we have that

      \[
        \beta = p_\alpha(T) q(T) \alpha + r(T) \alpha = r(T)(\alpha)
      \]

      Then $\beta = r(T)(\alpha) = d_0 \alpha + d_1 T(\alpha) + \cdots + d_{k -
      1} T^{k - 1}(\alpha)$

      So $\B$ spans $V$.

    \item $\B$ linearly independent.

      \TODO{} sorry, computer died

  \end{enumerate}

\end{document}
